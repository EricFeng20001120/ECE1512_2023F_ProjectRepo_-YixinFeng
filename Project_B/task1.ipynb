{"cells":[{"cell_type":"markdown","source":["###Reference\n","https://github.com/VICO-UoE/DatasetCondensation"],"metadata":{"id":"FgHeAl6EFW3I"}},{"cell_type":"markdown","source":["# Import"],"metadata":{"id":"udhOCqo2LS-Q"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/ECE1513/Project_B_Supp/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0tSMWuck1LP7","executionInfo":{"status":"ok","timestamp":1701135471190,"user_tz":300,"elapsed":17171,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"987e49c6-8d88-4323-a28b-505333da7715"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/ECE1513/Project_B_Supp\n"]}]},{"cell_type":"code","source":["!pip install fvcore"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7f-ImTZoNy0Y","executionInfo":{"status":"ok","timestamp":1700949047093,"user_tz":300,"elapsed":12029,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"30b38b43-0642-4431-93ac-8b3c0b526eda"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting fvcore\n","  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.23.5)\n","Collecting yacs>=0.1.6 (from fvcore)\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.1)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.3.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n","Collecting iopath>=0.1.7 (from fvcore)\n","  Downloading iopath-0.1.10.tar.gz (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.5.0)\n","Collecting portalocker (from iopath>=0.1.7->fvcore)\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Building wheels for collected packages: fvcore, iopath\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=6d9fd72f85912e1a02b78999f0a9f1dae467335d512ea01d0deebb2636cf7270\n","  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n","  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=7e99f996c83744f5a657a575f08a8513084fe9ceb1ad4cbdbb8b6e4d13773988\n","  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n","Successfully built fvcore iopath\n","Installing collected packages: yacs, portalocker, iopath, fvcore\n","Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwDWllrr1KV4"},"outputs":[],"source":["import networks\n","import utils\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms,datasets\n","import tensorflow as tf\n","import pandas as pd\n","import torch.optim.lr_scheduler\n","import copy\n","from torchvision.utils import save_image\n"]},{"cell_type":"markdown","source":["# Data Retrival"],"metadata":{"id":"atd7FwIaLOl_"}},{"cell_type":"code","source":["# Here you need to unzip the folder either on drive or local directory\n","#!unzip /content/drive/MyDrive/ECE1513/Project_B_Supp/mhist_dataset/images.zip -d /content/drive/MyDrive/ECE1513/Project_B_Supp/mhist_dataset\n"],"metadata":{"id":"6omk1ARMgDdf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_directories(base_path, sub_folders):\n","\n","    if not tf.io.gfile.exists(base_path):\n","        tf.io.gfile.mkdir(base_path)\n","\n","    for folder in sub_folders:\n","        folder_path = f\"{base_path}/{folder}\"\n","        if not tf.io.gfile.exists(folder_path):\n","            tf.io.gfile.mkdir(folder_path)\n","\n","base_paths = ['./mhist_dataset/train', './mhist_dataset/test','./mhist_dataset/augmentation']\n","sub_folders = ['HP', 'SSA']\n","\n","for path in base_paths:\n","    create_directories(path, sub_folders)"],"metadata":{"id":"eOpZvdGahSCF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = './mhist_dataset/annotations.csv'\n","df = pd.read_csv(path)\n","csv = dict(df)\n","image_num = len(csv.get('Image Name'))\n","\n","\n","for i in range(image_num):\n","  if csv.get('Partition')[i] == \"train\":\n","    if csv.get('Majority Vote Label')[i] == \"HP\":\n","      src = './mhist_dataset/images/' + csv.get('Image Name')[i]\n","      dst = './mhist_dataset/train/HP/' + csv.get('Image Name')[i]\n","      tf.io.gfile.copy(src, dst, overwrite=True)\n","    else:\n","      src = './mhist_dataset/images/' + csv.get('Image Name')[i]\n","      dst = './mhist_dataset/train/SSA/' + csv.get('Image Name')[i]\n","      tf.io.gfile.copy(src, dst, overwrite=True)\n","  else:\n","    if csv.get('Majority Vote Label')[i] == \"HP\":\n","      src = './mhist_dataset/images/' + csv.get('Image Name')[i]\n","      dst = './mhist_dataset/test/HP/' + csv.get('Image Name')[i]\n","      tf.io.gfile.copy(src, dst, overwrite=True)\n","    else:\n","      src = './mhist_dataset/images/' + csv.get('Image Name')[i]\n","      dst = './mhist_dataset/test/SSA/' + csv.get('Image Name')[i]\n","      tf.io.gfile.copy(src, dst, overwrite=True)"],"metadata":{"id":"d-qaF9YYh7yN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib. image as image\n","from random import random\n","\n","path = './mhist_dataset/annotations.csv'\n","df = pd.read_csv(path)\n","csv = dict(df)\n","image_num = len(csv.get('Image Name'))\n","\n","for i in range(image_num):\n","    label = csv.get('Majority Vote Label')[i]\n","    src = './mhist_dataset/images/' + csv.get('Image Name')[i]\n","    img = image.imread(src)\n","    img_aug = tf.image.rot90(tf.image.random_flip_up_down(img), k=round(random()*3))\n","\n","    base_name = os.path.splitext(csv.get('Image Name')[i])[0]\n","    extension = os.path.splitext(csv.get('Image Name')[i])[1]\n","    augmented_name = base_name + '_aug' + extension\n","\n","    if label == \"HP\":\n","        # Save augmented image\n","        tf.keras.utils.save_img('./mhist_dataset/augmentation/HP/' + augmented_name, img_aug, data_format=None, file_format=None, scale=True)\n","        # Save original image\n","        tf.keras.utils.save_img('./mhist_dataset/augmentation/HP/' + csv.get('Image Name')[i], img, data_format=None, file_format=None, scale=True)\n","    elif label == \"SSA\":\n","        # Save augmented image\n","        tf.keras.utils.save_img('./mhist_dataset/augmentation/SSA/' + augmented_name, img_aug, data_format=None, file_format=None, scale=True)\n","        # Save original image\n","        tf.keras.utils.save_img('./mhist_dataset/augmentation/SSA/' + csv.get('Image Name')[i], img, data_format=None, file_format=None, scale=True)\n"],"metadata":{"id":"SAowUK-fC2mP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Baseline Training"],"metadata":{"id":"AA4gpAMaLeoC"}},{"cell_type":"markdown","source":["##MHIST"],"metadata":{"id":"NJw7GQi3NgAn"}},{"cell_type":"code","source":["# initialize args\n","args = type('', (), {})()\n","args.lr_net = 0.01 # learning rate of the network\n","args.lr_img = 0.1 # learning rate of synthetic dataset\n","args.dsa_strategy = None\n","args.method = 'DC'\n","args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","args.dsa_param = utils.ParamDiffAug()\n","args.dsa = True if args.method == 'DSA' else False\n","args.dc_aug_param = None"],"metadata":{"id":"a8r1cWvxHknn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["IMG_SIZE = (32,32)\n","train_dir = './mhist_dataset/train'\n","test_dir = './mhist_dataset/test'\n","BATCH_SIZE=32\n","\n","\n","transform = transforms.Compose([\n","    transforms.Resize(IMG_SIZE),\n","    transforms.ToTensor(),\n","])\n","# Create the datasets\n","train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n","test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n","\n","# Create the data loaders\n","trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n","testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n"],"metadata":{"id":"IhaGuQH--sMY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_dataset(it_eval, net, trainloader, testloader, args):\n","    net = net.to(args.device)\n","    criterion = nn.CrossEntropyLoss().to(args.device)\n","    optimizer = torch.optim.SGD(net.parameters(), lr=args.lr_net)\n","    net.train()\n","    # Cosine Annealing Scheduler\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epoch_train)\n","    # Train the network\n","    for ep in range(args.epoch_train):\n","        loss_train, acc_train = utils.epoch('train', trainloader, net, optimizer, criterion, args, aug=True)\n","        scheduler.step()\n","        print(f'{it_eval} Training Accuracy: {acc_train:.4f}, Training loss: {loss_train:.4f}')\n","    # Evaluate on the test set\n","    net.eval()\n","    loss_test, acc_test = utils.epoch('test', testloader, net, optimizer, criterion, args, aug=False)\n","\n","    print(f'{it_eval} Training Accuracy: {acc_train:.4f}, Testing Accuracy: {acc_test:.4f}')\n","\n","    return acc_train, acc_test"],"metadata":{"id":"IhpMHRw92Upk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args.model = 'ConvNetD5'\n","net = utils.get_network(args.model, channel=3, num_classes=2, im_size=IMG_SIZE).to(args.device)"],"metadata":{"id":"5Xsz87CSAlVV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracy = []\n","it_eval = 1\n","args.epoch_train = 30\n","train_acc, test_acc = evaluate_dataset(it_eval, net, trainloader, testloader, args)\n","accuracy.append(test_acc)\n","\n","# Print or process the accuracy as needed\n","print(f'Final Test Accuracy: {test_acc:.4f}')"],"metadata":{"id":"DVJ8MHYvDD2V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701136708136,"user_tz":300,"elapsed":456095,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"5d625941-39b7-4a3d-f08e-0b3ce751fc3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 Training Accuracy: 0.8423, Training loss: 0.4018\n","1 Training Accuracy: 0.9103, Training loss: 0.2715\n","1 Training Accuracy: 0.9306, Training loss: 0.2379\n","1 Training Accuracy: 0.9531, Training loss: 0.2083\n","1 Training Accuracy: 0.9605, Training loss: 0.1931\n","1 Training Accuracy: 0.9697, Training loss: 0.1677\n","1 Training Accuracy: 0.9853, Training loss: 0.1445\n","1 Training Accuracy: 0.9940, Training loss: 0.1158\n","1 Training Accuracy: 0.9926, Training loss: 0.1114\n","1 Training Accuracy: 0.9977, Training loss: 0.0950\n","1 Training Accuracy: 0.9982, Training loss: 0.0865\n","1 Training Accuracy: 0.9982, Training loss: 0.0798\n","1 Training Accuracy: 0.9986, Training loss: 0.0755\n","1 Training Accuracy: 0.9986, Training loss: 0.0727\n","1 Training Accuracy: 0.9991, Training loss: 0.0694\n","1 Training Accuracy: 0.9991, Training loss: 0.0665\n","1 Training Accuracy: 0.9995, Training loss: 0.0644\n","1 Training Accuracy: 0.9995, Training loss: 0.0625\n","1 Training Accuracy: 0.9995, Training loss: 0.0610\n","1 Training Accuracy: 0.9995, Training loss: 0.0597\n","1 Training Accuracy: 0.9995, Training loss: 0.0587\n","1 Training Accuracy: 0.9995, Training loss: 0.0579\n","1 Training Accuracy: 0.9995, Training loss: 0.0572\n","1 Training Accuracy: 0.9995, Training loss: 0.0567\n","1 Training Accuracy: 0.9995, Training loss: 0.0563\n","1 Training Accuracy: 0.9995, Training loss: 0.0559\n","1 Training Accuracy: 0.9995, Training loss: 0.0557\n","1 Training Accuracy: 0.9995, Training loss: 0.0556\n","1 Training Accuracy: 0.9995, Training loss: 0.0555\n","1 Training Accuracy: 0.9995, Training loss: 0.0554\n","1 Training Accuracy: 0.9995, Testing Accuracy: 0.7738\n","Final Test Accuracy: 0.7738\n"]}]},{"cell_type":"code","source":["# # Save the model's state dictionary\n","torch.save(net.state_dict(), 'conv_mhist.pth')\n","\n","# net = utils.get_network(args.model, channel=3, num_classes=2, im_size=IMG_SIZE).to(args.device)\n","# net.load_state_dict(torch.load('conv7_mhist.pth'))\n","# net.eval()\n"],"metadata":{"id":"dY5ryI6CJBh1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss().to(args.device)\n","optimizer = torch.optim.SGD(net.parameters(), lr=args.lr_net)\n","# Evaluate on the test set\n","loss_test, acc_test = utils.epoch('test', testloader, net, optimizer, criterion, args, aug=False)\n","\n","print(f'Testing Accuracy: {acc_test:.4f}')"],"metadata":{"id":"3rqA6gulKdWQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from fvcore.nn import FlopCountAnalysis, parameter_count\n","\n","def calculate_flops(model, input_size):\n","    inputs = torch.randn(input_size).to(args.device)\n","    flop_analysis = FlopCountAnalysis(model, inputs)\n","    return flop_analysis.total()\n","\n","# Example usage\n","flops = calculate_flops(net, (1, 3, *IMG_SIZE))  # Adjust the input size as needed\n","print(f\"FLOPs: {flops}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3GkDTQjNW2C","executionInfo":{"status":"ok","timestamp":1700950215087,"user_tz":300,"elapsed":196,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"faea1a0c-abc3-49c2-a809-f2cbe05b520d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::avg_pool2d encountered 3 time(s)\n"]},{"output_type":"stream","name":"stdout","text":["FLOPs: 51589120\n"]}]},{"cell_type":"markdown","source":["## MNIST"],"metadata":{"id":"MFfPmd0XNZHU"}},{"cell_type":"code","source":["args.model = 'ConvNet'\n","args.dataset = 'MNIST'\n","# Now you can use the dot notation\n","channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = utils.get_dataset(args.dataset, './data')\n","\n","# Load and train network\n","net = utils.get_network(args.model, channel=channel, num_classes=num_classes, im_size=im_size).to(args.device)"],"metadata":{"id":"jel7AQ9FElI8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load dataset\n","channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = utils.get_dataset(args.dataset, './data')\n","args.batch_train = 32\n","# Load and train network\n","net = utils.get_network(args.model, channel=channel, num_classes=num_classes, im_size=im_size).to(args.device)\n","trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n","accuracy = []\n","it_eval = 1\n","train_acc, test_acc = evaluate_dataset(it_eval, net, trainloader, testloader, args)\n","accuracy.append(test_acc)\n","\n","# Print or process the accuracy as needed\n","print(f'Final Test Accuracy: {test_acc:.4f}')"],"metadata":{"id":"d0ysw1GnDNd8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700950528967,"user_tz":300,"elapsed":222728,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"97c1a040-c24a-44cb-8e42-19f1c3735f7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 Training Accuracy: 0.9627, Training loss: 0.1600\n","1 Training Accuracy: 0.9858, Training loss: 0.0526\n","1 Training Accuracy: 0.9890, Training loss: 0.0397\n","1 Training Accuracy: 0.9907, Training loss: 0.0326\n","1 Training Accuracy: 0.9922, Training loss: 0.0274\n","1 Training Accuracy: 0.9938, Training loss: 0.0235\n","1 Training Accuracy: 0.9945, Training loss: 0.0208\n","1 Training Accuracy: 0.9953, Training loss: 0.0186\n","1 Training Accuracy: 0.9958, Training loss: 0.0174\n","1 Training Accuracy: 0.9960, Training loss: 0.0166\n","1 Training Accuracy: 0.9960, Testing Accuracy: 0.9938\n","Final Test Accuracy: 0.9938\n"]}]},{"cell_type":"code","source":["# Save the model's state dictionary\n","torch.save(net.state_dict(), 'conv3_mnist.pth')\n","\n","net = utils.get_network(args.model, channel=channel, num_classes=num_classes, im_size=im_size).to(args.device)\n","net.load_state_dict(torch.load('conv3_mnist.pth'))\n","net.eval()\n"],"metadata":{"id":"aY8sHAYzLHUx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from fvcore.nn import FlopCountAnalysis, parameter_count\n","\n","def calculate_flops(model, input_size):\n","    inputs = torch.randn(input_size).to(args.device)\n","    flop_analysis = FlopCountAnalysis(model, inputs)\n","    return flop_analysis.total()\n","\n","# Example usage\n","flops = calculate_flops(net, (1, channel, *im_size))  # Adjust the input size as needed\n","print(f\"FLOPs: {flops}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B2D24px2Pk34","executionInfo":{"status":"ok","timestamp":1700950633580,"user_tz":300,"elapsed":108,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"6e5d8289-e7e9-4793-c4b1-72066408e429"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::avg_pool2d encountered 3 time(s)\n"]},{"output_type":"stream","name":"stdout","text":["FLOPs: 49246208\n"]}]},{"cell_type":"markdown","source":["# Synthetic Data Training"],"metadata":{"id":"bdEEjqvIVbLz"}},{"cell_type":"code","source":["def data_condensation(outer_loop,inner_loop):\n","  # load real dataset\n","  channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = utils.get_dataset(args.dataset, args.data_path)\n","  models = utils.get_eval_pool(args.eval_mode, args.model, args.model)\n","\n","  # record accuracy of each model\n","  records = dict()\n","\n","  for model in models:\n","    records[model] = []\n","\n","  images_real = []\n","  labels_real = []\n","\n","  #prepare individual image tensors for batch processing since PyTorch models typically expect inputs to have a batch dimension.\n","  images_real = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))] # converting to 3D tensor,(1,height,width)\n","  labels_real = [dst_train[i][1] for i in range(len(dst_train))]\n","\n","  indices_class = [[] for c in range(num_classes)]\n","  for i, lab in enumerate(labels_real):\n","      indices_class[lab].append(i)\n","  images_real = torch.cat(images_real, dim=0).to(args.device)\n","  labels_real = torch.tensor(labels_real, dtype=torch.long, device=args.device) #torch.int64\n","\n","  def get_images(c,n): # get n images from c classes\n","    idx_shuffle = np.random.permutation(indices_class[c])[:n]\n","    return images_real[idx_shuffle]\n","\n","  ''' initialize the synthetic data '''\n","  if args.init == 'real':\n","      print('initialize synthetic data from random real images')\n","      label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]\n","      image_syn = torch.randn(size=(num_classes*args.ipc, channel, im_size[0], im_size[1]), dtype=torch.float, requires_grad=True, device=args.device)\n","      for c in range(num_classes):\n","          image_syn.data[c*args.ipc:(c+1)*args.ipc] = get_images(c, args.ipc).detach().data\n","  else:\n","      print('initialize synthetic data from random noise')\n","      # args.ipc is the image per class\n","      image_syn = torch.randn(size=(num_classes*args.ipc, channel, im_size[0], im_size[1]), dtype=torch.float, requires_grad=True, device=args.device)\n","      label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]\n","\n","  # Training loop\n","\n","  ''' training '''\n","  optimizer_img = torch.optim.SGD([image_syn, ], lr=args.lr_img, momentum=0.5) # optimizer for synthetic data, optimize the list of image_syn in this case\n","  optimizer_img.zero_grad()\n","  criterion = nn.CrossEntropyLoss().to(args.device)\n","  print('%s training begins'%utils.get_time())\n","\n","  for it in range(args.Iteration+1):\n","    ''' Train synthetic data '''\n","    net = utils.get_network(args.model, channel, num_classes, im_size).to(args.device) # get model\n","    net.train()\n","    net_parameters = list(net.parameters())\n","    optimizer_net = torch.optim.SGD(net.parameters(), lr=args.lr_net)  # optimizer for synthetic data\n","    optimizer_net.zero_grad()\n","    #outer loop\n","    for k in range(outer_loop):\n","      loss = torch.tensor(0.0).to(args.device)\n","      for c in range(num_classes):\n","        batch_img_real = get_images(c, args.batch_real)\n","        batch_lab_real = torch.ones((batch_img_real.shape[0],), device=args.device, dtype=torch.long) * c\n","        img_syn = image_syn[c*args.ipc:(c+1)*args.ipc].reshape((args.ipc, channel, im_size[0], im_size[1]))\n","        lab_syn = torch.ones((args.ipc,), device=args.device, dtype=torch.long) * c\n","\n","        output_real = net(batch_img_real)\n","        loss_real = criterion(output_real, batch_lab_real)\n","        gw_real = torch.autograd.grad(loss_real, net_parameters)\n","        gw_real = list((_.detach().clone() for _ in gw_real))\n","\n","        output_syn = net(img_syn)\n","        loss_syn = criterion(output_syn, lab_syn)\n","        gw_syn = torch.autograd.grad(loss_syn, net_parameters, create_graph=True)\n","\n","        #compare gradient\n","        loss += utils.match_loss(gw_syn, gw_real, args)\n","      # update synthetic dataset with SGD\n","      optimizer_img.zero_grad()\n","      loss.backward()\n","      optimizer_img.step()\n","      torch.cuda.empty_cache()  # Clear cached memory\n","\n","      ''' update network '''\n","      image_syn_train, label_syn_train = copy.deepcopy(image_syn.detach()), copy.deepcopy(label_syn.detach())  # avoid any unaware modification\n","      dst_syn_train = utils.TensorDataset(image_syn_train, label_syn_train)\n","      trainloader = torch.utils.data.DataLoader(dst_syn_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n","\n","\n","      # inner_loop to update model weight\n","      for il in range(inner_loop):\n","          utils.epoch('train', trainloader, net, optimizer_net, criterion, args, aug = True if args.dsa else False)\n","\n","    # synthetic dataset evaluation\n","    if it == args.Iteration:\n","      args.epoch_eval_train = 200\n","      torch.cuda.empty_cache()\n","      for model in models:\n","        accuracy = []\n","        #loop over number of random model initialization\n","        for eval in range(args.num_eval):\n","          # load a network\n","          image_syn_eval, label_syn_eval = copy.deepcopy(image_syn.detach()), copy.deepcopy(label_syn.detach()) # avoid any unaware modification\n","          net = utils.get_network(model, channel, num_classes, im_size).to(args.device)\n","          _, acc_train, acc_test = utils.evaluate_synset(eval, net, image_syn_eval, label_syn_eval, testloader, args)\n","          accuracy.append(acc_test)\n","\n","        print('Evaluate synthetic data on model: %s, mean accuracy = %.4f'%(model, np.mean(accuracy)))\n","        records[model] += accuracy\n","      ''' save and visualize'''\n","      save_name = os.path.join(args.save_path, 'vis_%s_%s_%s_%dipc_iter%d.png'%(args.method, args.dataset, args.model, args.ipc, it))\n","      #image_syn_vis = copy.deepcopy(image_syn.detach().cpu())\n","      if not tf.io.gfile.exists(args.save_path):\n","        tf.io.gfile.mkdir(args.save_path)\n","      save_image(image_syn, save_name, nrow=args.ipc)\n","\n","  for model in models:\n","    accuracy = records[model]\n","    print(\"model = %s, accuracy= %.2f\"%(args.model, np.mean(accuracy)*100))\n","  return image_syn, label_syn\n","\n"],"metadata":{"id":"APFITyonuI45"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## MNIST"],"metadata":{"id":"YHf2EeyJysGM"}},{"cell_type":"code","source":["#args = type('', (), {})()\n","args.save_path = 'result'\n","args.dataset = 'MNIST'\n","args.eval_mode = 'S'\n","args.init = 'real'\n","args.data_path = './data'\n","args.dis_metric = 'ours'\n","args.device = 'cuda'\n","args.model = 'ConvNetD3'\n","args.num_eval=1\n","\n","outer_loop = 50\n","inner_loop = 1\n","args.Iteration = 10\n","args.batch_train = 256\n","args.batch_real = 256\n","args.num_eval = 200\n","args.ipc = 10 # image per class\n","args.lr_net = 0.01 # learning rate of the network\n","args.lr_img = 0.1 # learning rate of synthetic dataset\n","\n","image_syn, label_syn = data_condensation(outer_loop,inner_loop)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0aUq47Q48ri","executionInfo":{"status":"ok","timestamp":1701138557153,"user_tz":300,"elapsed":841403,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"f1f542d9-89e5-40f7-fe6b-36503dbaf5ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["initialize synthetic data from random real images\n","[2023-11-28 02:15:42] training begins\n","[2023-11-28 02:20:05] Evaluate_00: epoch = 0200 train time = 2 s train loss = 0.004958 train acc = 1.0000, test acc = 0.9318\n","[2023-11-28 02:20:11] Evaluate_01: epoch = 0200 train time = 2 s train loss = 0.004847 train acc = 1.0000, test acc = 0.9348\n","[2023-11-28 02:20:16] Evaluate_02: epoch = 0200 train time = 2 s train loss = 0.004741 train acc = 1.0000, test acc = 0.9312\n","[2023-11-28 02:20:22] Evaluate_03: epoch = 0200 train time = 2 s train loss = 0.005147 train acc = 1.0000, test acc = 0.9324\n","[2023-11-28 02:20:27] Evaluate_04: epoch = 0200 train time = 2 s train loss = 0.004672 train acc = 1.0000, test acc = 0.9322\n","[2023-11-28 02:20:33] Evaluate_05: epoch = 0200 train time = 2 s train loss = 0.004716 train acc = 1.0000, test acc = 0.9333\n","[2023-11-28 02:20:39] Evaluate_06: epoch = 0200 train time = 2 s train loss = 0.005552 train acc = 1.0000, test acc = 0.9279\n","[2023-11-28 02:20:44] Evaluate_07: epoch = 0200 train time = 2 s train loss = 0.004969 train acc = 1.0000, test acc = 0.9332\n","[2023-11-28 02:20:50] Evaluate_08: epoch = 0200 train time = 2 s train loss = 0.004974 train acc = 1.0000, test acc = 0.9327\n","[2023-11-28 02:20:55] Evaluate_09: epoch = 0200 train time = 2 s train loss = 0.005206 train acc = 1.0000, test acc = 0.9330\n","[2023-11-28 02:21:00] Evaluate_10: epoch = 0200 train time = 2 s train loss = 0.005075 train acc = 1.0000, test acc = 0.9321\n","[2023-11-28 02:21:07] Evaluate_11: epoch = 0200 train time = 2 s train loss = 0.004870 train acc = 1.0000, test acc = 0.9365\n","[2023-11-28 02:21:12] Evaluate_12: epoch = 0200 train time = 2 s train loss = 0.004821 train acc = 1.0000, test acc = 0.9318\n","[2023-11-28 02:21:17] Evaluate_13: epoch = 0200 train time = 2 s train loss = 0.004838 train acc = 1.0000, test acc = 0.9353\n","[2023-11-28 02:21:23] Evaluate_14: epoch = 0200 train time = 2 s train loss = 0.004866 train acc = 1.0000, test acc = 0.9341\n","[2023-11-28 02:21:28] Evaluate_15: epoch = 0200 train time = 2 s train loss = 0.004814 train acc = 1.0000, test acc = 0.9354\n","[2023-11-28 02:21:34] Evaluate_16: epoch = 0200 train time = 2 s train loss = 0.004682 train acc = 1.0000, test acc = 0.9337\n","[2023-11-28 02:21:40] Evaluate_17: epoch = 0200 train time = 2 s train loss = 0.005193 train acc = 1.0000, test acc = 0.9324\n","[2023-11-28 02:21:45] Evaluate_18: epoch = 0200 train time = 2 s train loss = 0.004831 train acc = 1.0000, test acc = 0.9344\n","[2023-11-28 02:21:50] Evaluate_19: epoch = 0200 train time = 2 s train loss = 0.004940 train acc = 1.0000, test acc = 0.9333\n","[2023-11-28 02:21:56] Evaluate_20: epoch = 0200 train time = 2 s train loss = 0.004723 train acc = 1.0000, test acc = 0.9362\n","[2023-11-28 02:22:02] Evaluate_21: epoch = 0200 train time = 2 s train loss = 0.005014 train acc = 1.0000, test acc = 0.9322\n","[2023-11-28 02:22:07] Evaluate_22: epoch = 0200 train time = 2 s train loss = 0.005046 train acc = 1.0000, test acc = 0.9311\n","[2023-11-28 02:22:13] Evaluate_23: epoch = 0200 train time = 2 s train loss = 0.004796 train acc = 1.0000, test acc = 0.9360\n","[2023-11-28 02:22:18] Evaluate_24: epoch = 0200 train time = 2 s train loss = 0.004932 train acc = 1.0000, test acc = 0.9347\n","[2023-11-28 02:22:24] Evaluate_25: epoch = 0200 train time = 2 s train loss = 0.005036 train acc = 1.0000, test acc = 0.9317\n","[2023-11-28 02:22:30] Evaluate_26: epoch = 0200 train time = 2 s train loss = 0.005082 train acc = 1.0000, test acc = 0.9322\n","[2023-11-28 02:22:35] Evaluate_27: epoch = 0200 train time = 2 s train loss = 0.005044 train acc = 1.0000, test acc = 0.9324\n","[2023-11-28 02:22:41] Evaluate_28: epoch = 0200 train time = 2 s train loss = 0.004837 train acc = 1.0000, test acc = 0.9326\n","[2023-11-28 02:22:46] Evaluate_29: epoch = 0200 train time = 2 s train loss = 0.004647 train acc = 1.0000, test acc = 0.9340\n","[2023-11-28 02:22:51] Evaluate_30: epoch = 0200 train time = 2 s train loss = 0.005043 train acc = 1.0000, test acc = 0.9327\n","[2023-11-28 02:22:58] Evaluate_31: epoch = 0200 train time = 2 s train loss = 0.005031 train acc = 1.0000, test acc = 0.9318\n","[2023-11-28 02:23:03] Evaluate_32: epoch = 0200 train time = 2 s train loss = 0.005103 train acc = 1.0000, test acc = 0.9337\n","[2023-11-28 02:23:08] Evaluate_33: epoch = 0200 train time = 2 s train loss = 0.005002 train acc = 1.0000, test acc = 0.9341\n","[2023-11-28 02:23:14] Evaluate_34: epoch = 0200 train time = 2 s train loss = 0.005040 train acc = 1.0000, test acc = 0.9328\n","[2023-11-28 02:23:19] Evaluate_35: epoch = 0200 train time = 2 s train loss = 0.005024 train acc = 1.0000, test acc = 0.9357\n","[2023-11-28 02:23:25] Evaluate_36: epoch = 0200 train time = 2 s train loss = 0.004769 train acc = 1.0000, test acc = 0.9379\n","[2023-11-28 02:23:31] Evaluate_37: epoch = 0200 train time = 2 s train loss = 0.004808 train acc = 1.0000, test acc = 0.9329\n","[2023-11-28 02:23:36] Evaluate_38: epoch = 0200 train time = 2 s train loss = 0.004922 train acc = 1.0000, test acc = 0.9323\n","[2023-11-28 02:23:42] Evaluate_39: epoch = 0200 train time = 2 s train loss = 0.005032 train acc = 1.0000, test acc = 0.9326\n","[2023-11-28 02:23:47] Evaluate_40: epoch = 0200 train time = 2 s train loss = 0.004655 train acc = 1.0000, test acc = 0.9332\n","[2023-11-28 02:23:52] Evaluate_41: epoch = 0200 train time = 2 s train loss = 0.005157 train acc = 1.0000, test acc = 0.9303\n","[2023-11-28 02:23:58] Evaluate_42: epoch = 0200 train time = 2 s train loss = 0.004886 train acc = 1.0000, test acc = 0.9365\n","[2023-11-28 02:24:04] Evaluate_43: epoch = 0200 train time = 2 s train loss = 0.005038 train acc = 1.0000, test acc = 0.9352\n","[2023-11-28 02:24:09] Evaluate_44: epoch = 0200 train time = 2 s train loss = 0.004796 train acc = 1.0000, test acc = 0.9327\n","[2023-11-28 02:24:15] Evaluate_45: epoch = 0200 train time = 2 s train loss = 0.004508 train acc = 1.0000, test acc = 0.9341\n","[2023-11-28 02:24:20] Evaluate_46: epoch = 0200 train time = 2 s train loss = 0.004932 train acc = 1.0000, test acc = 0.9311\n","[2023-11-28 02:24:26] Evaluate_47: epoch = 0200 train time = 2 s train loss = 0.004870 train acc = 1.0000, test acc = 0.9340\n","[2023-11-28 02:24:32] Evaluate_48: epoch = 0200 train time = 2 s train loss = 0.004695 train acc = 1.0000, test acc = 0.9337\n","[2023-11-28 02:24:37] Evaluate_49: epoch = 0200 train time = 2 s train loss = 0.004811 train acc = 1.0000, test acc = 0.9360\n","[2023-11-28 02:24:42] Evaluate_50: epoch = 0200 train time = 2 s train loss = 0.005020 train acc = 1.0000, test acc = 0.9310\n","[2023-11-28 02:24:49] Evaluate_51: epoch = 0200 train time = 2 s train loss = 0.004888 train acc = 1.0000, test acc = 0.9324\n","[2023-11-28 02:24:54] Evaluate_52: epoch = 0200 train time = 2 s train loss = 0.004807 train acc = 1.0000, test acc = 0.9334\n","[2023-11-28 02:24:59] Evaluate_53: epoch = 0200 train time = 2 s train loss = 0.005119 train acc = 1.0000, test acc = 0.9308\n","[2023-11-28 02:25:05] Evaluate_54: epoch = 0200 train time = 2 s train loss = 0.004900 train acc = 1.0000, test acc = 0.9306\n","[2023-11-28 02:25:10] Evaluate_55: epoch = 0200 train time = 2 s train loss = 0.004582 train acc = 1.0000, test acc = 0.9307\n","[2023-11-28 02:25:16] Evaluate_56: epoch = 0200 train time = 2 s train loss = 0.004636 train acc = 1.0000, test acc = 0.9334\n","[2023-11-28 02:25:22] Evaluate_57: epoch = 0200 train time = 2 s train loss = 0.004744 train acc = 1.0000, test acc = 0.9346\n","[2023-11-28 02:25:27] Evaluate_58: epoch = 0200 train time = 2 s train loss = 0.004956 train acc = 1.0000, test acc = 0.9334\n","[2023-11-28 02:25:33] Evaluate_59: epoch = 0200 train time = 2 s train loss = 0.004845 train acc = 1.0000, test acc = 0.9333\n","[2023-11-28 02:25:38] Evaluate_60: epoch = 0200 train time = 2 s train loss = 0.004534 train acc = 1.0000, test acc = 0.9367\n","[2023-11-28 02:25:44] Evaluate_61: epoch = 0200 train time = 2 s train loss = 0.004795 train acc = 1.0000, test acc = 0.9332\n","[2023-11-28 02:25:50] Evaluate_62: epoch = 0200 train time = 2 s train loss = 0.005198 train acc = 1.0000, test acc = 0.9295\n","[2023-11-28 02:25:55] Evaluate_63: epoch = 0200 train time = 2 s train loss = 0.004940 train acc = 1.0000, test acc = 0.9338\n","[2023-11-28 02:26:00] Evaluate_64: epoch = 0200 train time = 2 s train loss = 0.005184 train acc = 1.0000, test acc = 0.9331\n","[2023-11-28 02:26:07] Evaluate_65: epoch = 0200 train time = 2 s train loss = 0.004814 train acc = 1.0000, test acc = 0.9293\n","[2023-11-28 02:26:12] Evaluate_66: epoch = 0200 train time = 2 s train loss = 0.004574 train acc = 1.0000, test acc = 0.9345\n","[2023-11-28 02:26:17] Evaluate_67: epoch = 0200 train time = 2 s train loss = 0.004935 train acc = 1.0000, test acc = 0.9340\n","[2023-11-28 02:26:23] Evaluate_68: epoch = 0200 train time = 2 s train loss = 0.004840 train acc = 1.0000, test acc = 0.9342\n","[2023-11-28 02:26:28] Evaluate_69: epoch = 0200 train time = 2 s train loss = 0.004873 train acc = 1.0000, test acc = 0.9338\n","[2023-11-28 02:26:34] Evaluate_70: epoch = 0200 train time = 2 s train loss = 0.005042 train acc = 1.0000, test acc = 0.9355\n","[2023-11-28 02:26:40] Evaluate_71: epoch = 0200 train time = 2 s train loss = 0.004405 train acc = 1.0000, test acc = 0.9356\n","[2023-11-28 02:26:45] Evaluate_72: epoch = 0200 train time = 2 s train loss = 0.005391 train acc = 1.0000, test acc = 0.9315\n","[2023-11-28 02:26:51] Evaluate_73: epoch = 0200 train time = 2 s train loss = 0.005008 train acc = 1.0000, test acc = 0.9330\n","[2023-11-28 02:26:56] Evaluate_74: epoch = 0200 train time = 2 s train loss = 0.004903 train acc = 1.0000, test acc = 0.9328\n","[2023-11-28 02:27:02] Evaluate_75: epoch = 0200 train time = 2 s train loss = 0.004996 train acc = 1.0000, test acc = 0.9322\n","[2023-11-28 02:27:08] Evaluate_76: epoch = 0200 train time = 2 s train loss = 0.005007 train acc = 1.0000, test acc = 0.9343\n","[2023-11-28 02:27:13] Evaluate_77: epoch = 0200 train time = 2 s train loss = 0.004857 train acc = 1.0000, test acc = 0.9306\n","[2023-11-28 02:27:18] Evaluate_78: epoch = 0200 train time = 2 s train loss = 0.005040 train acc = 1.0000, test acc = 0.9341\n","[2023-11-28 02:27:24] Evaluate_79: epoch = 0200 train time = 2 s train loss = 0.004705 train acc = 1.0000, test acc = 0.9324\n","[2023-11-28 02:27:30] Evaluate_80: epoch = 0200 train time = 2 s train loss = 0.004910 train acc = 1.0000, test acc = 0.9287\n","[2023-11-28 02:27:35] Evaluate_81: epoch = 0200 train time = 2 s train loss = 0.005083 train acc = 1.0000, test acc = 0.9344\n","[2023-11-28 02:27:41] Evaluate_82: epoch = 0200 train time = 2 s train loss = 0.004682 train acc = 1.0000, test acc = 0.9356\n","[2023-11-28 02:27:46] Evaluate_83: epoch = 0200 train time = 2 s train loss = 0.004758 train acc = 1.0000, test acc = 0.9365\n","[2023-11-28 02:27:52] Evaluate_84: epoch = 0200 train time = 2 s train loss = 0.004726 train acc = 1.0000, test acc = 0.9304\n","[2023-11-28 02:27:58] Evaluate_85: epoch = 0200 train time = 2 s train loss = 0.005158 train acc = 1.0000, test acc = 0.9300\n","[2023-11-28 02:28:03] Evaluate_86: epoch = 0200 train time = 2 s train loss = 0.004826 train acc = 1.0000, test acc = 0.9336\n","[2023-11-28 02:28:09] Evaluate_87: epoch = 0200 train time = 2 s train loss = 0.004903 train acc = 1.0000, test acc = 0.9303\n","[2023-11-28 02:28:15] Evaluate_88: epoch = 0200 train time = 2 s train loss = 0.004939 train acc = 1.0000, test acc = 0.9325\n","[2023-11-28 02:28:20] Evaluate_89: epoch = 0200 train time = 2 s train loss = 0.005049 train acc = 1.0000, test acc = 0.9320\n","[2023-11-28 02:28:26] Evaluate_90: epoch = 0200 train time = 2 s train loss = 0.005063 train acc = 1.0000, test acc = 0.9323\n","[2023-11-28 02:28:31] Evaluate_91: epoch = 0200 train time = 2 s train loss = 0.005027 train acc = 1.0000, test acc = 0.9330\n","[2023-11-28 02:28:36] Evaluate_92: epoch = 0200 train time = 2 s train loss = 0.005132 train acc = 1.0000, test acc = 0.9303\n","[2023-11-28 02:28:42] Evaluate_93: epoch = 0200 train time = 2 s train loss = 0.004671 train acc = 1.0000, test acc = 0.9316\n","[2023-11-28 02:28:48] Evaluate_94: epoch = 0200 train time = 2 s train loss = 0.004925 train acc = 1.0000, test acc = 0.9314\n","[2023-11-28 02:28:53] Evaluate_95: epoch = 0200 train time = 2 s train loss = 0.004933 train acc = 1.0000, test acc = 0.9311\n","[2023-11-28 02:28:59] Evaluate_96: epoch = 0200 train time = 2 s train loss = 0.004980 train acc = 1.0000, test acc = 0.9278\n","[2023-11-28 02:29:04] Evaluate_97: epoch = 0200 train time = 2 s train loss = 0.004823 train acc = 1.0000, test acc = 0.9320\n","[2023-11-28 02:29:10] Evaluate_98: epoch = 0200 train time = 2 s train loss = 0.004795 train acc = 1.0000, test acc = 0.9361\n","[2023-11-28 02:29:16] Evaluate_99: epoch = 0200 train time = 2 s train loss = 0.004818 train acc = 1.0000, test acc = 0.9325\n","Evaluate synthetic data on model: ConvNetD3, mean accuracy = 0.9330\n","model = ConvNetD3, accuracy= 93.30\n"]}]},{"cell_type":"code","source":["import io\n","torch.save(image_syn, 'image_syn_mnist.pt')\n","# Save to io.BytesIO buffer\n","buffer = io.BytesIO()\n","torch.save(image_syn, buffer)\n","\n","torch.save(label_syn, 'label_syn_mnist.pt')\n","# Save to io.BytesIO buffer\n","buffer = io.BytesIO()\n","torch.save(label_syn, buffer)"],"metadata":{"id":"Sxw_8Rou5N1t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## MHIST"],"metadata":{"id":"bUfiXxKJA-Es"}},{"cell_type":"code","source":["args = type('', (), {})()\n","args.save_path = 'result'\n","args.model = 'ConvNet'\n","args.dataset = 'MHIST'\n","args.eval_mode = 'S'\n","args.init = 'real'\n","args.data_path = './data'\n","args.dis_metric = 'ours'\n","args.device = 'cuda'\n","args.method = 'DC'\n","args.dsa_param = utils.ParamDiffAug()\n","args.dsa = True if args.method == 'DSA' else False\n","args.dc_aug_param = None\n","\n","args.num_eval = 200 # number of random initialized weights\n","args.Iteration = 10 #Iteration\n","outer_loop = 50\n","inner_loop = 1\n","args.batch_train = 128\n","args.batch_real = 128\n","#args.num_eval = 3\n","args.lr_net = 0.01 # learning rate of the network\n","args.lr_img = 0.1 # learning rate of synthetic dataset\n","args.ipc = 50 # image per class\n","\n","image_syn, label_syn = data_condensation(outer_loop,inner_loop)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0M3LhBbW5R6T","executionInfo":{"status":"ok","timestamp":1700951044063,"user_tz":300,"elapsed":121263,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"98f6b5e8-44fa-4da6-ed12-89d26a3908f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["initialize synthetic data from random real images\n","[2023-11-25 22:22:32] training begins\n","[2023-11-25 22:23:49] Evaluate_00: epoch = 0200 train time = 2 s train loss = 0.001953 train acc = 1.0000, test acc = 0.7277\n","[2023-11-25 22:23:56] Evaluate_01: epoch = 0200 train time = 3 s train loss = 0.001997 train acc = 1.0000, test acc = 0.7308\n","[2023-11-25 22:24:03] Evaluate_02: epoch = 0200 train time = 3 s train loss = 0.001832 train acc = 1.0000, test acc = 0.7349\n","Evaluate synthetic data on model: ConvNet, mean accuracy = 0.7311\n","model = ConvNet, accuracy= 73.11\n"]}]},{"cell_type":"code","source":["import io\n","torch.save(image_syn, 'image_syn.pt')\n","# Save to io.BytesIO buffer\n","buffer = io.BytesIO()\n","torch.save(image_syn, buffer)\n","\n","torch.save(label_syn, 'label_syn.pt')\n","# Save to io.BytesIO buffer\n","buffer = io.BytesIO()\n","torch.save(label_syn, buffer)"],"metadata":{"id":"RD1g19it2oZB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Gaussian noise Synthetic Data Training"],"metadata":{"id":"FjCZDmiuUrLY"}},{"cell_type":"markdown","source":["##MNIST"],"metadata":{"id":"DqB8TJ0eBTkA"}},{"cell_type":"code","source":["args.init = 'noise'\n","args.save_path = 'result'\n","args.dataset = 'MNIST'\n","args.eval_mode = 'S'\n","args.data_path = './data'\n","args.dis_metric = 'ours'\n","args.device = 'cuda'\n","args.model = 'ConvNetD3'\n","args.num_eval=100\n","\n","outer_loop = 10\n","inner_loop = 50\n","args.Iteration = 10\n","args.batch_train = 256\n","args.batch_real = 256\n","args.num_eval = 1\n","args.ipc = 10 # image per class\n","args.lr_net = 0.01 # learning rate of the network\n","args.lr_img = 0.1 # learning rate of synthetic dataset\n","\n","image_syn_noise_mnist, label_syn_noise_mnist = data_condensation(outer_loop,inner_loop)"],"metadata":{"id":"Y58v325gUty2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700951803458,"user_tz":300,"elapsed":164116,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"950e5a2a-be06-4088-b737-daf42a0a81b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["initialize synthetic data from random noise\n","[2023-11-25 22:34:29] training begins\n","[2023-11-25 22:36:43] Evaluate_00: epoch = 0200 train time = 2 s train loss = 0.007150 train acc = 1.0000, test acc = 0.8563\n","Evaluate synthetic data on model: ConvNetD3, mean accuracy = 0.8563\n","model = ConvNetD3, accuracy= 85.63\n"]}]},{"cell_type":"markdown","source":["## MHIST"],"metadata":{"id":"82aT3nlhBXaF"}},{"cell_type":"code","source":["args = type('', (), {})()\n","args.save_path = 'result'\n","args.model = 'ConvNet'\n","args.dataset = 'MHIST'\n","args.eval_mode = 'S'\n","args.init = 'noise'\n","args.data_path = './data'\n","args.dis_metric = 'ours'\n","args.device = 'cuda'\n","args.method = 'DC'\n","args.dsa_param = utils.ParamDiffAug()\n","args.dsa = True if args.method == 'DSA' else False\n","args.dc_aug_param = None\n","\n","args.num_eval = 200 # number of random initialized weights\n","args.Iteration = 10 #Iteration\n","outer_loop = 50\n","inner_loop = 1\n","args.batch_train = 128\n","args.batch_real = 128\n","#args.num_eval = 3\n","args.lr_net = 0.01 # learning rate of the network\n","args.lr_img = 0.1 # learning rate of synthetic dataset\n","args.ipc = 50 # image per class\n","\n","image_syn_noise_mhist, label_syn_noise_mhist = data_condensation(outer_loop,inner_loop)"],"metadata":{"id":"pRAHa3AMU2uk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700952318299,"user_tz":300,"elapsed":120285,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"52218f2e-c0c4-4b65-b5ae-4fcaf5da1c77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["initialize synthetic data from random noise\n","[2023-11-25 22:43:46] training begins\n","[2023-11-25 22:45:03] Evaluate_00: epoch = 0200 train time = 2 s train loss = 0.001096 train acc = 1.0000, test acc = 0.6325\n","[2023-11-25 22:45:10] Evaluate_01: epoch = 0200 train time = 2 s train loss = 0.001192 train acc = 1.0000, test acc = 0.6448\n","[2023-11-25 22:45:18] Evaluate_02: epoch = 0200 train time = 3 s train loss = 0.001177 train acc = 1.0000, test acc = 0.6264\n","Evaluate synthetic data on model: ConvNet, mean accuracy = 0.6346\n","model = ConvNet, accuracy= 63.46\n"]}]},{"cell_type":"markdown","source":["# Train Network from Scratch with Synthetic Dataset"],"metadata":{"id":"Lh9nAIEVVBfO"}},{"cell_type":"code","source":["image_syn_noise_mhist = torch.load('image_syn.pt')\n","label_syn_noise_mhist = torch.load('label_syn.pt')\n","\n","image_syn_noise_mnist = torch.load('image_syn_mnist.pt')\n","label_syn_noise_mnist = torch.load('label_syn_mnist.pt')"],"metadata":{"id":"x6VDa3NNqo7r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## MNIST"],"metadata":{"id":"mKkbfL4AB8If"}},{"cell_type":"code","source":["args.dataset = 'MNIST'\n","args.epoch_train = 200\n","channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = utils.get_dataset(args.dataset, args.data_path)\n","it_eval = args.num_eval\n","args.model = 'ConvNetD3'\n","net = utils.get_network(args.model,channel=channel,num_classes=num_classes,im_size=im_size).to(args.device)\n","\n","images_train = image_syn_noise_mnist.to(args.device)\n","labels_train = label_syn_noise_mnist.to(args.device)\n","dst_train = utils.TensorDataset(images_train, labels_train)\n","trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n","\n","it_eval = 1\n","train_acc, test_acc = evaluate_dataset(it_eval, net, trainloader, testloader, args)\n","#accuracy.append(test_acc)\n","\n","# Print or process the accuracy as needed\n","print(f'Final Test Accuracy: {test_acc:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e7As48aXVHL3","executionInfo":{"status":"ok","timestamp":1700952641753,"user_tz":300,"elapsed":6769,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"5b8bd698-1c1f-4167-d03c-c33808a6c56a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 Training Accuracy: 0.1100, Training loss: 2.3617\n","1 Training Accuracy: 0.2600, Training loss: 2.2205\n","1 Training Accuracy: 0.4600, Training loss: 2.1216\n","1 Training Accuracy: 0.6800, Training loss: 2.0376\n","1 Training Accuracy: 0.7300, Training loss: 1.9609\n","1 Training Accuracy: 0.7800, Training loss: 1.8892\n","1 Training Accuracy: 0.8100, Training loss: 1.8223\n","1 Training Accuracy: 0.8200, Training loss: 1.7597\n","1 Training Accuracy: 0.8400, Training loss: 1.7009\n","1 Training Accuracy: 0.8600, Training loss: 1.6454\n","1 Training Accuracy: 0.8700, Training loss: 1.5922\n","1 Training Accuracy: 0.9100, Training loss: 1.5414\n","1 Training Accuracy: 0.9300, Training loss: 1.4933\n","1 Training Accuracy: 0.9600, Training loss: 1.4471\n","1 Training Accuracy: 0.9700, Training loss: 1.4027\n","1 Training Accuracy: 0.9700, Training loss: 1.3603\n","1 Training Accuracy: 0.9700, Training loss: 1.3195\n","1 Training Accuracy: 0.9800, Training loss: 1.2803\n","1 Training Accuracy: 0.9800, Training loss: 1.2425\n","1 Training Accuracy: 0.9800, Training loss: 1.2062\n","1 Training Accuracy: 0.9800, Training loss: 1.1712\n","1 Training Accuracy: 0.9800, Training loss: 1.1376\n","1 Training Accuracy: 0.9800, Training loss: 1.1050\n","1 Training Accuracy: 0.9800, Training loss: 1.0736\n","1 Training Accuracy: 0.9800, Training loss: 1.0434\n","1 Training Accuracy: 0.9800, Training loss: 1.0143\n","1 Training Accuracy: 0.9800, Training loss: 0.9864\n","1 Training Accuracy: 0.9800, Training loss: 0.9595\n","1 Training Accuracy: 0.9900, Training loss: 0.9335\n","1 Training Accuracy: 1.0000, Training loss: 0.9086\n","1 Training Accuracy: 1.0000, Training loss: 0.8846\n","1 Training Accuracy: 1.0000, Training loss: 0.8616\n","1 Training Accuracy: 1.0000, Training loss: 0.8393\n","1 Training Accuracy: 1.0000, Training loss: 0.8179\n","1 Training Accuracy: 1.0000, Training loss: 0.7974\n","1 Training Accuracy: 1.0000, Training loss: 0.7776\n","1 Training Accuracy: 1.0000, Training loss: 0.7585\n","1 Training Accuracy: 1.0000, Training loss: 0.7400\n","1 Training Accuracy: 1.0000, Training loss: 0.7222\n","1 Training Accuracy: 1.0000, Training loss: 0.7049\n","1 Training Accuracy: 1.0000, Training loss: 0.6883\n","1 Training Accuracy: 1.0000, Training loss: 0.6722\n","1 Training Accuracy: 1.0000, Training loss: 0.6567\n","1 Training Accuracy: 1.0000, Training loss: 0.6418\n","1 Training Accuracy: 1.0000, Training loss: 0.6274\n","1 Training Accuracy: 1.0000, Training loss: 0.6136\n","1 Training Accuracy: 1.0000, Training loss: 0.6002\n","1 Training Accuracy: 1.0000, Training loss: 0.5874\n","1 Training Accuracy: 1.0000, Training loss: 0.5750\n","1 Training Accuracy: 1.0000, Training loss: 0.5630\n","1 Training Accuracy: 1.0000, Training loss: 0.5515\n","1 Training Accuracy: 1.0000, Training loss: 0.5403\n","1 Training Accuracy: 1.0000, Training loss: 0.5296\n","1 Training Accuracy: 1.0000, Training loss: 0.5192\n","1 Training Accuracy: 1.0000, Training loss: 0.5091\n","1 Training Accuracy: 1.0000, Training loss: 0.4994\n","1 Training Accuracy: 1.0000, Training loss: 0.4899\n","1 Training Accuracy: 1.0000, Training loss: 0.4808\n","1 Training Accuracy: 1.0000, Training loss: 0.4719\n","1 Training Accuracy: 1.0000, Training loss: 0.4634\n","1 Training Accuracy: 1.0000, Training loss: 0.4551\n","1 Training Accuracy: 1.0000, Training loss: 0.4471\n","1 Training Accuracy: 1.0000, Training loss: 0.4394\n","1 Training Accuracy: 1.0000, Training loss: 0.4319\n","1 Training Accuracy: 1.0000, Training loss: 0.4247\n","1 Training Accuracy: 1.0000, Training loss: 0.4176\n","1 Training Accuracy: 1.0000, Training loss: 0.4107\n","1 Training Accuracy: 1.0000, Training loss: 0.4040\n","1 Training Accuracy: 1.0000, Training loss: 0.3976\n","1 Training Accuracy: 1.0000, Training loss: 0.3914\n","1 Training Accuracy: 1.0000, Training loss: 0.3854\n","1 Training Accuracy: 1.0000, Training loss: 0.3795\n","1 Training Accuracy: 1.0000, Training loss: 0.3739\n","1 Training Accuracy: 1.0000, Training loss: 0.3684\n","1 Training Accuracy: 1.0000, Training loss: 0.3631\n","1 Training Accuracy: 1.0000, Training loss: 0.3580\n","1 Training Accuracy: 1.0000, Training loss: 0.3530\n","1 Training Accuracy: 1.0000, Training loss: 0.3481\n","1 Training Accuracy: 1.0000, Training loss: 0.3434\n","1 Training Accuracy: 1.0000, Training loss: 0.3389\n","1 Training Accuracy: 1.0000, Training loss: 0.3345\n","1 Training Accuracy: 1.0000, Training loss: 0.3302\n","1 Training Accuracy: 1.0000, Training loss: 0.3261\n","1 Training Accuracy: 1.0000, Training loss: 0.3221\n","1 Training Accuracy: 1.0000, Training loss: 0.3182\n","1 Training Accuracy: 1.0000, Training loss: 0.3144\n","1 Training Accuracy: 1.0000, Training loss: 0.3107\n","1 Training Accuracy: 1.0000, Training loss: 0.3071\n","1 Training Accuracy: 1.0000, Training loss: 0.3036\n","1 Training Accuracy: 1.0000, Training loss: 0.3003\n","1 Training Accuracy: 1.0000, Training loss: 0.2970\n","1 Training Accuracy: 1.0000, Training loss: 0.2938\n","1 Training Accuracy: 1.0000, Training loss: 0.2908\n","1 Training Accuracy: 1.0000, Training loss: 0.2878\n","1 Training Accuracy: 1.0000, Training loss: 0.2849\n","1 Training Accuracy: 1.0000, Training loss: 0.2821\n","1 Training Accuracy: 1.0000, Training loss: 0.2793\n","1 Training Accuracy: 1.0000, Training loss: 0.2767\n","1 Training Accuracy: 1.0000, Training loss: 0.2741\n","1 Training Accuracy: 1.0000, Training loss: 0.2716\n","1 Training Accuracy: 1.0000, Training loss: 0.2692\n","1 Training Accuracy: 1.0000, Training loss: 0.2668\n","1 Training Accuracy: 1.0000, Training loss: 0.2645\n","1 Training Accuracy: 1.0000, Training loss: 0.2623\n","1 Training Accuracy: 1.0000, Training loss: 0.2601\n","1 Training Accuracy: 1.0000, Training loss: 0.2580\n","1 Training Accuracy: 1.0000, Training loss: 0.2560\n","1 Training Accuracy: 1.0000, Training loss: 0.2540\n","1 Training Accuracy: 1.0000, Training loss: 0.2521\n","1 Training Accuracy: 1.0000, Training loss: 0.2502\n","1 Training Accuracy: 1.0000, Training loss: 0.2484\n","1 Training Accuracy: 1.0000, Training loss: 0.2467\n","1 Training Accuracy: 1.0000, Training loss: 0.2450\n","1 Training Accuracy: 1.0000, Training loss: 0.2433\n","1 Training Accuracy: 1.0000, Training loss: 0.2417\n","1 Training Accuracy: 1.0000, Training loss: 0.2401\n","1 Training Accuracy: 1.0000, Training loss: 0.2386\n","1 Training Accuracy: 1.0000, Training loss: 0.2371\n","1 Training Accuracy: 1.0000, Training loss: 0.2357\n","1 Training Accuracy: 1.0000, Training loss: 0.2343\n","1 Training Accuracy: 1.0000, Training loss: 0.2330\n","1 Training Accuracy: 1.0000, Training loss: 0.2317\n","1 Training Accuracy: 1.0000, Training loss: 0.2304\n","1 Training Accuracy: 1.0000, Training loss: 0.2292\n","1 Training Accuracy: 1.0000, Training loss: 0.2280\n","1 Training Accuracy: 1.0000, Training loss: 0.2268\n","1 Training Accuracy: 1.0000, Training loss: 0.2257\n","1 Training Accuracy: 1.0000, Training loss: 0.2246\n","1 Training Accuracy: 1.0000, Training loss: 0.2236\n","1 Training Accuracy: 1.0000, Training loss: 0.2226\n","1 Training Accuracy: 1.0000, Training loss: 0.2216\n","1 Training Accuracy: 1.0000, Training loss: 0.2206\n","1 Training Accuracy: 1.0000, Training loss: 0.2197\n","1 Training Accuracy: 1.0000, Training loss: 0.2188\n","1 Training Accuracy: 1.0000, Training loss: 0.2179\n","1 Training Accuracy: 1.0000, Training loss: 0.2171\n","1 Training Accuracy: 1.0000, Training loss: 0.2163\n","1 Training Accuracy: 1.0000, Training loss: 0.2155\n","1 Training Accuracy: 1.0000, Training loss: 0.2147\n","1 Training Accuracy: 1.0000, Training loss: 0.2140\n","1 Training Accuracy: 1.0000, Training loss: 0.2133\n","1 Training Accuracy: 1.0000, Training loss: 0.2126\n","1 Training Accuracy: 1.0000, Training loss: 0.2120\n","1 Training Accuracy: 1.0000, Training loss: 0.2113\n","1 Training Accuracy: 1.0000, Training loss: 0.2107\n","1 Training Accuracy: 1.0000, Training loss: 0.2102\n","1 Training Accuracy: 1.0000, Training loss: 0.2096\n","1 Training Accuracy: 1.0000, Training loss: 0.2090\n","1 Training Accuracy: 1.0000, Training loss: 0.2085\n","1 Training Accuracy: 1.0000, Training loss: 0.2080\n","1 Training Accuracy: 1.0000, Training loss: 0.2075\n","1 Training Accuracy: 1.0000, Training loss: 0.2071\n","1 Training Accuracy: 1.0000, Training loss: 0.2066\n","1 Training Accuracy: 1.0000, Training loss: 0.2062\n","1 Training Accuracy: 1.0000, Training loss: 0.2058\n","1 Training Accuracy: 1.0000, Training loss: 0.2054\n","1 Training Accuracy: 1.0000, Training loss: 0.2050\n","1 Training Accuracy: 1.0000, Training loss: 0.2047\n","1 Training Accuracy: 1.0000, Training loss: 0.2044\n","1 Training Accuracy: 1.0000, Training loss: 0.2040\n","1 Training Accuracy: 1.0000, Training loss: 0.2037\n","1 Training Accuracy: 1.0000, Training loss: 0.2034\n","1 Training Accuracy: 1.0000, Training loss: 0.2031\n","1 Training Accuracy: 1.0000, Training loss: 0.2029\n","1 Training Accuracy: 1.0000, Training loss: 0.2026\n","1 Training Accuracy: 1.0000, Training loss: 0.2024\n","1 Training Accuracy: 1.0000, Training loss: 0.2022\n","1 Training Accuracy: 1.0000, Training loss: 0.2020\n","1 Training Accuracy: 1.0000, Training loss: 0.2018\n","1 Training Accuracy: 1.0000, Training loss: 0.2016\n","1 Training Accuracy: 1.0000, Training loss: 0.2014\n","1 Training Accuracy: 1.0000, Training loss: 0.2012\n","1 Training Accuracy: 1.0000, Training loss: 0.2011\n","1 Training Accuracy: 1.0000, Training loss: 0.2009\n","1 Training Accuracy: 1.0000, Training loss: 0.2008\n","1 Training Accuracy: 1.0000, Training loss: 0.2007\n","1 Training Accuracy: 1.0000, Training loss: 0.2006\n","1 Training Accuracy: 1.0000, Training loss: 0.2005\n","1 Training Accuracy: 1.0000, Training loss: 0.2004\n","1 Training Accuracy: 1.0000, Training loss: 0.2003\n","1 Training Accuracy: 1.0000, Training loss: 0.2002\n","1 Training Accuracy: 1.0000, Training loss: 0.2001\n","1 Training Accuracy: 1.0000, Training loss: 0.2001\n","1 Training Accuracy: 1.0000, Training loss: 0.2000\n","1 Training Accuracy: 1.0000, Training loss: 0.2000\n","1 Training Accuracy: 1.0000, Training loss: 0.1999\n","1 Training Accuracy: 1.0000, Training loss: 0.1999\n","1 Training Accuracy: 1.0000, Training loss: 0.1998\n","1 Training Accuracy: 1.0000, Training loss: 0.1998\n","1 Training Accuracy: 1.0000, Training loss: 0.1998\n","1 Training Accuracy: 1.0000, Training loss: 0.1997\n","1 Training Accuracy: 1.0000, Training loss: 0.1997\n","1 Training Accuracy: 1.0000, Training loss: 0.1997\n","1 Training Accuracy: 1.0000, Training loss: 0.1997\n","1 Training Accuracy: 1.0000, Training loss: 0.1997\n","1 Training Accuracy: 1.0000, Training loss: 0.1997\n","1 Training Accuracy: 1.0000, Training loss: 0.1997\n","1 Training Accuracy: 1.0000, Training loss: 0.1997\n","1 Training Accuracy: 1.0000, Training loss: 0.1997\n","1 Training Accuracy: 1.0000, Training loss: 0.1997\n","1 Training Accuracy: 1.0000, Testing Accuracy: 0.9149\n","Final Test Accuracy: 0.9149\n"]}]},{"cell_type":"markdown","source":["## MHIST"],"metadata":{"id":"FjA8_9LqB_r5"}},{"cell_type":"code","source":["args.dataset = 'MHIST'\n","args.epoch_train = 200\n","channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = utils.get_dataset(args.dataset, args.data_path)\n","it_eval = args.num_eval\n","args.model = 'ConvNet'\n","net = utils.get_network(args.model,channel=channel,num_classes=num_classes,im_size=im_size).to(args.device)\n","\n","images_train = image_syn_noise_mhist.to(args.device)\n","labels_train = label_syn_noise_mhist.to(args.device)\n","dst_train = utils.TensorDataset(images_train, labels_train)\n","trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n","\n","it_eval = 1\n","train_acc, test_acc = evaluate_dataset(it_eval, net, trainloader, testloader, args)\n","#accuracy.append(test_acc)\n","\n","# Print or process the accuracy as needed\n","print(f'Final Test Accuracy: {test_acc:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gUnjuWtfWmkr","executionInfo":{"status":"ok","timestamp":1700952676661,"user_tz":300,"elapsed":17696,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"8cdfb995-4abc-4c46-a9a5-699d917c71d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 Training Accuracy: 0.5000, Training loss: 0.6935\n","1 Training Accuracy: 0.5400, Training loss: 0.6776\n","1 Training Accuracy: 0.5300, Training loss: 0.6688\n","1 Training Accuracy: 0.5200, Training loss: 0.6657\n","1 Training Accuracy: 0.5100, Training loss: 0.6667\n","1 Training Accuracy: 0.5200, Training loss: 0.6726\n","1 Training Accuracy: 0.5000, Training loss: 0.6778\n","1 Training Accuracy: 0.5200, Training loss: 0.6838\n","1 Training Accuracy: 0.5000, Training loss: 0.6813\n","1 Training Accuracy: 0.5200, Training loss: 0.6781\n","1 Training Accuracy: 0.5000, Training loss: 0.6642\n","1 Training Accuracy: 0.5400, Training loss: 0.6539\n","1 Training Accuracy: 0.5100, Training loss: 0.6346\n","1 Training Accuracy: 0.5400, Training loss: 0.6211\n","1 Training Accuracy: 0.5200, Training loss: 0.6006\n","1 Training Accuracy: 0.5800, Training loss: 0.5858\n","1 Training Accuracy: 0.5800, Training loss: 0.5664\n","1 Training Accuracy: 0.5900, Training loss: 0.5522\n","1 Training Accuracy: 0.6300, Training loss: 0.5348\n","1 Training Accuracy: 0.7000, Training loss: 0.5212\n","1 Training Accuracy: 0.7400, Training loss: 0.5054\n","1 Training Accuracy: 0.7200, Training loss: 0.4922\n","1 Training Accuracy: 0.8000, Training loss: 0.4779\n","1 Training Accuracy: 0.8200, Training loss: 0.4657\n","1 Training Accuracy: 0.8700, Training loss: 0.4530\n","1 Training Accuracy: 0.8700, Training loss: 0.4419\n","1 Training Accuracy: 0.9300, Training loss: 0.4308\n","1 Training Accuracy: 0.9200, Training loss: 0.4209\n","1 Training Accuracy: 0.9400, Training loss: 0.4112\n","1 Training Accuracy: 0.9600, Training loss: 0.4024\n","1 Training Accuracy: 0.9600, Training loss: 0.3942\n","1 Training Accuracy: 0.9800, Training loss: 0.3864\n","1 Training Accuracy: 0.9700, Training loss: 0.3790\n","1 Training Accuracy: 0.9800, Training loss: 0.3721\n","1 Training Accuracy: 0.9700, Training loss: 0.3655\n","1 Training Accuracy: 0.9800, Training loss: 0.3593\n","1 Training Accuracy: 0.9700, Training loss: 0.3533\n","1 Training Accuracy: 0.9900, Training loss: 0.3475\n","1 Training Accuracy: 1.0000, Training loss: 0.3418\n","1 Training Accuracy: 0.9900, Training loss: 0.3363\n","1 Training Accuracy: 1.0000, Training loss: 0.3310\n","1 Training Accuracy: 0.9900, Training loss: 0.3259\n","1 Training Accuracy: 1.0000, Training loss: 0.3208\n","1 Training Accuracy: 0.9900, Training loss: 0.3158\n","1 Training Accuracy: 1.0000, Training loss: 0.3110\n","1 Training Accuracy: 1.0000, Training loss: 0.3063\n","1 Training Accuracy: 1.0000, Training loss: 0.3017\n","1 Training Accuracy: 1.0000, Training loss: 0.2972\n","1 Training Accuracy: 1.0000, Training loss: 0.2928\n","1 Training Accuracy: 1.0000, Training loss: 0.2884\n","1 Training Accuracy: 1.0000, Training loss: 0.2842\n","1 Training Accuracy: 1.0000, Training loss: 0.2801\n","1 Training Accuracy: 1.0000, Training loss: 0.2760\n","1 Training Accuracy: 1.0000, Training loss: 0.2721\n","1 Training Accuracy: 1.0000, Training loss: 0.2682\n","1 Training Accuracy: 1.0000, Training loss: 0.2643\n","1 Training Accuracy: 1.0000, Training loss: 0.2606\n","1 Training Accuracy: 1.0000, Training loss: 0.2570\n","1 Training Accuracy: 1.0000, Training loss: 0.2534\n","1 Training Accuracy: 1.0000, Training loss: 0.2499\n","1 Training Accuracy: 1.0000, Training loss: 0.2465\n","1 Training Accuracy: 1.0000, Training loss: 0.2432\n","1 Training Accuracy: 1.0000, Training loss: 0.2399\n","1 Training Accuracy: 1.0000, Training loss: 0.2367\n","1 Training Accuracy: 1.0000, Training loss: 0.2336\n","1 Training Accuracy: 1.0000, Training loss: 0.2306\n","1 Training Accuracy: 1.0000, Training loss: 0.2276\n","1 Training Accuracy: 1.0000, Training loss: 0.2247\n","1 Training Accuracy: 1.0000, Training loss: 0.2218\n","1 Training Accuracy: 1.0000, Training loss: 0.2191\n","1 Training Accuracy: 1.0000, Training loss: 0.2163\n","1 Training Accuracy: 1.0000, Training loss: 0.2137\n","1 Training Accuracy: 1.0000, Training loss: 0.2111\n","1 Training Accuracy: 1.0000, Training loss: 0.2086\n","1 Training Accuracy: 1.0000, Training loss: 0.2061\n","1 Training Accuracy: 1.0000, Training loss: 0.2037\n","1 Training Accuracy: 1.0000, Training loss: 0.2013\n","1 Training Accuracy: 1.0000, Training loss: 0.1990\n","1 Training Accuracy: 1.0000, Training loss: 0.1968\n","1 Training Accuracy: 1.0000, Training loss: 0.1946\n","1 Training Accuracy: 1.0000, Training loss: 0.1925\n","1 Training Accuracy: 1.0000, Training loss: 0.1904\n","1 Training Accuracy: 1.0000, Training loss: 0.1883\n","1 Training Accuracy: 1.0000, Training loss: 0.1863\n","1 Training Accuracy: 1.0000, Training loss: 0.1844\n","1 Training Accuracy: 1.0000, Training loss: 0.1825\n","1 Training Accuracy: 1.0000, Training loss: 0.1806\n","1 Training Accuracy: 1.0000, Training loss: 0.1788\n","1 Training Accuracy: 1.0000, Training loss: 0.1770\n","1 Training Accuracy: 1.0000, Training loss: 0.1753\n","1 Training Accuracy: 1.0000, Training loss: 0.1736\n","1 Training Accuracy: 1.0000, Training loss: 0.1720\n","1 Training Accuracy: 1.0000, Training loss: 0.1704\n","1 Training Accuracy: 1.0000, Training loss: 0.1688\n","1 Training Accuracy: 1.0000, Training loss: 0.1673\n","1 Training Accuracy: 1.0000, Training loss: 0.1658\n","1 Training Accuracy: 1.0000, Training loss: 0.1644\n","1 Training Accuracy: 1.0000, Training loss: 0.1630\n","1 Training Accuracy: 1.0000, Training loss: 0.1616\n","1 Training Accuracy: 1.0000, Training loss: 0.1602\n","1 Training Accuracy: 1.0000, Training loss: 0.1589\n","1 Training Accuracy: 1.0000, Training loss: 0.1577\n","1 Training Accuracy: 1.0000, Training loss: 0.1564\n","1 Training Accuracy: 1.0000, Training loss: 0.1552\n","1 Training Accuracy: 1.0000, Training loss: 0.1540\n","1 Training Accuracy: 1.0000, Training loss: 0.1529\n","1 Training Accuracy: 1.0000, Training loss: 0.1518\n","1 Training Accuracy: 1.0000, Training loss: 0.1507\n","1 Training Accuracy: 1.0000, Training loss: 0.1497\n","1 Training Accuracy: 1.0000, Training loss: 0.1486\n","1 Training Accuracy: 1.0000, Training loss: 0.1476\n","1 Training Accuracy: 1.0000, Training loss: 0.1467\n","1 Training Accuracy: 1.0000, Training loss: 0.1457\n","1 Training Accuracy: 1.0000, Training loss: 0.1448\n","1 Training Accuracy: 1.0000, Training loss: 0.1439\n","1 Training Accuracy: 1.0000, Training loss: 0.1430\n","1 Training Accuracy: 1.0000, Training loss: 0.1422\n","1 Training Accuracy: 1.0000, Training loss: 0.1414\n","1 Training Accuracy: 1.0000, Training loss: 0.1406\n","1 Training Accuracy: 1.0000, Training loss: 0.1398\n","1 Training Accuracy: 1.0000, Training loss: 0.1390\n","1 Training Accuracy: 1.0000, Training loss: 0.1383\n","1 Training Accuracy: 1.0000, Training loss: 0.1376\n","1 Training Accuracy: 1.0000, Training loss: 0.1369\n","1 Training Accuracy: 1.0000, Training loss: 0.1362\n","1 Training Accuracy: 1.0000, Training loss: 0.1356\n","1 Training Accuracy: 1.0000, Training loss: 0.1349\n","1 Training Accuracy: 1.0000, Training loss: 0.1343\n","1 Training Accuracy: 1.0000, Training loss: 0.1337\n","1 Training Accuracy: 1.0000, Training loss: 0.1331\n","1 Training Accuracy: 1.0000, Training loss: 0.1326\n","1 Training Accuracy: 1.0000, Training loss: 0.1320\n","1 Training Accuracy: 1.0000, Training loss: 0.1315\n","1 Training Accuracy: 1.0000, Training loss: 0.1310\n","1 Training Accuracy: 1.0000, Training loss: 0.1305\n","1 Training Accuracy: 1.0000, Training loss: 0.1300\n","1 Training Accuracy: 1.0000, Training loss: 0.1295\n","1 Training Accuracy: 1.0000, Training loss: 0.1291\n","1 Training Accuracy: 1.0000, Training loss: 0.1287\n","1 Training Accuracy: 1.0000, Training loss: 0.1282\n","1 Training Accuracy: 1.0000, Training loss: 0.1278\n","1 Training Accuracy: 1.0000, Training loss: 0.1274\n","1 Training Accuracy: 1.0000, Training loss: 0.1271\n","1 Training Accuracy: 1.0000, Training loss: 0.1267\n","1 Training Accuracy: 1.0000, Training loss: 0.1263\n","1 Training Accuracy: 1.0000, Training loss: 0.1260\n","1 Training Accuracy: 1.0000, Training loss: 0.1257\n","1 Training Accuracy: 1.0000, Training loss: 0.1254\n","1 Training Accuracy: 1.0000, Training loss: 0.1251\n","1 Training Accuracy: 1.0000, Training loss: 0.1248\n","1 Training Accuracy: 1.0000, Training loss: 0.1245\n","1 Training Accuracy: 1.0000, Training loss: 0.1242\n","1 Training Accuracy: 1.0000, Training loss: 0.1240\n","1 Training Accuracy: 1.0000, Training loss: 0.1237\n","1 Training Accuracy: 1.0000, Training loss: 0.1235\n","1 Training Accuracy: 1.0000, Training loss: 0.1232\n","1 Training Accuracy: 1.0000, Training loss: 0.1230\n","1 Training Accuracy: 1.0000, Training loss: 0.1228\n","1 Training Accuracy: 1.0000, Training loss: 0.1226\n","1 Training Accuracy: 1.0000, Training loss: 0.1224\n","1 Training Accuracy: 1.0000, Training loss: 0.1223\n","1 Training Accuracy: 1.0000, Training loss: 0.1221\n","1 Training Accuracy: 1.0000, Training loss: 0.1219\n","1 Training Accuracy: 1.0000, Training loss: 0.1218\n","1 Training Accuracy: 1.0000, Training loss: 0.1216\n","1 Training Accuracy: 1.0000, Training loss: 0.1215\n","1 Training Accuracy: 1.0000, Training loss: 0.1213\n","1 Training Accuracy: 1.0000, Training loss: 0.1212\n","1 Training Accuracy: 1.0000, Training loss: 0.1211\n","1 Training Accuracy: 1.0000, Training loss: 0.1210\n","1 Training Accuracy: 1.0000, Training loss: 0.1209\n","1 Training Accuracy: 1.0000, Training loss: 0.1208\n","1 Training Accuracy: 1.0000, Training loss: 0.1207\n","1 Training Accuracy: 1.0000, Training loss: 0.1206\n","1 Training Accuracy: 1.0000, Training loss: 0.1205\n","1 Training Accuracy: 1.0000, Training loss: 0.1205\n","1 Training Accuracy: 1.0000, Training loss: 0.1204\n","1 Training Accuracy: 1.0000, Training loss: 0.1203\n","1 Training Accuracy: 1.0000, Training loss: 0.1203\n","1 Training Accuracy: 1.0000, Training loss: 0.1202\n","1 Training Accuracy: 1.0000, Training loss: 0.1202\n","1 Training Accuracy: 1.0000, Training loss: 0.1201\n","1 Training Accuracy: 1.0000, Training loss: 0.1201\n","1 Training Accuracy: 1.0000, Training loss: 0.1201\n","1 Training Accuracy: 1.0000, Training loss: 0.1200\n","1 Training Accuracy: 1.0000, Training loss: 0.1200\n","1 Training Accuracy: 1.0000, Training loss: 0.1200\n","1 Training Accuracy: 1.0000, Training loss: 0.1200\n","1 Training Accuracy: 1.0000, Training loss: 0.1200\n","1 Training Accuracy: 1.0000, Training loss: 0.1199\n","1 Training Accuracy: 1.0000, Training loss: 0.1199\n","1 Training Accuracy: 1.0000, Training loss: 0.1199\n","1 Training Accuracy: 1.0000, Training loss: 0.1199\n","1 Training Accuracy: 1.0000, Training loss: 0.1199\n","1 Training Accuracy: 1.0000, Training loss: 0.1199\n","1 Training Accuracy: 1.0000, Training loss: 0.1199\n","1 Training Accuracy: 1.0000, Training loss: 0.1199\n","1 Training Accuracy: 1.0000, Training loss: 0.1199\n","1 Training Accuracy: 1.0000, Training loss: 0.1199\n","1 Training Accuracy: 1.0000, Training loss: 0.1199\n","1 Training Accuracy: 1.0000, Testing Accuracy: 0.6940\n","Final Test Accuracy: 0.6940\n"]}]},{"cell_type":"markdown","source":["# Cross-architectural Generation"],"metadata":{"id":"lFv39CrJaFPA"}},{"cell_type":"markdown","source":["## MNIST"],"metadata":{"id":"_ul1xB8AbzrJ"}},{"cell_type":"code","source":["args.dataset = 'MNIST'\n","channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = utils.get_dataset(args.dataset, args.data_path)\n","args.batch_train = 32\n","\n","args.model = 'AlexNet'\n","net = utils.get_network(args.model,channel=channel,num_classes=num_classes,im_size=im_size).to(args.device)\n","it_eval = args.num_eval\n","\n","images_train = image_syn_noise_mnist.to(args.device)\n","labels_train = label_syn_noise_mnist.to(args.device)\n","dst_train = utils.TensorDataset(images_train, labels_train)\n","trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n","it_eval = 1\n","acc_train, acc_test = evaluate_dataset(it_eval,net,trainloader,testloader,args)\n","print(\"train with synthetic data, accuracy = %.4f\"%(acc_test))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oAw9wAqJaLg7","executionInfo":{"status":"ok","timestamp":1700952776643,"user_tz":300,"elapsed":9931,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"04b235fa-a5db-4087-e512-c2b33fa7b0bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 Training Accuracy: 0.0900, Training loss: 2.3023\n","1 Training Accuracy: 0.0700, Training loss: 2.3007\n","1 Training Accuracy: 0.1300, Training loss: 2.2997\n","1 Training Accuracy: 0.1000, Training loss: 2.2983\n","1 Training Accuracy: 0.1400, Training loss: 2.2968\n","1 Training Accuracy: 0.1000, Training loss: 2.2953\n","1 Training Accuracy: 0.1700, Training loss: 2.2939\n","1 Training Accuracy: 0.1900, Training loss: 2.2933\n","1 Training Accuracy: 0.2000, Training loss: 2.2916\n","1 Training Accuracy: 0.2100, Training loss: 2.2903\n","1 Training Accuracy: 0.1600, Training loss: 2.2886\n","1 Training Accuracy: 0.1700, Training loss: 2.2872\n","1 Training Accuracy: 0.3400, Training loss: 2.2858\n","1 Training Accuracy: 0.3600, Training loss: 2.2843\n","1 Training Accuracy: 0.5200, Training loss: 2.2822\n","1 Training Accuracy: 0.3700, Training loss: 2.2805\n","1 Training Accuracy: 0.4600, Training loss: 2.2789\n","1 Training Accuracy: 0.1500, Training loss: 2.2776\n","1 Training Accuracy: 0.3200, Training loss: 2.2751\n","1 Training Accuracy: 0.3200, Training loss: 2.2734\n","1 Training Accuracy: 0.1500, Training loss: 2.2712\n","1 Training Accuracy: 0.1900, Training loss: 2.2688\n","1 Training Accuracy: 0.1700, Training loss: 2.2679\n","1 Training Accuracy: 0.1900, Training loss: 2.2649\n","1 Training Accuracy: 0.1100, Training loss: 2.2619\n","1 Training Accuracy: 0.1800, Training loss: 2.2607\n","1 Training Accuracy: 0.2000, Training loss: 2.2567\n","1 Training Accuracy: 0.1100, Training loss: 2.2539\n","1 Training Accuracy: 0.1100, Training loss: 2.2505\n","1 Training Accuracy: 0.1600, Training loss: 2.2467\n","1 Training Accuracy: 0.1600, Training loss: 2.2446\n","1 Training Accuracy: 0.3600, Training loss: 2.2405\n","1 Training Accuracy: 0.4100, Training loss: 2.2365\n","1 Training Accuracy: 0.5200, Training loss: 2.2333\n","1 Training Accuracy: 0.2500, Training loss: 2.2295\n","1 Training Accuracy: 0.3500, Training loss: 2.2265\n","1 Training Accuracy: 0.3700, Training loss: 2.2213\n","1 Training Accuracy: 0.4200, Training loss: 2.2139\n","1 Training Accuracy: 0.3100, Training loss: 2.2066\n","1 Training Accuracy: 0.4700, Training loss: 2.2007\n","1 Training Accuracy: 0.5100, Training loss: 2.1970\n","1 Training Accuracy: 0.4300, Training loss: 2.1889\n","1 Training Accuracy: 0.4500, Training loss: 2.1800\n","1 Training Accuracy: 0.1100, Training loss: 2.1730\n","1 Training Accuracy: 0.2800, Training loss: 2.1648\n","1 Training Accuracy: 0.2500, Training loss: 2.1498\n","1 Training Accuracy: 0.3700, Training loss: 2.1414\n","1 Training Accuracy: 0.3800, Training loss: 2.1284\n","1 Training Accuracy: 0.3300, Training loss: 2.1137\n","1 Training Accuracy: 0.3300, Training loss: 2.0873\n","1 Training Accuracy: 0.2300, Training loss: 2.0715\n","1 Training Accuracy: 0.3900, Training loss: 2.0435\n","1 Training Accuracy: 0.3600, Training loss: 2.0294\n","1 Training Accuracy: 0.3100, Training loss: 1.9890\n","1 Training Accuracy: 0.2700, Training loss: 1.9752\n","1 Training Accuracy: 0.6500, Training loss: 1.9005\n","1 Training Accuracy: 0.5300, Training loss: 1.8409\n","1 Training Accuracy: 0.5000, Training loss: 1.8136\n","1 Training Accuracy: 0.4200, Training loss: 1.7674\n","1 Training Accuracy: 0.6400, Training loss: 1.6537\n","1 Training Accuracy: 0.5800, Training loss: 1.5752\n","1 Training Accuracy: 0.5200, Training loss: 1.5361\n","1 Training Accuracy: 0.5300, Training loss: 1.5152\n","1 Training Accuracy: 0.5400, Training loss: 1.4861\n","1 Training Accuracy: 0.5800, Training loss: 1.2843\n","1 Training Accuracy: 0.5600, Training loss: 1.4539\n","1 Training Accuracy: 0.4800, Training loss: 1.7726\n","1 Training Accuracy: 0.6900, Training loss: 1.1367\n","1 Training Accuracy: 0.6900, Training loss: 1.1293\n","1 Training Accuracy: 0.6500, Training loss: 1.0619\n","1 Training Accuracy: 0.7900, Training loss: 0.8625\n","1 Training Accuracy: 0.7400, Training loss: 0.9142\n","1 Training Accuracy: 0.7100, Training loss: 0.8991\n","1 Training Accuracy: 0.7100, Training loss: 0.8971\n","1 Training Accuracy: 0.7800, Training loss: 0.7366\n","1 Training Accuracy: 0.7900, Training loss: 0.8112\n","1 Training Accuracy: 0.8400, Training loss: 0.6556\n","1 Training Accuracy: 0.4900, Training loss: 1.9232\n","1 Training Accuracy: 0.8000, Training loss: 0.9180\n","1 Training Accuracy: 0.8400, Training loss: 0.7479\n","1 Training Accuracy: 0.8200, Training loss: 0.6153\n","1 Training Accuracy: 0.8200, Training loss: 0.5587\n","1 Training Accuracy: 0.8400, Training loss: 0.5150\n","1 Training Accuracy: 0.8400, Training loss: 0.4940\n","1 Training Accuracy: 0.8500, Training loss: 0.4494\n","1 Training Accuracy: 0.8000, Training loss: 0.5663\n","1 Training Accuracy: 0.7900, Training loss: 0.6394\n","1 Training Accuracy: 0.8800, Training loss: 0.3770\n","1 Training Accuracy: 0.9300, Training loss: 0.3246\n","1 Training Accuracy: 0.8000, Training loss: 0.4814\n","1 Training Accuracy: 0.8900, Training loss: 0.3840\n","1 Training Accuracy: 0.9600, Training loss: 0.2827\n","1 Training Accuracy: 0.9400, Training loss: 0.2970\n","1 Training Accuracy: 0.9500, Training loss: 0.2659\n","1 Training Accuracy: 0.9700, Training loss: 0.2475\n","1 Training Accuracy: 0.9300, Training loss: 0.2663\n","1 Training Accuracy: 0.9400, Training loss: 0.2193\n","1 Training Accuracy: 0.9400, Training loss: 0.2289\n","1 Training Accuracy: 0.9700, Training loss: 0.2198\n","1 Training Accuracy: 0.9800, Training loss: 0.1935\n","1 Training Accuracy: 0.9600, Training loss: 0.2324\n","1 Training Accuracy: 0.9800, Training loss: 0.1768\n","1 Training Accuracy: 0.9400, Training loss: 0.2026\n","1 Training Accuracy: 0.9500, Training loss: 0.1755\n","1 Training Accuracy: 0.9800, Training loss: 0.1417\n","1 Training Accuracy: 0.9600, Training loss: 0.1754\n","1 Training Accuracy: 0.9900, Training loss: 0.1326\n","1 Training Accuracy: 0.9900, Training loss: 0.1283\n","1 Training Accuracy: 0.9800, Training loss: 0.1313\n","1 Training Accuracy: 0.9900, Training loss: 0.1142\n","1 Training Accuracy: 0.9800, Training loss: 0.1119\n","1 Training Accuracy: 0.9900, Training loss: 0.1158\n","1 Training Accuracy: 0.9600, Training loss: 0.2023\n","1 Training Accuracy: 0.9900, Training loss: 0.1087\n","1 Training Accuracy: 0.9800, Training loss: 0.1285\n","1 Training Accuracy: 0.9900, Training loss: 0.0937\n","1 Training Accuracy: 0.9900, Training loss: 0.0913\n","1 Training Accuracy: 0.9900, Training loss: 0.0966\n","1 Training Accuracy: 0.9900, Training loss: 0.1000\n","1 Training Accuracy: 0.9900, Training loss: 0.0773\n","1 Training Accuracy: 0.9900, Training loss: 0.0797\n","1 Training Accuracy: 1.0000, Training loss: 0.0753\n","1 Training Accuracy: 0.9900, Training loss: 0.0733\n","1 Training Accuracy: 0.9900, Training loss: 0.0711\n","1 Training Accuracy: 0.9900, Training loss: 0.0706\n","1 Training Accuracy: 0.9900, Training loss: 0.0755\n","1 Training Accuracy: 0.9900, Training loss: 0.0627\n","1 Training Accuracy: 0.9900, Training loss: 0.0748\n","1 Training Accuracy: 1.0000, Training loss: 0.0626\n","1 Training Accuracy: 0.9900, Training loss: 0.0786\n","1 Training Accuracy: 0.9900, Training loss: 0.0632\n","1 Training Accuracy: 0.9900, Training loss: 0.0649\n","1 Training Accuracy: 0.9900, Training loss: 0.0572\n","1 Training Accuracy: 0.9900, Training loss: 0.0555\n","1 Training Accuracy: 0.9900, Training loss: 0.0564\n","1 Training Accuracy: 1.0000, Training loss: 0.0544\n","1 Training Accuracy: 1.0000, Training loss: 0.0525\n","1 Training Accuracy: 0.9900, Training loss: 0.0517\n","1 Training Accuracy: 1.0000, Training loss: 0.0509\n","1 Training Accuracy: 1.0000, Training loss: 0.0509\n","1 Training Accuracy: 1.0000, Training loss: 0.0510\n","1 Training Accuracy: 1.0000, Training loss: 0.0512\n","1 Training Accuracy: 0.9900, Training loss: 0.0493\n","1 Training Accuracy: 0.9900, Training loss: 0.0522\n","1 Training Accuracy: 0.9900, Training loss: 0.0471\n","1 Training Accuracy: 1.0000, Training loss: 0.0447\n","1 Training Accuracy: 0.9900, Training loss: 0.0463\n","1 Training Accuracy: 0.9900, Training loss: 0.0440\n","1 Training Accuracy: 1.0000, Training loss: 0.0443\n","1 Training Accuracy: 1.0000, Training loss: 0.0426\n","1 Training Accuracy: 1.0000, Training loss: 0.0438\n","1 Training Accuracy: 1.0000, Training loss: 0.0415\n","1 Training Accuracy: 1.0000, Training loss: 0.0414\n","1 Training Accuracy: 1.0000, Training loss: 0.0406\n","1 Training Accuracy: 1.0000, Training loss: 0.0404\n","1 Training Accuracy: 1.0000, Training loss: 0.0403\n","1 Training Accuracy: 1.0000, Training loss: 0.0395\n","1 Training Accuracy: 1.0000, Training loss: 0.0394\n","1 Training Accuracy: 1.0000, Training loss: 0.0396\n","1 Training Accuracy: 1.0000, Training loss: 0.0394\n","1 Training Accuracy: 1.0000, Training loss: 0.0387\n","1 Training Accuracy: 1.0000, Training loss: 0.0383\n","1 Training Accuracy: 1.0000, Training loss: 0.0378\n","1 Training Accuracy: 1.0000, Training loss: 0.0380\n","1 Training Accuracy: 1.0000, Training loss: 0.0372\n","1 Training Accuracy: 1.0000, Training loss: 0.0373\n","1 Training Accuracy: 1.0000, Training loss: 0.0373\n","1 Training Accuracy: 1.0000, Training loss: 0.0378\n","1 Training Accuracy: 1.0000, Training loss: 0.0376\n","1 Training Accuracy: 1.0000, Training loss: 0.0369\n","1 Training Accuracy: 1.0000, Training loss: 0.0366\n","1 Training Accuracy: 1.0000, Training loss: 0.0365\n","1 Training Accuracy: 1.0000, Training loss: 0.0360\n","1 Training Accuracy: 1.0000, Training loss: 0.0370\n","1 Training Accuracy: 1.0000, Training loss: 0.0366\n","1 Training Accuracy: 1.0000, Training loss: 0.0360\n","1 Training Accuracy: 1.0000, Training loss: 0.0361\n","1 Training Accuracy: 1.0000, Training loss: 0.0354\n","1 Training Accuracy: 1.0000, Training loss: 0.0351\n","1 Training Accuracy: 1.0000, Training loss: 0.0349\n","1 Training Accuracy: 1.0000, Training loss: 0.0349\n","1 Training Accuracy: 1.0000, Training loss: 0.0350\n","1 Training Accuracy: 1.0000, Training loss: 0.0350\n","1 Training Accuracy: 1.0000, Training loss: 0.0348\n","1 Training Accuracy: 1.0000, Training loss: 0.0347\n","1 Training Accuracy: 1.0000, Training loss: 0.0347\n","1 Training Accuracy: 1.0000, Training loss: 0.0347\n","1 Training Accuracy: 1.0000, Training loss: 0.0346\n","1 Training Accuracy: 1.0000, Training loss: 0.0346\n","1 Training Accuracy: 1.0000, Training loss: 0.0346\n","1 Training Accuracy: 1.0000, Training loss: 0.0345\n","1 Training Accuracy: 1.0000, Training loss: 0.0345\n","1 Training Accuracy: 1.0000, Training loss: 0.0345\n","1 Training Accuracy: 1.0000, Training loss: 0.0345\n","1 Training Accuracy: 1.0000, Training loss: 0.0345\n","1 Training Accuracy: 1.0000, Training loss: 0.0345\n","1 Training Accuracy: 1.0000, Training loss: 0.0345\n","1 Training Accuracy: 1.0000, Training loss: 0.0345\n","1 Training Accuracy: 1.0000, Training loss: 0.0345\n","1 Training Accuracy: 1.0000, Training loss: 0.0345\n","1 Training Accuracy: 1.0000, Testing Accuracy: 0.8444\n","train with synthetic data, accuracy = 0.8444\n"]}]},{"cell_type":"code","source":["args.dataset = 'MNIST'\n","channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = utils.get_dataset(args.dataset, args.data_path)\n","args.batch_train = 32\n","\n","args.model = 'VGG11'\n","net = utils.get_network(args.model,channel=channel,num_classes=num_classes,im_size=im_size).to(args.device)\n","it_eval = args.num_eval\n","\n","images_train = image_syn_noise_mnist.to(args.device)\n","labels_train = label_syn_noise_mnist.to(args.device)\n","dst_train = utils.TensorDataset(images_train, labels_train)\n","trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n","it_eval = 1\n","acc_train, acc_test = evaluate_dataset(it_eval,net,trainloader,testloader,args)\n","print(\"train with synthetic data, accuracy = %.4f\"%(acc_test))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yrRe4fkda_l1","executionInfo":{"status":"ok","timestamp":1700952794470,"user_tz":300,"elapsed":12476,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"4173db0e-cad3-4e7b-da81-47deae4c2644"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 Training Accuracy: 0.0700, Training loss: 2.6498\n","1 Training Accuracy: 0.1200, Training loss: 3.6214\n","1 Training Accuracy: 0.0900, Training loss: 2.9937\n","1 Training Accuracy: 0.1000, Training loss: 4.3262\n","1 Training Accuracy: 0.0900, Training loss: 5.8690\n","1 Training Accuracy: 0.0700, Training loss: 4.3325\n","1 Training Accuracy: 0.0800, Training loss: 4.3367\n","1 Training Accuracy: 0.1400, Training loss: 3.6476\n","1 Training Accuracy: 0.1300, Training loss: 3.6192\n","1 Training Accuracy: 0.2500, Training loss: 2.4882\n","1 Training Accuracy: 0.1500, Training loss: 2.7832\n","1 Training Accuracy: 0.2000, Training loss: 2.3692\n","1 Training Accuracy: 0.5000, Training loss: 1.7920\n","1 Training Accuracy: 0.4700, Training loss: 1.7956\n","1 Training Accuracy: 0.5400, Training loss: 1.7039\n","1 Training Accuracy: 0.3400, Training loss: 2.5980\n","1 Training Accuracy: 0.6300, Training loss: 1.3730\n","1 Training Accuracy: 0.7400, Training loss: 1.0732\n","1 Training Accuracy: 0.6300, Training loss: 1.1796\n","1 Training Accuracy: 0.7700, Training loss: 0.8978\n","1 Training Accuracy: 0.8100, Training loss: 0.7885\n","1 Training Accuracy: 0.9700, Training loss: 0.4046\n","1 Training Accuracy: 1.0000, Training loss: 0.2998\n","1 Training Accuracy: 1.0000, Training loss: 0.2335\n","1 Training Accuracy: 1.0000, Training loss: 0.1742\n","1 Training Accuracy: 1.0000, Training loss: 0.1395\n","1 Training Accuracy: 1.0000, Training loss: 0.1280\n","1 Training Accuracy: 1.0000, Training loss: 0.1113\n","1 Training Accuracy: 1.0000, Training loss: 0.0972\n","1 Training Accuracy: 1.0000, Training loss: 0.0866\n","1 Training Accuracy: 1.0000, Training loss: 0.0801\n","1 Training Accuracy: 1.0000, Training loss: 0.0764\n","1 Training Accuracy: 1.0000, Training loss: 0.0744\n","1 Training Accuracy: 1.0000, Training loss: 0.0692\n","1 Training Accuracy: 1.0000, Training loss: 0.0628\n","1 Training Accuracy: 1.0000, Training loss: 0.0578\n","1 Training Accuracy: 1.0000, Training loss: 0.0551\n","1 Training Accuracy: 1.0000, Training loss: 0.0525\n","1 Training Accuracy: 1.0000, Training loss: 0.0497\n","1 Training Accuracy: 1.0000, Training loss: 0.0483\n","1 Training Accuracy: 1.0000, Training loss: 0.0455\n","1 Training Accuracy: 1.0000, Training loss: 0.0436\n","1 Training Accuracy: 1.0000, Training loss: 0.0421\n","1 Training Accuracy: 1.0000, Training loss: 0.0411\n","1 Training Accuracy: 1.0000, Training loss: 0.0400\n","1 Training Accuracy: 1.0000, Training loss: 0.0384\n","1 Training Accuracy: 1.0000, Training loss: 0.0369\n","1 Training Accuracy: 1.0000, Training loss: 0.0357\n","1 Training Accuracy: 1.0000, Training loss: 0.0346\n","1 Training Accuracy: 1.0000, Training loss: 0.0337\n","1 Training Accuracy: 1.0000, Training loss: 0.0330\n","1 Training Accuracy: 1.0000, Training loss: 0.0321\n","1 Training Accuracy: 1.0000, Training loss: 0.0313\n","1 Training Accuracy: 1.0000, Training loss: 0.0307\n","1 Training Accuracy: 1.0000, Training loss: 0.0298\n","1 Training Accuracy: 1.0000, Training loss: 0.0291\n","1 Training Accuracy: 1.0000, Training loss: 0.0285\n","1 Training Accuracy: 1.0000, Training loss: 0.0279\n","1 Training Accuracy: 1.0000, Training loss: 0.0273\n","1 Training Accuracy: 1.0000, Training loss: 0.0268\n","1 Training Accuracy: 1.0000, Training loss: 0.0263\n","1 Training Accuracy: 1.0000, Training loss: 0.0257\n","1 Training Accuracy: 1.0000, Training loss: 0.0253\n","1 Training Accuracy: 1.0000, Training loss: 0.0249\n","1 Training Accuracy: 1.0000, Training loss: 0.0244\n","1 Training Accuracy: 1.0000, Training loss: 0.0240\n","1 Training Accuracy: 1.0000, Training loss: 0.0235\n","1 Training Accuracy: 1.0000, Training loss: 0.0231\n","1 Training Accuracy: 1.0000, Training loss: 0.0228\n","1 Training Accuracy: 1.0000, Training loss: 0.0225\n","1 Training Accuracy: 1.0000, Training loss: 0.0221\n","1 Training Accuracy: 1.0000, Training loss: 0.0218\n","1 Training Accuracy: 1.0000, Training loss: 0.0215\n","1 Training Accuracy: 1.0000, Training loss: 0.0211\n","1 Training Accuracy: 1.0000, Training loss: 0.0209\n","1 Training Accuracy: 1.0000, Training loss: 0.0205\n","1 Training Accuracy: 1.0000, Training loss: 0.0203\n","1 Training Accuracy: 1.0000, Training loss: 0.0200\n","1 Training Accuracy: 1.0000, Training loss: 0.0198\n","1 Training Accuracy: 1.0000, Training loss: 0.0195\n","1 Training Accuracy: 1.0000, Training loss: 0.0193\n","1 Training Accuracy: 1.0000, Training loss: 0.0191\n","1 Training Accuracy: 1.0000, Training loss: 0.0189\n","1 Training Accuracy: 1.0000, Training loss: 0.0187\n","1 Training Accuracy: 1.0000, Training loss: 0.0185\n","1 Training Accuracy: 1.0000, Training loss: 0.0183\n","1 Training Accuracy: 1.0000, Training loss: 0.0181\n","1 Training Accuracy: 1.0000, Training loss: 0.0179\n","1 Training Accuracy: 1.0000, Training loss: 0.0178\n","1 Training Accuracy: 1.0000, Training loss: 0.0176\n","1 Training Accuracy: 1.0000, Training loss: 0.0175\n","1 Training Accuracy: 1.0000, Training loss: 0.0173\n","1 Training Accuracy: 1.0000, Training loss: 0.0171\n","1 Training Accuracy: 1.0000, Training loss: 0.0170\n","1 Training Accuracy: 1.0000, Training loss: 0.0169\n","1 Training Accuracy: 1.0000, Training loss: 0.0167\n","1 Training Accuracy: 1.0000, Training loss: 0.0166\n","1 Training Accuracy: 1.0000, Training loss: 0.0165\n","1 Training Accuracy: 1.0000, Training loss: 0.0163\n","1 Training Accuracy: 1.0000, Training loss: 0.0162\n","1 Training Accuracy: 1.0000, Training loss: 0.0161\n","1 Training Accuracy: 1.0000, Training loss: 0.0159\n","1 Training Accuracy: 1.0000, Training loss: 0.0158\n","1 Training Accuracy: 1.0000, Training loss: 0.0157\n","1 Training Accuracy: 1.0000, Training loss: 0.0156\n","1 Training Accuracy: 1.0000, Training loss: 0.0155\n","1 Training Accuracy: 1.0000, Training loss: 0.0154\n","1 Training Accuracy: 1.0000, Training loss: 0.0153\n","1 Training Accuracy: 1.0000, Training loss: 0.0152\n","1 Training Accuracy: 1.0000, Training loss: 0.0151\n","1 Training Accuracy: 1.0000, Training loss: 0.0150\n","1 Training Accuracy: 1.0000, Training loss: 0.0149\n","1 Training Accuracy: 1.0000, Training loss: 0.0149\n","1 Training Accuracy: 1.0000, Training loss: 0.0148\n","1 Training Accuracy: 1.0000, Training loss: 0.0147\n","1 Training Accuracy: 1.0000, Training loss: 0.0146\n","1 Training Accuracy: 1.0000, Training loss: 0.0145\n","1 Training Accuracy: 1.0000, Training loss: 0.0145\n","1 Training Accuracy: 1.0000, Training loss: 0.0144\n","1 Training Accuracy: 1.0000, Training loss: 0.0143\n","1 Training Accuracy: 1.0000, Training loss: 0.0143\n","1 Training Accuracy: 1.0000, Training loss: 0.0142\n","1 Training Accuracy: 1.0000, Training loss: 0.0142\n","1 Training Accuracy: 1.0000, Training loss: 0.0141\n","1 Training Accuracy: 1.0000, Training loss: 0.0140\n","1 Training Accuracy: 1.0000, Training loss: 0.0140\n","1 Training Accuracy: 1.0000, Training loss: 0.0139\n","1 Training Accuracy: 1.0000, Training loss: 0.0139\n","1 Training Accuracy: 1.0000, Training loss: 0.0138\n","1 Training Accuracy: 1.0000, Training loss: 0.0138\n","1 Training Accuracy: 1.0000, Training loss: 0.0137\n","1 Training Accuracy: 1.0000, Training loss: 0.0137\n","1 Training Accuracy: 1.0000, Training loss: 0.0136\n","1 Training Accuracy: 1.0000, Training loss: 0.0136\n","1 Training Accuracy: 1.0000, Training loss: 0.0135\n","1 Training Accuracy: 1.0000, Training loss: 0.0135\n","1 Training Accuracy: 1.0000, Training loss: 0.0135\n","1 Training Accuracy: 1.0000, Training loss: 0.0134\n","1 Training Accuracy: 1.0000, Training loss: 0.0134\n","1 Training Accuracy: 1.0000, Training loss: 0.0133\n","1 Training Accuracy: 1.0000, Training loss: 0.0133\n","1 Training Accuracy: 1.0000, Training loss: 0.0133\n","1 Training Accuracy: 1.0000, Training loss: 0.0132\n","1 Training Accuracy: 1.0000, Training loss: 0.0132\n","1 Training Accuracy: 1.0000, Training loss: 0.0132\n","1 Training Accuracy: 1.0000, Training loss: 0.0131\n","1 Training Accuracy: 1.0000, Training loss: 0.0131\n","1 Training Accuracy: 1.0000, Training loss: 0.0131\n","1 Training Accuracy: 1.0000, Training loss: 0.0131\n","1 Training Accuracy: 1.0000, Training loss: 0.0130\n","1 Training Accuracy: 1.0000, Training loss: 0.0130\n","1 Training Accuracy: 1.0000, Training loss: 0.0130\n","1 Training Accuracy: 1.0000, Training loss: 0.0130\n","1 Training Accuracy: 1.0000, Training loss: 0.0130\n","1 Training Accuracy: 1.0000, Training loss: 0.0129\n","1 Training Accuracy: 1.0000, Training loss: 0.0129\n","1 Training Accuracy: 1.0000, Training loss: 0.0129\n","1 Training Accuracy: 1.0000, Training loss: 0.0129\n","1 Training Accuracy: 1.0000, Training loss: 0.0129\n","1 Training Accuracy: 1.0000, Training loss: 0.0128\n","1 Training Accuracy: 1.0000, Training loss: 0.0128\n","1 Training Accuracy: 1.0000, Training loss: 0.0128\n","1 Training Accuracy: 1.0000, Training loss: 0.0128\n","1 Training Accuracy: 1.0000, Training loss: 0.0128\n","1 Training Accuracy: 1.0000, Training loss: 0.0128\n","1 Training Accuracy: 1.0000, Training loss: 0.0128\n","1 Training Accuracy: 1.0000, Training loss: 0.0128\n","1 Training Accuracy: 1.0000, Training loss: 0.0127\n","1 Training Accuracy: 1.0000, Training loss: 0.0127\n","1 Training Accuracy: 1.0000, Training loss: 0.0127\n","1 Training Accuracy: 1.0000, Training loss: 0.0127\n","1 Training Accuracy: 1.0000, Training loss: 0.0127\n","1 Training Accuracy: 1.0000, Training loss: 0.0127\n","1 Training Accuracy: 1.0000, Training loss: 0.0127\n","1 Training Accuracy: 1.0000, Training loss: 0.0127\n","1 Training Accuracy: 1.0000, Training loss: 0.0127\n","1 Training Accuracy: 1.0000, Training loss: 0.0127\n","1 Training Accuracy: 1.0000, Training loss: 0.0127\n","1 Training Accuracy: 1.0000, Training loss: 0.0127\n","1 Training Accuracy: 1.0000, Training loss: 0.0127\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Testing Accuracy: 0.8798\n","train with synthetic data, accuracy = 0.8798\n"]}]},{"cell_type":"code","source":["args.dataset = 'MNIST'\n","channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = utils.get_dataset(args.dataset, args.data_path)\n","args.batch_train = 32\n","\n","args.model = 'ResNet18'\n","net = utils.get_network(args.model,channel=channel,num_classes=num_classes,im_size=im_size).to(args.device)\n","it_eval = args.num_eval\n","\n","images_train = image_syn_noise_mnist.to(args.device)\n","labels_train = label_syn_noise_mnist.to(args.device)\n","dst_train = utils.TensorDataset(images_train, labels_train)\n","trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n","it_eval = 1\n","acc_train, acc_test = evaluate_dataset(it_eval,net,trainloader,testloader,args)\n","print(\"train with synthetic data, accuracy = %.4f\"%(acc_test))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D3SFEL1rbS77","executionInfo":{"status":"ok","timestamp":1700952825308,"user_tz":300,"elapsed":28328,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"f162de51-4d4b-453b-f327-0eeb0075100c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 Training Accuracy: 0.0900, Training loss: 2.3758\n","1 Training Accuracy: 0.1100, Training loss: 2.2895\n","1 Training Accuracy: 0.1600, Training loss: 2.2426\n","1 Training Accuracy: 0.1500, Training loss: 2.2230\n","1 Training Accuracy: 0.3800, Training loss: 2.1272\n","1 Training Accuracy: 0.4600, Training loss: 2.0482\n","1 Training Accuracy: 0.3000, Training loss: 1.9973\n","1 Training Accuracy: 0.6800, Training loss: 1.8979\n","1 Training Accuracy: 0.6700, Training loss: 1.8343\n","1 Training Accuracy: 0.7800, Training loss: 1.7666\n","1 Training Accuracy: 0.6800, Training loss: 1.7353\n","1 Training Accuracy: 0.6200, Training loss: 1.8022\n","1 Training Accuracy: 0.7800, Training loss: 1.6075\n","1 Training Accuracy: 0.9000, Training loss: 1.5061\n","1 Training Accuracy: 0.7500, Training loss: 1.4969\n","1 Training Accuracy: 0.9300, Training loss: 1.4344\n","1 Training Accuracy: 0.9700, Training loss: 1.3313\n","1 Training Accuracy: 0.9700, Training loss: 1.3033\n","1 Training Accuracy: 0.8400, Training loss: 1.2829\n","1 Training Accuracy: 1.0000, Training loss: 1.2192\n","1 Training Accuracy: 1.0000, Training loss: 1.1291\n","1 Training Accuracy: 1.0000, Training loss: 1.0873\n","1 Training Accuracy: 1.0000, Training loss: 1.0349\n","1 Training Accuracy: 1.0000, Training loss: 0.9601\n","1 Training Accuracy: 0.9700, Training loss: 0.9801\n","1 Training Accuracy: 1.0000, Training loss: 0.9105\n","1 Training Accuracy: 1.0000, Training loss: 0.8366\n","1 Training Accuracy: 1.0000, Training loss: 0.8027\n","1 Training Accuracy: 1.0000, Training loss: 0.7449\n","1 Training Accuracy: 1.0000, Training loss: 0.6822\n","1 Training Accuracy: 1.0000, Training loss: 0.6248\n","1 Training Accuracy: 1.0000, Training loss: 0.5945\n","1 Training Accuracy: 1.0000, Training loss: 0.5709\n","1 Training Accuracy: 1.0000, Training loss: 0.4877\n","1 Training Accuracy: 1.0000, Training loss: 0.5020\n","1 Training Accuracy: 1.0000, Training loss: 0.4413\n","1 Training Accuracy: 1.0000, Training loss: 0.3919\n","1 Training Accuracy: 1.0000, Training loss: 0.4004\n","1 Training Accuracy: 1.0000, Training loss: 0.3771\n","1 Training Accuracy: 1.0000, Training loss: 0.3662\n","1 Training Accuracy: 1.0000, Training loss: 0.3297\n","1 Training Accuracy: 1.0000, Training loss: 0.2980\n","1 Training Accuracy: 1.0000, Training loss: 0.2896\n","1 Training Accuracy: 1.0000, Training loss: 0.2541\n","1 Training Accuracy: 1.0000, Training loss: 0.2635\n","1 Training Accuracy: 1.0000, Training loss: 0.2466\n","1 Training Accuracy: 1.0000, Training loss: 0.2211\n","1 Training Accuracy: 1.0000, Training loss: 0.2207\n","1 Training Accuracy: 1.0000, Training loss: 0.2023\n","1 Training Accuracy: 1.0000, Training loss: 0.1937\n","1 Training Accuracy: 1.0000, Training loss: 0.1869\n","1 Training Accuracy: 1.0000, Training loss: 0.1833\n","1 Training Accuracy: 1.0000, Training loss: 0.1738\n","1 Training Accuracy: 1.0000, Training loss: 0.1714\n","1 Training Accuracy: 1.0000, Training loss: 0.1607\n","1 Training Accuracy: 1.0000, Training loss: 0.1548\n","1 Training Accuracy: 1.0000, Training loss: 0.1478\n","1 Training Accuracy: 1.0000, Training loss: 0.1440\n","1 Training Accuracy: 1.0000, Training loss: 0.1424\n","1 Training Accuracy: 1.0000, Training loss: 0.1349\n","1 Training Accuracy: 1.0000, Training loss: 0.1303\n","1 Training Accuracy: 1.0000, Training loss: 0.1267\n","1 Training Accuracy: 1.0000, Training loss: 0.1238\n","1 Training Accuracy: 1.0000, Training loss: 0.1209\n","1 Training Accuracy: 1.0000, Training loss: 0.1189\n","1 Training Accuracy: 1.0000, Training loss: 0.1163\n","1 Training Accuracy: 1.0000, Training loss: 0.1151\n","1 Training Accuracy: 1.0000, Training loss: 0.1093\n","1 Training Accuracy: 1.0000, Training loss: 0.1059\n","1 Training Accuracy: 1.0000, Training loss: 0.1044\n","1 Training Accuracy: 1.0000, Training loss: 0.1030\n","1 Training Accuracy: 1.0000, Training loss: 0.0987\n","1 Training Accuracy: 1.0000, Training loss: 0.0979\n","1 Training Accuracy: 1.0000, Training loss: 0.0949\n","1 Training Accuracy: 1.0000, Training loss: 0.0940\n","1 Training Accuracy: 1.0000, Training loss: 0.0926\n","1 Training Accuracy: 1.0000, Training loss: 0.0902\n","1 Training Accuracy: 1.0000, Training loss: 0.0886\n","1 Training Accuracy: 1.0000, Training loss: 0.0870\n","1 Training Accuracy: 1.0000, Training loss: 0.0856\n","1 Training Accuracy: 1.0000, Training loss: 0.0847\n","1 Training Accuracy: 1.0000, Training loss: 0.0826\n","1 Training Accuracy: 1.0000, Training loss: 0.0817\n","1 Training Accuracy: 1.0000, Training loss: 0.0800\n","1 Training Accuracy: 1.0000, Training loss: 0.0789\n","1 Training Accuracy: 1.0000, Training loss: 0.0776\n","1 Training Accuracy: 1.0000, Training loss: 0.0768\n","1 Training Accuracy: 1.0000, Training loss: 0.0755\n","1 Training Accuracy: 1.0000, Training loss: 0.0753\n","1 Training Accuracy: 1.0000, Training loss: 0.0736\n","1 Training Accuracy: 1.0000, Training loss: 0.0728\n","1 Training Accuracy: 1.0000, Training loss: 0.0718\n","1 Training Accuracy: 1.0000, Training loss: 0.0709\n","1 Training Accuracy: 1.0000, Training loss: 0.0701\n","1 Training Accuracy: 1.0000, Training loss: 0.0694\n","1 Training Accuracy: 1.0000, Training loss: 0.0683\n","1 Training Accuracy: 1.0000, Training loss: 0.0675\n","1 Training Accuracy: 1.0000, Training loss: 0.0671\n","1 Training Accuracy: 1.0000, Training loss: 0.0660\n","1 Training Accuracy: 1.0000, Training loss: 0.0655\n","1 Training Accuracy: 1.0000, Training loss: 0.0647\n","1 Training Accuracy: 1.0000, Training loss: 0.0643\n","1 Training Accuracy: 1.0000, Training loss: 0.0634\n","1 Training Accuracy: 1.0000, Training loss: 0.0627\n","1 Training Accuracy: 1.0000, Training loss: 0.0625\n","1 Training Accuracy: 1.0000, Training loss: 0.0621\n","1 Training Accuracy: 1.0000, Training loss: 0.0612\n","1 Training Accuracy: 1.0000, Training loss: 0.0608\n","1 Training Accuracy: 1.0000, Training loss: 0.0603\n","1 Training Accuracy: 1.0000, Training loss: 0.0598\n","1 Training Accuracy: 1.0000, Training loss: 0.0593\n","1 Training Accuracy: 1.0000, Training loss: 0.0588\n","1 Training Accuracy: 1.0000, Training loss: 0.0584\n","1 Training Accuracy: 1.0000, Training loss: 0.0580\n","1 Training Accuracy: 1.0000, Training loss: 0.0577\n","1 Training Accuracy: 1.0000, Training loss: 0.0572\n","1 Training Accuracy: 1.0000, Training loss: 0.0567\n","1 Training Accuracy: 1.0000, Training loss: 0.0564\n","1 Training Accuracy: 1.0000, Training loss: 0.0561\n","1 Training Accuracy: 1.0000, Training loss: 0.0557\n","1 Training Accuracy: 1.0000, Training loss: 0.0552\n","1 Training Accuracy: 1.0000, Training loss: 0.0550\n","1 Training Accuracy: 1.0000, Training loss: 0.0547\n","1 Training Accuracy: 1.0000, Training loss: 0.0544\n","1 Training Accuracy: 1.0000, Training loss: 0.0541\n","1 Training Accuracy: 1.0000, Training loss: 0.0539\n","1 Training Accuracy: 1.0000, Training loss: 0.0535\n","1 Training Accuracy: 1.0000, Training loss: 0.0533\n","1 Training Accuracy: 1.0000, Training loss: 0.0530\n","1 Training Accuracy: 1.0000, Training loss: 0.0527\n","1 Training Accuracy: 1.0000, Training loss: 0.0526\n","1 Training Accuracy: 1.0000, Training loss: 0.0523\n","1 Training Accuracy: 1.0000, Training loss: 0.0520\n","1 Training Accuracy: 1.0000, Training loss: 0.0518\n","1 Training Accuracy: 1.0000, Training loss: 0.0516\n","1 Training Accuracy: 1.0000, Training loss: 0.0514\n","1 Training Accuracy: 1.0000, Training loss: 0.0512\n","1 Training Accuracy: 1.0000, Training loss: 0.0510\n","1 Training Accuracy: 1.0000, Training loss: 0.0508\n","1 Training Accuracy: 1.0000, Training loss: 0.0507\n","1 Training Accuracy: 1.0000, Training loss: 0.0505\n","1 Training Accuracy: 1.0000, Training loss: 0.0504\n","1 Training Accuracy: 1.0000, Training loss: 0.0502\n","1 Training Accuracy: 1.0000, Training loss: 0.0501\n","1 Training Accuracy: 1.0000, Training loss: 0.0499\n","1 Training Accuracy: 1.0000, Training loss: 0.0498\n","1 Training Accuracy: 1.0000, Training loss: 0.0496\n","1 Training Accuracy: 1.0000, Training loss: 0.0495\n","1 Training Accuracy: 1.0000, Training loss: 0.0494\n","1 Training Accuracy: 1.0000, Training loss: 0.0493\n","1 Training Accuracy: 1.0000, Training loss: 0.0491\n","1 Training Accuracy: 1.0000, Training loss: 0.0490\n","1 Training Accuracy: 1.0000, Training loss: 0.0489\n","1 Training Accuracy: 1.0000, Training loss: 0.0488\n","1 Training Accuracy: 1.0000, Training loss: 0.0487\n","1 Training Accuracy: 1.0000, Training loss: 0.0486\n","1 Training Accuracy: 1.0000, Training loss: 0.0485\n","1 Training Accuracy: 1.0000, Training loss: 0.0485\n","1 Training Accuracy: 1.0000, Training loss: 0.0484\n","1 Training Accuracy: 1.0000, Training loss: 0.0483\n","1 Training Accuracy: 1.0000, Training loss: 0.0482\n","1 Training Accuracy: 1.0000, Training loss: 0.0481\n","1 Training Accuracy: 1.0000, Training loss: 0.0481\n","1 Training Accuracy: 1.0000, Training loss: 0.0480\n","1 Training Accuracy: 1.0000, Training loss: 0.0480\n","1 Training Accuracy: 1.0000, Training loss: 0.0479\n","1 Training Accuracy: 1.0000, Training loss: 0.0478\n","1 Training Accuracy: 1.0000, Training loss: 0.0478\n","1 Training Accuracy: 1.0000, Training loss: 0.0478\n","1 Training Accuracy: 1.0000, Training loss: 0.0477\n","1 Training Accuracy: 1.0000, Training loss: 0.0477\n","1 Training Accuracy: 1.0000, Training loss: 0.0476\n","1 Training Accuracy: 1.0000, Training loss: 0.0476\n","1 Training Accuracy: 1.0000, Training loss: 0.0476\n","1 Training Accuracy: 1.0000, Training loss: 0.0475\n","1 Training Accuracy: 1.0000, Training loss: 0.0475\n","1 Training Accuracy: 1.0000, Training loss: 0.0475\n","1 Training Accuracy: 1.0000, Training loss: 0.0474\n","1 Training Accuracy: 1.0000, Training loss: 0.0474\n","1 Training Accuracy: 1.0000, Training loss: 0.0474\n","1 Training Accuracy: 1.0000, Training loss: 0.0474\n","1 Training Accuracy: 1.0000, Training loss: 0.0474\n","1 Training Accuracy: 1.0000, Training loss: 0.0473\n","1 Training Accuracy: 1.0000, Training loss: 0.0473\n","1 Training Accuracy: 1.0000, Training loss: 0.0473\n","1 Training Accuracy: 1.0000, Training loss: 0.0473\n","1 Training Accuracy: 1.0000, Training loss: 0.0473\n","1 Training Accuracy: 1.0000, Training loss: 0.0473\n","1 Training Accuracy: 1.0000, Training loss: 0.0473\n","1 Training Accuracy: 1.0000, Training loss: 0.0473\n","1 Training Accuracy: 1.0000, Training loss: 0.0473\n","1 Training Accuracy: 1.0000, Training loss: 0.0473\n","1 Training Accuracy: 1.0000, Training loss: 0.0472\n","1 Training Accuracy: 1.0000, Training loss: 0.0472\n","1 Training Accuracy: 1.0000, Training loss: 0.0472\n","1 Training Accuracy: 1.0000, Training loss: 0.0472\n","1 Training Accuracy: 1.0000, Training loss: 0.0472\n","1 Training Accuracy: 1.0000, Training loss: 0.0472\n","1 Training Accuracy: 1.0000, Training loss: 0.0472\n","1 Training Accuracy: 1.0000, Training loss: 0.0472\n","1 Training Accuracy: 1.0000, Testing Accuracy: 0.7508\n","train with synthetic data, accuracy = 0.7508\n"]}]},{"cell_type":"markdown","source":["##MHIST"],"metadata":{"id":"oQlzVhpPbyb-"}},{"cell_type":"code","source":["args.dataset = 'MHIST'\n","args.epoch_train = 20\n","channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = utils.get_dataset(args.dataset, args.data_path)\n","#dst_test"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R_EPI31WPZU0","executionInfo":{"status":"ok","timestamp":1700952843546,"user_tz":300,"elapsed":12572,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"1d9e9ca2-8bcc-420a-f41f-59a3c9a98855"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset ImageFolder\n","    Number of datapoints: 977\n","    Root location: ./mhist_dataset/test\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=warn)\n","               ToTensor()\n","               Normalize(mean=tensor([0.7378, 0.6486, 0.7752]), std=tensor([0.1889, 0.2307, 0.1640]))\n","           )"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["args.epoch_train = 200\n","\n","#image_syn_noise_mhist = torch.load('image_syn.pt')\n","#label_syn_noise_mhist = torch.load('label_syn.pt')\n","\n","args.dataset = 'MHIST'\n","channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = utils.get_dataset(args.dataset, args.data_path)\n","#args.batch_train = 32\n","\n","args.model = 'AlexNet'\n","net = utils.get_network(args.model,channel=channel,num_classes=num_classes,im_size=im_size).to(args.device)\n","#net = AlexNet(channel=channel, num_classes=num_classes)\n","it_eval = args.num_eval\n","\n","images_train = image_syn_noise_mhist.to(args.device)\n","labels_train = label_syn_noise_mhist.to(args.device)\n","dst_train = utils.TensorDataset(images_train, labels_train)\n","trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n","it_eval = 1\n","acc_train, acc_test = evaluate_dataset(it_eval,net,trainloader,testloader,args)\n","print(\"train with synthetic data, accuracy = %.4f\"%(acc_test))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7mNKHZVjbVKc","executionInfo":{"status":"ok","timestamp":1700952867588,"user_tz":300,"elapsed":21737,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"fc58bf22-72ba-4619-9314-c32c3a99e551"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 Training Accuracy: 0.3700, Training loss: 0.6938\n","1 Training Accuracy: 0.5000, Training loss: 0.6933\n","1 Training Accuracy: 0.5200, Training loss: 0.6933\n","1 Training Accuracy: 0.5000, Training loss: 0.6932\n","1 Training Accuracy: 0.5000, Training loss: 0.6930\n","1 Training Accuracy: 0.5300, Training loss: 0.6928\n","1 Training Accuracy: 0.5000, Training loss: 0.6927\n","1 Training Accuracy: 0.5000, Training loss: 0.6925\n","1 Training Accuracy: 0.5000, Training loss: 0.6924\n","1 Training Accuracy: 0.5000, Training loss: 0.6922\n","1 Training Accuracy: 0.6200, Training loss: 0.6920\n","1 Training Accuracy: 0.5100, Training loss: 0.6922\n","1 Training Accuracy: 0.5000, Training loss: 0.6918\n","1 Training Accuracy: 0.5700, Training loss: 0.6918\n","1 Training Accuracy: 0.5100, Training loss: 0.6915\n","1 Training Accuracy: 0.5000, Training loss: 0.6916\n","1 Training Accuracy: 0.5000, Training loss: 0.6912\n","1 Training Accuracy: 0.5300, Training loss: 0.6910\n","1 Training Accuracy: 0.6300, Training loss: 0.6908\n","1 Training Accuracy: 0.6600, Training loss: 0.6908\n","1 Training Accuracy: 0.6700, Training loss: 0.6905\n","1 Training Accuracy: 0.5100, Training loss: 0.6904\n","1 Training Accuracy: 0.6900, Training loss: 0.6903\n","1 Training Accuracy: 0.5600, Training loss: 0.6903\n","1 Training Accuracy: 0.5000, Training loss: 0.6903\n","1 Training Accuracy: 0.5000, Training loss: 0.6900\n","1 Training Accuracy: 0.5000, Training loss: 0.6900\n","1 Training Accuracy: 0.5000, Training loss: 0.6900\n","1 Training Accuracy: 0.5000, Training loss: 0.6898\n","1 Training Accuracy: 0.6800, Training loss: 0.6892\n","1 Training Accuracy: 0.6600, Training loss: 0.6895\n","1 Training Accuracy: 0.6900, Training loss: 0.6892\n","1 Training Accuracy: 0.6500, Training loss: 0.6892\n","1 Training Accuracy: 0.5800, Training loss: 0.6889\n","1 Training Accuracy: 0.5900, Training loss: 0.6888\n","1 Training Accuracy: 0.6100, Training loss: 0.6887\n","1 Training Accuracy: 0.5700, Training loss: 0.6885\n","1 Training Accuracy: 0.5800, Training loss: 0.6885\n","1 Training Accuracy: 0.5200, Training loss: 0.6884\n","1 Training Accuracy: 0.5500, Training loss: 0.6882\n","1 Training Accuracy: 0.5100, Training loss: 0.6882\n","1 Training Accuracy: 0.5600, Training loss: 0.6881\n","1 Training Accuracy: 0.5400, Training loss: 0.6878\n","1 Training Accuracy: 0.6500, Training loss: 0.6875\n","1 Training Accuracy: 0.5400, Training loss: 0.6878\n","1 Training Accuracy: 0.5000, Training loss: 0.6879\n","1 Training Accuracy: 0.5000, Training loss: 0.6879\n","1 Training Accuracy: 0.5500, Training loss: 0.6871\n","1 Training Accuracy: 0.5700, Training loss: 0.6866\n","1 Training Accuracy: 0.5900, Training loss: 0.6863\n","1 Training Accuracy: 0.6200, Training loss: 0.6860\n","1 Training Accuracy: 0.6400, Training loss: 0.6859\n","1 Training Accuracy: 0.6200, Training loss: 0.6857\n","1 Training Accuracy: 0.5800, Training loss: 0.6858\n","1 Training Accuracy: 0.6500, Training loss: 0.6850\n","1 Training Accuracy: 0.6100, Training loss: 0.6854\n","1 Training Accuracy: 0.6200, Training loss: 0.6848\n","1 Training Accuracy: 0.6300, Training loss: 0.6848\n","1 Training Accuracy: 0.6300, Training loss: 0.6845\n","1 Training Accuracy: 0.6300, Training loss: 0.6843\n","1 Training Accuracy: 0.6200, Training loss: 0.6844\n","1 Training Accuracy: 0.5600, Training loss: 0.6850\n","1 Training Accuracy: 0.5600, Training loss: 0.6853\n","1 Training Accuracy: 0.5200, Training loss: 0.6849\n","1 Training Accuracy: 0.5500, Training loss: 0.6843\n","1 Training Accuracy: 0.6300, Training loss: 0.6833\n","1 Training Accuracy: 0.6100, Training loss: 0.6835\n","1 Training Accuracy: 0.6200, Training loss: 0.6831\n","1 Training Accuracy: 0.6100, Training loss: 0.6826\n","1 Training Accuracy: 0.6400, Training loss: 0.6829\n","1 Training Accuracy: 0.6300, Training loss: 0.6820\n","1 Training Accuracy: 0.6500, Training loss: 0.6818\n","1 Training Accuracy: 0.6300, Training loss: 0.6817\n","1 Training Accuracy: 0.6300, Training loss: 0.6820\n","1 Training Accuracy: 0.6300, Training loss: 0.6813\n","1 Training Accuracy: 0.6500, Training loss: 0.6806\n","1 Training Accuracy: 0.6100, Training loss: 0.6810\n","1 Training Accuracy: 0.6200, Training loss: 0.6803\n","1 Training Accuracy: 0.6300, Training loss: 0.6803\n","1 Training Accuracy: 0.6200, Training loss: 0.6793\n","1 Training Accuracy: 0.6200, Training loss: 0.6790\n","1 Training Accuracy: 0.6200, Training loss: 0.6788\n","1 Training Accuracy: 0.6200, Training loss: 0.6792\n","1 Training Accuracy: 0.6000, Training loss: 0.6779\n","1 Training Accuracy: 0.6100, Training loss: 0.6785\n","1 Training Accuracy: 0.6100, Training loss: 0.6777\n","1 Training Accuracy: 0.6300, Training loss: 0.6777\n","1 Training Accuracy: 0.6000, Training loss: 0.6775\n","1 Training Accuracy: 0.6600, Training loss: 0.6775\n","1 Training Accuracy: 0.6200, Training loss: 0.6779\n","1 Training Accuracy: 0.6700, Training loss: 0.6760\n","1 Training Accuracy: 0.6600, Training loss: 0.6760\n","1 Training Accuracy: 0.6600, Training loss: 0.6754\n","1 Training Accuracy: 0.6600, Training loss: 0.6751\n","1 Training Accuracy: 0.6600, Training loss: 0.6750\n","1 Training Accuracy: 0.6100, Training loss: 0.6746\n","1 Training Accuracy: 0.6000, Training loss: 0.6744\n","1 Training Accuracy: 0.6700, Training loss: 0.6738\n","1 Training Accuracy: 0.6600, Training loss: 0.6734\n","1 Training Accuracy: 0.6600, Training loss: 0.6731\n","1 Training Accuracy: 0.6300, Training loss: 0.6730\n","1 Training Accuracy: 0.5800, Training loss: 0.6725\n","1 Training Accuracy: 0.6200, Training loss: 0.6723\n","1 Training Accuracy: 0.6200, Training loss: 0.6721\n","1 Training Accuracy: 0.6000, Training loss: 0.6711\n","1 Training Accuracy: 0.6100, Training loss: 0.6712\n","1 Training Accuracy: 0.6000, Training loss: 0.6704\n","1 Training Accuracy: 0.6100, Training loss: 0.6704\n","1 Training Accuracy: 0.6400, Training loss: 0.6704\n","1 Training Accuracy: 0.6000, Training loss: 0.6697\n","1 Training Accuracy: 0.6200, Training loss: 0.6695\n","1 Training Accuracy: 0.6500, Training loss: 0.6689\n","1 Training Accuracy: 0.6500, Training loss: 0.6697\n","1 Training Accuracy: 0.6200, Training loss: 0.6680\n","1 Training Accuracy: 0.6700, Training loss: 0.6684\n","1 Training Accuracy: 0.6500, Training loss: 0.6680\n","1 Training Accuracy: 0.6100, Training loss: 0.6683\n","1 Training Accuracy: 0.6500, Training loss: 0.6677\n","1 Training Accuracy: 0.6600, Training loss: 0.6678\n","1 Training Accuracy: 0.6400, Training loss: 0.6675\n","1 Training Accuracy: 0.6600, Training loss: 0.6665\n","1 Training Accuracy: 0.6500, Training loss: 0.6660\n","1 Training Accuracy: 0.6500, Training loss: 0.6663\n","1 Training Accuracy: 0.6400, Training loss: 0.6658\n","1 Training Accuracy: 0.6000, Training loss: 0.6653\n","1 Training Accuracy: 0.6100, Training loss: 0.6674\n","1 Training Accuracy: 0.6100, Training loss: 0.6654\n","1 Training Accuracy: 0.6100, Training loss: 0.6662\n","1 Training Accuracy: 0.6200, Training loss: 0.6655\n","1 Training Accuracy: 0.6200, Training loss: 0.6650\n","1 Training Accuracy: 0.6100, Training loss: 0.6639\n","1 Training Accuracy: 0.6100, Training loss: 0.6637\n","1 Training Accuracy: 0.6100, Training loss: 0.6632\n","1 Training Accuracy: 0.6400, Training loss: 0.6625\n","1 Training Accuracy: 0.6100, Training loss: 0.6624\n","1 Training Accuracy: 0.6300, Training loss: 0.6617\n","1 Training Accuracy: 0.6300, Training loss: 0.6616\n","1 Training Accuracy: 0.6300, Training loss: 0.6620\n","1 Training Accuracy: 0.6300, Training loss: 0.6608\n","1 Training Accuracy: 0.6300, Training loss: 0.6606\n","1 Training Accuracy: 0.5900, Training loss: 0.6612\n","1 Training Accuracy: 0.6100, Training loss: 0.6606\n","1 Training Accuracy: 0.5900, Training loss: 0.6598\n","1 Training Accuracy: 0.6300, Training loss: 0.6599\n","1 Training Accuracy: 0.5900, Training loss: 0.6598\n","1 Training Accuracy: 0.6400, Training loss: 0.6595\n","1 Training Accuracy: 0.6500, Training loss: 0.6599\n","1 Training Accuracy: 0.6400, Training loss: 0.6592\n","1 Training Accuracy: 0.6400, Training loss: 0.6591\n","1 Training Accuracy: 0.6500, Training loss: 0.6595\n","1 Training Accuracy: 0.6400, Training loss: 0.6586\n","1 Training Accuracy: 0.6300, Training loss: 0.6585\n","1 Training Accuracy: 0.6100, Training loss: 0.6582\n","1 Training Accuracy: 0.6000, Training loss: 0.6580\n","1 Training Accuracy: 0.6000, Training loss: 0.6579\n","1 Training Accuracy: 0.6000, Training loss: 0.6580\n","1 Training Accuracy: 0.6100, Training loss: 0.6578\n","1 Training Accuracy: 0.6000, Training loss: 0.6574\n","1 Training Accuracy: 0.5900, Training loss: 0.6573\n","1 Training Accuracy: 0.5900, Training loss: 0.6569\n","1 Training Accuracy: 0.6000, Training loss: 0.6567\n","1 Training Accuracy: 0.5900, Training loss: 0.6567\n","1 Training Accuracy: 0.6000, Training loss: 0.6565\n","1 Training Accuracy: 0.6100, Training loss: 0.6567\n","1 Training Accuracy: 0.6100, Training loss: 0.6564\n","1 Training Accuracy: 0.6100, Training loss: 0.6564\n","1 Training Accuracy: 0.6000, Training loss: 0.6564\n","1 Training Accuracy: 0.6000, Training loss: 0.6562\n","1 Training Accuracy: 0.6000, Training loss: 0.6561\n","1 Training Accuracy: 0.6000, Training loss: 0.6562\n","1 Training Accuracy: 0.5900, Training loss: 0.6559\n","1 Training Accuracy: 0.6000, Training loss: 0.6563\n","1 Training Accuracy: 0.5900, Training loss: 0.6558\n","1 Training Accuracy: 0.6000, Training loss: 0.6561\n","1 Training Accuracy: 0.6000, Training loss: 0.6560\n","1 Training Accuracy: 0.6000, Training loss: 0.6558\n","1 Training Accuracy: 0.5900, Training loss: 0.6561\n","1 Training Accuracy: 0.5900, Training loss: 0.6558\n","1 Training Accuracy: 0.6000, Training loss: 0.6561\n","1 Training Accuracy: 0.6000, Training loss: 0.6557\n","1 Training Accuracy: 0.6000, Training loss: 0.6558\n","1 Training Accuracy: 0.6000, Training loss: 0.6558\n","1 Training Accuracy: 0.5900, Training loss: 0.6557\n","1 Training Accuracy: 0.6000, Training loss: 0.6556\n","1 Training Accuracy: 0.5900, Training loss: 0.6557\n","1 Training Accuracy: 0.6000, Training loss: 0.6557\n","1 Training Accuracy: 0.5900, Training loss: 0.6556\n","1 Training Accuracy: 0.5900, Training loss: 0.6556\n","1 Training Accuracy: 0.5900, Training loss: 0.6556\n","1 Training Accuracy: 0.5900, Training loss: 0.6556\n","1 Training Accuracy: 0.5900, Training loss: 0.6555\n","1 Training Accuracy: 0.5900, Training loss: 0.6555\n","1 Training Accuracy: 0.5900, Training loss: 0.6555\n","1 Training Accuracy: 0.5900, Training loss: 0.6555\n","1 Training Accuracy: 0.5900, Training loss: 0.6555\n","1 Training Accuracy: 0.5900, Training loss: 0.6555\n","1 Training Accuracy: 0.5900, Training loss: 0.6555\n","1 Training Accuracy: 0.5900, Training loss: 0.6555\n","1 Training Accuracy: 0.5900, Training loss: 0.6555\n","1 Training Accuracy: 0.5900, Training loss: 0.6555\n","1 Training Accuracy: 0.5900, Testing Accuracy: 0.5599\n","train with synthetic data, accuracy = 0.5599\n"]}]},{"cell_type":"code","source":["args.dataset = 'MHIST'\n","channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = utils.get_dataset(args.dataset, args.data_path)\n","args.batch_train = 32\n","\n","args.model = 'VGG11'\n","net = utils.get_network(args.model,channel=channel,num_classes=num_classes,im_size=im_size).to(args.device)\n","it_eval = args.num_eval\n","\n","images_train = image_syn_noise_mhist.to(args.device)\n","labels_train = label_syn_noise_mhist.to(args.device)\n","dst_train = utils.TensorDataset(images_train, labels_train)\n","trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n","it_eval = 1\n","acc_train, acc_test = evaluate_dataset(it_eval,net,trainloader,testloader,args)\n","print(\"train with synthetic data, accuracy = %.4f\"%(acc_test))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q6czjnRqb8N5","executionInfo":{"status":"ok","timestamp":1700952892318,"user_tz":300,"elapsed":24754,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"e1d5da15-b9d3-435a-ff26-ebf600cda559"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 Training Accuracy: 0.5200, Training loss: 1.8551\n","1 Training Accuracy: 0.4000, Training loss: 4.0590\n","1 Training Accuracy: 0.4400, Training loss: 3.5744\n","1 Training Accuracy: 0.4400, Training loss: 2.6629\n","1 Training Accuracy: 0.5200, Training loss: 3.0567\n","1 Training Accuracy: 0.4600, Training loss: 2.1445\n","1 Training Accuracy: 0.5000, Training loss: 1.5006\n","1 Training Accuracy: 0.7600, Training loss: 0.6437\n","1 Training Accuracy: 0.5200, Training loss: 2.5084\n","1 Training Accuracy: 0.4600, Training loss: 1.6501\n","1 Training Accuracy: 0.5200, Training loss: 1.2401\n","1 Training Accuracy: 0.3800, Training loss: 2.0906\n","1 Training Accuracy: 0.8800, Training loss: 0.4712\n","1 Training Accuracy: 0.5900, Training loss: 0.6321\n","1 Training Accuracy: 0.5000, Training loss: 0.9168\n","1 Training Accuracy: 0.6700, Training loss: 0.4612\n","1 Training Accuracy: 0.4800, Training loss: 1.4252\n","1 Training Accuracy: 0.4800, Training loss: 1.4983\n","1 Training Accuracy: 0.5200, Training loss: 2.3512\n","1 Training Accuracy: 0.5900, Training loss: 1.3278\n","1 Training Accuracy: 0.8600, Training loss: 0.4517\n","1 Training Accuracy: 0.3600, Training loss: 2.3477\n","1 Training Accuracy: 0.5900, Training loss: 1.3469\n","1 Training Accuracy: 0.7400, Training loss: 0.4741\n","1 Training Accuracy: 0.9500, Training loss: 0.2561\n","1 Training Accuracy: 1.0000, Training loss: 0.0543\n","1 Training Accuracy: 1.0000, Training loss: 0.0411\n","1 Training Accuracy: 1.0000, Training loss: 0.0320\n","1 Training Accuracy: 1.0000, Training loss: 0.0261\n","1 Training Accuracy: 1.0000, Training loss: 0.0224\n","1 Training Accuracy: 1.0000, Training loss: 0.0196\n","1 Training Accuracy: 1.0000, Training loss: 0.0173\n","1 Training Accuracy: 1.0000, Training loss: 0.0155\n","1 Training Accuracy: 1.0000, Training loss: 0.0139\n","1 Training Accuracy: 1.0000, Training loss: 0.0128\n","1 Training Accuracy: 1.0000, Training loss: 0.0118\n","1 Training Accuracy: 1.0000, Training loss: 0.0108\n","1 Training Accuracy: 1.0000, Training loss: 0.0100\n","1 Training Accuracy: 1.0000, Training loss: 0.0094\n","1 Training Accuracy: 1.0000, Training loss: 0.0088\n","1 Training Accuracy: 1.0000, Training loss: 0.0083\n","1 Training Accuracy: 1.0000, Training loss: 0.0079\n","1 Training Accuracy: 1.0000, Training loss: 0.0075\n","1 Training Accuracy: 1.0000, Training loss: 0.0071\n","1 Training Accuracy: 1.0000, Training loss: 0.0068\n","1 Training Accuracy: 1.0000, Training loss: 0.0065\n","1 Training Accuracy: 1.0000, Training loss: 0.0062\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0057\n","1 Training Accuracy: 1.0000, Training loss: 0.0055\n","1 Training Accuracy: 1.0000, Training loss: 0.0053\n","1 Training Accuracy: 1.0000, Training loss: 0.0051\n","1 Training Accuracy: 1.0000, Training loss: 0.0049\n","1 Training Accuracy: 1.0000, Training loss: 0.0048\n","1 Training Accuracy: 1.0000, Training loss: 0.0046\n","1 Training Accuracy: 1.0000, Training loss: 0.0045\n","1 Training Accuracy: 1.0000, Training loss: 0.0043\n","1 Training Accuracy: 1.0000, Training loss: 0.0042\n","1 Training Accuracy: 1.0000, Training loss: 0.0041\n","1 Training Accuracy: 1.0000, Training loss: 0.0040\n","1 Training Accuracy: 1.0000, Training loss: 0.0039\n","1 Training Accuracy: 1.0000, Training loss: 0.0038\n","1 Training Accuracy: 1.0000, Training loss: 0.0037\n","1 Training Accuracy: 1.0000, Training loss: 0.0036\n","1 Training Accuracy: 1.0000, Training loss: 0.0036\n","1 Training Accuracy: 1.0000, Training loss: 0.0035\n","1 Training Accuracy: 1.0000, Training loss: 0.0034\n","1 Training Accuracy: 1.0000, Training loss: 0.0033\n","1 Training Accuracy: 1.0000, Training loss: 0.0033\n","1 Training Accuracy: 1.0000, Training loss: 0.0032\n","1 Training Accuracy: 1.0000, Training loss: 0.0031\n","1 Training Accuracy: 1.0000, Training loss: 0.0031\n","1 Training Accuracy: 1.0000, Training loss: 0.0030\n","1 Training Accuracy: 1.0000, Training loss: 0.0030\n","1 Training Accuracy: 1.0000, Training loss: 0.0029\n","1 Training Accuracy: 1.0000, Training loss: 0.0029\n","1 Training Accuracy: 1.0000, Training loss: 0.0028\n","1 Training Accuracy: 1.0000, Training loss: 0.0028\n","1 Training Accuracy: 1.0000, Training loss: 0.0027\n","1 Training Accuracy: 1.0000, Training loss: 0.0027\n","1 Training Accuracy: 1.0000, Training loss: 0.0027\n","1 Training Accuracy: 1.0000, Training loss: 0.0026\n","1 Training Accuracy: 1.0000, Training loss: 0.0026\n","1 Training Accuracy: 1.0000, Training loss: 0.0025\n","1 Training Accuracy: 1.0000, Training loss: 0.0025\n","1 Training Accuracy: 1.0000, Training loss: 0.0025\n","1 Training Accuracy: 1.0000, Training loss: 0.0024\n","1 Training Accuracy: 1.0000, Training loss: 0.0024\n","1 Training Accuracy: 1.0000, Training loss: 0.0024\n","1 Training Accuracy: 1.0000, Training loss: 0.0024\n","1 Training Accuracy: 1.0000, Training loss: 0.0023\n","1 Training Accuracy: 1.0000, Training loss: 0.0023\n","1 Training Accuracy: 1.0000, Training loss: 0.0023\n","1 Training Accuracy: 1.0000, Training loss: 0.0023\n","1 Training Accuracy: 1.0000, Training loss: 0.0022\n","1 Training Accuracy: 1.0000, Training loss: 0.0022\n","1 Training Accuracy: 1.0000, Training loss: 0.0022\n","1 Training Accuracy: 1.0000, Training loss: 0.0022\n","1 Training Accuracy: 1.0000, Training loss: 0.0021\n","1 Training Accuracy: 1.0000, Training loss: 0.0021\n","1 Training Accuracy: 1.0000, Training loss: 0.0021\n","1 Training Accuracy: 1.0000, Training loss: 0.0021\n","1 Training Accuracy: 1.0000, Training loss: 0.0021\n","1 Training Accuracy: 1.0000, Training loss: 0.0021\n","1 Training Accuracy: 1.0000, Training loss: 0.0020\n","1 Training Accuracy: 1.0000, Training loss: 0.0020\n","1 Training Accuracy: 1.0000, Training loss: 0.0020\n","1 Training Accuracy: 1.0000, Training loss: 0.0020\n","1 Training Accuracy: 1.0000, Training loss: 0.0020\n","1 Training Accuracy: 1.0000, Training loss: 0.0020\n","1 Training Accuracy: 1.0000, Training loss: 0.0019\n","1 Training Accuracy: 1.0000, Training loss: 0.0019\n","1 Training Accuracy: 1.0000, Training loss: 0.0019\n","1 Training Accuracy: 1.0000, Training loss: 0.0019\n","1 Training Accuracy: 1.0000, Training loss: 0.0019\n","1 Training Accuracy: 1.0000, Training loss: 0.0019\n","1 Training Accuracy: 1.0000, Training loss: 0.0019\n","1 Training Accuracy: 1.0000, Training loss: 0.0019\n","1 Training Accuracy: 1.0000, Training loss: 0.0018\n","1 Training Accuracy: 1.0000, Training loss: 0.0018\n","1 Training Accuracy: 1.0000, Training loss: 0.0018\n","1 Training Accuracy: 1.0000, Training loss: 0.0018\n","1 Training Accuracy: 1.0000, Training loss: 0.0018\n","1 Training Accuracy: 1.0000, Training loss: 0.0018\n","1 Training Accuracy: 1.0000, Training loss: 0.0018\n","1 Training Accuracy: 1.0000, Training loss: 0.0018\n","1 Training Accuracy: 1.0000, Training loss: 0.0018\n","1 Training Accuracy: 1.0000, Training loss: 0.0018\n","1 Training Accuracy: 1.0000, Training loss: 0.0018\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0017\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Training loss: 0.0016\n","1 Training Accuracy: 1.0000, Testing Accuracy: 0.5527\n","train with synthetic data, accuracy = 0.5527\n"]}]},{"cell_type":"code","source":["args.dataset = 'MHIST'\n","channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = utils.get_dataset(args.dataset, args.data_path)\n","args.batch_train = 32\n","\n","args.model = 'ResNet18'\n","net = utils.get_network(args.model,channel=channel,num_classes=num_classes,im_size=im_size).to(args.device)\n","it_eval = args.num_eval\n","\n","images_train = image_syn_noise_mhist.to(args.device)\n","labels_train = label_syn_noise_mhist.to(args.device)\n","dst_train = utils.TensorDataset(images_train, labels_train)\n","trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n","it_eval = 1\n","acc_train, acc_test = evaluate_dataset(it_eval,net,trainloader,testloader,args)\n","print(\"train with synthetic data, accuracy = %.4f\"%(acc_test))\n"],"metadata":{"id":"d5aYzq0Wb840","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700952929013,"user_tz":300,"elapsed":36718,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"28bb39fa-dd1c-437b-be3d-25800c6943fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 Training Accuracy: 0.5600, Training loss: 0.7176\n","1 Training Accuracy: 0.6000, Training loss: 1.0314\n","1 Training Accuracy: 0.5000, Training loss: 0.8791\n","1 Training Accuracy: 0.6500, Training loss: 0.5975\n","1 Training Accuracy: 0.8300, Training loss: 0.5322\n","1 Training Accuracy: 1.0000, Training loss: 0.4831\n","1 Training Accuracy: 1.0000, Training loss: 0.4537\n","1 Training Accuracy: 0.6800, Training loss: 0.4752\n","1 Training Accuracy: 0.7600, Training loss: 0.5325\n","1 Training Accuracy: 1.0000, Training loss: 0.3408\n","1 Training Accuracy: 0.8500, Training loss: 0.3780\n","1 Training Accuracy: 0.9900, Training loss: 0.2583\n","1 Training Accuracy: 1.0000, Training loss: 0.2598\n","1 Training Accuracy: 1.0000, Training loss: 0.2198\n","1 Training Accuracy: 1.0000, Training loss: 0.2229\n","1 Training Accuracy: 1.0000, Training loss: 0.1705\n","1 Training Accuracy: 1.0000, Training loss: 0.1324\n","1 Training Accuracy: 1.0000, Training loss: 0.1317\n","1 Training Accuracy: 1.0000, Training loss: 0.1046\n","1 Training Accuracy: 1.0000, Training loss: 0.1009\n","1 Training Accuracy: 1.0000, Training loss: 0.0946\n","1 Training Accuracy: 1.0000, Training loss: 0.0712\n","1 Training Accuracy: 1.0000, Training loss: 0.0630\n","1 Training Accuracy: 1.0000, Training loss: 0.0565\n","1 Training Accuracy: 1.0000, Training loss: 0.0560\n","1 Training Accuracy: 1.0000, Training loss: 0.0501\n","1 Training Accuracy: 1.0000, Training loss: 0.0463\n","1 Training Accuracy: 1.0000, Training loss: 0.0423\n","1 Training Accuracy: 1.0000, Training loss: 0.0391\n","1 Training Accuracy: 1.0000, Training loss: 0.0405\n","1 Training Accuracy: 1.0000, Training loss: 0.0360\n","1 Training Accuracy: 1.0000, Training loss: 0.0341\n","1 Training Accuracy: 1.0000, Training loss: 0.0338\n","1 Training Accuracy: 1.0000, Training loss: 0.0296\n","1 Training Accuracy: 1.0000, Training loss: 0.0274\n","1 Training Accuracy: 1.0000, Training loss: 0.0277\n","1 Training Accuracy: 1.0000, Training loss: 0.0256\n","1 Training Accuracy: 1.0000, Training loss: 0.0251\n","1 Training Accuracy: 1.0000, Training loss: 0.0237\n","1 Training Accuracy: 1.0000, Training loss: 0.0223\n","1 Training Accuracy: 1.0000, Training loss: 0.0216\n","1 Training Accuracy: 1.0000, Training loss: 0.0215\n","1 Training Accuracy: 1.0000, Training loss: 0.0205\n","1 Training Accuracy: 1.0000, Training loss: 0.0194\n","1 Training Accuracy: 1.0000, Training loss: 0.0185\n","1 Training Accuracy: 1.0000, Training loss: 0.0179\n","1 Training Accuracy: 1.0000, Training loss: 0.0176\n","1 Training Accuracy: 1.0000, Training loss: 0.0168\n","1 Training Accuracy: 1.0000, Training loss: 0.0166\n","1 Training Accuracy: 1.0000, Training loss: 0.0162\n","1 Training Accuracy: 1.0000, Training loss: 0.0154\n","1 Training Accuracy: 1.0000, Training loss: 0.0153\n","1 Training Accuracy: 1.0000, Training loss: 0.0149\n","1 Training Accuracy: 1.0000, Training loss: 0.0144\n","1 Training Accuracy: 1.0000, Training loss: 0.0144\n","1 Training Accuracy: 1.0000, Training loss: 0.0141\n","1 Training Accuracy: 1.0000, Training loss: 0.0135\n","1 Training Accuracy: 1.0000, Training loss: 0.0134\n","1 Training Accuracy: 1.0000, Training loss: 0.0129\n","1 Training Accuracy: 1.0000, Training loss: 0.0126\n","1 Training Accuracy: 1.0000, Training loss: 0.0124\n","1 Training Accuracy: 1.0000, Training loss: 0.0123\n","1 Training Accuracy: 1.0000, Training loss: 0.0121\n","1 Training Accuracy: 1.0000, Training loss: 0.0119\n","1 Training Accuracy: 1.0000, Training loss: 0.0115\n","1 Training Accuracy: 1.0000, Training loss: 0.0114\n","1 Training Accuracy: 1.0000, Training loss: 0.0113\n","1 Training Accuracy: 1.0000, Training loss: 0.0111\n","1 Training Accuracy: 1.0000, Training loss: 0.0109\n","1 Training Accuracy: 1.0000, Training loss: 0.0106\n","1 Training Accuracy: 1.0000, Training loss: 0.0105\n","1 Training Accuracy: 1.0000, Training loss: 0.0103\n","1 Training Accuracy: 1.0000, Training loss: 0.0102\n","1 Training Accuracy: 1.0000, Training loss: 0.0100\n","1 Training Accuracy: 1.0000, Training loss: 0.0099\n","1 Training Accuracy: 1.0000, Training loss: 0.0097\n","1 Training Accuracy: 1.0000, Training loss: 0.0096\n","1 Training Accuracy: 1.0000, Training loss: 0.0095\n","1 Training Accuracy: 1.0000, Training loss: 0.0093\n","1 Training Accuracy: 1.0000, Training loss: 0.0093\n","1 Training Accuracy: 1.0000, Training loss: 0.0091\n","1 Training Accuracy: 1.0000, Training loss: 0.0090\n","1 Training Accuracy: 1.0000, Training loss: 0.0089\n","1 Training Accuracy: 1.0000, Training loss: 0.0089\n","1 Training Accuracy: 1.0000, Training loss: 0.0087\n","1 Training Accuracy: 1.0000, Training loss: 0.0086\n","1 Training Accuracy: 1.0000, Training loss: 0.0086\n","1 Training Accuracy: 1.0000, Training loss: 0.0084\n","1 Training Accuracy: 1.0000, Training loss: 0.0084\n","1 Training Accuracy: 1.0000, Training loss: 0.0083\n","1 Training Accuracy: 1.0000, Training loss: 0.0082\n","1 Training Accuracy: 1.0000, Training loss: 0.0081\n","1 Training Accuracy: 1.0000, Training loss: 0.0080\n","1 Training Accuracy: 1.0000, Training loss: 0.0079\n","1 Training Accuracy: 1.0000, Training loss: 0.0079\n","1 Training Accuracy: 1.0000, Training loss: 0.0078\n","1 Training Accuracy: 1.0000, Training loss: 0.0078\n","1 Training Accuracy: 1.0000, Training loss: 0.0077\n","1 Training Accuracy: 1.0000, Training loss: 0.0076\n","1 Training Accuracy: 1.0000, Training loss: 0.0076\n","1 Training Accuracy: 1.0000, Training loss: 0.0075\n","1 Training Accuracy: 1.0000, Training loss: 0.0075\n","1 Training Accuracy: 1.0000, Training loss: 0.0074\n","1 Training Accuracy: 1.0000, Training loss: 0.0073\n","1 Training Accuracy: 1.0000, Training loss: 0.0073\n","1 Training Accuracy: 1.0000, Training loss: 0.0072\n","1 Training Accuracy: 1.0000, Training loss: 0.0072\n","1 Training Accuracy: 1.0000, Training loss: 0.0071\n","1 Training Accuracy: 1.0000, Training loss: 0.0071\n","1 Training Accuracy: 1.0000, Training loss: 0.0070\n","1 Training Accuracy: 1.0000, Training loss: 0.0070\n","1 Training Accuracy: 1.0000, Training loss: 0.0070\n","1 Training Accuracy: 1.0000, Training loss: 0.0069\n","1 Training Accuracy: 1.0000, Training loss: 0.0069\n","1 Training Accuracy: 1.0000, Training loss: 0.0068\n","1 Training Accuracy: 1.0000, Training loss: 0.0068\n","1 Training Accuracy: 1.0000, Training loss: 0.0068\n","1 Training Accuracy: 1.0000, Training loss: 0.0067\n","1 Training Accuracy: 1.0000, Training loss: 0.0067\n","1 Training Accuracy: 1.0000, Training loss: 0.0067\n","1 Training Accuracy: 1.0000, Training loss: 0.0066\n","1 Training Accuracy: 1.0000, Training loss: 0.0066\n","1 Training Accuracy: 1.0000, Training loss: 0.0066\n","1 Training Accuracy: 1.0000, Training loss: 0.0065\n","1 Training Accuracy: 1.0000, Training loss: 0.0065\n","1 Training Accuracy: 1.0000, Training loss: 0.0065\n","1 Training Accuracy: 1.0000, Training loss: 0.0065\n","1 Training Accuracy: 1.0000, Training loss: 0.0064\n","1 Training Accuracy: 1.0000, Training loss: 0.0064\n","1 Training Accuracy: 1.0000, Training loss: 0.0064\n","1 Training Accuracy: 1.0000, Training loss: 0.0064\n","1 Training Accuracy: 1.0000, Training loss: 0.0063\n","1 Training Accuracy: 1.0000, Training loss: 0.0063\n","1 Training Accuracy: 1.0000, Training loss: 0.0063\n","1 Training Accuracy: 1.0000, Training loss: 0.0063\n","1 Training Accuracy: 1.0000, Training loss: 0.0063\n","1 Training Accuracy: 1.0000, Training loss: 0.0062\n","1 Training Accuracy: 1.0000, Training loss: 0.0062\n","1 Training Accuracy: 1.0000, Training loss: 0.0062\n","1 Training Accuracy: 1.0000, Training loss: 0.0062\n","1 Training Accuracy: 1.0000, Training loss: 0.0062\n","1 Training Accuracy: 1.0000, Training loss: 0.0061\n","1 Training Accuracy: 1.0000, Training loss: 0.0061\n","1 Training Accuracy: 1.0000, Training loss: 0.0061\n","1 Training Accuracy: 1.0000, Training loss: 0.0061\n","1 Training Accuracy: 1.0000, Training loss: 0.0061\n","1 Training Accuracy: 1.0000, Training loss: 0.0061\n","1 Training Accuracy: 1.0000, Training loss: 0.0061\n","1 Training Accuracy: 1.0000, Training loss: 0.0061\n","1 Training Accuracy: 1.0000, Training loss: 0.0060\n","1 Training Accuracy: 1.0000, Training loss: 0.0060\n","1 Training Accuracy: 1.0000, Training loss: 0.0060\n","1 Training Accuracy: 1.0000, Training loss: 0.0060\n","1 Training Accuracy: 1.0000, Training loss: 0.0060\n","1 Training Accuracy: 1.0000, Training loss: 0.0060\n","1 Training Accuracy: 1.0000, Training loss: 0.0060\n","1 Training Accuracy: 1.0000, Training loss: 0.0060\n","1 Training Accuracy: 1.0000, Training loss: 0.0060\n","1 Training Accuracy: 1.0000, Training loss: 0.0060\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0059\n","1 Training Accuracy: 1.0000, Training loss: 0.0058\n","1 Training Accuracy: 1.0000, Training loss: 0.0058\n","1 Training Accuracy: 1.0000, Training loss: 0.0058\n","1 Training Accuracy: 1.0000, Training loss: 0.0058\n","1 Training Accuracy: 1.0000, Training loss: 0.0058\n","1 Training Accuracy: 1.0000, Training loss: 0.0058\n","1 Training Accuracy: 1.0000, Training loss: 0.0058\n","1 Training Accuracy: 1.0000, Training loss: 0.0058\n","1 Training Accuracy: 1.0000, Training loss: 0.0058\n","1 Training Accuracy: 1.0000, Training loss: 0.0058\n","1 Training Accuracy: 1.0000, Training loss: 0.0058\n","1 Training Accuracy: 1.0000, Training loss: 0.0058\n","1 Training Accuracy: 1.0000, Training loss: 0.0058\n","1 Training Accuracy: 1.0000, Training loss: 0.0058\n","1 Training Accuracy: 1.0000, Training loss: 0.0058\n","1 Training Accuracy: 1.0000, Training loss: 0.0058\n","1 Training Accuracy: 1.0000, Training loss: 0.0058\n","1 Training Accuracy: 1.0000, Testing Accuracy: 0.5015\n","train with synthetic data, accuracy = 0.5015\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}