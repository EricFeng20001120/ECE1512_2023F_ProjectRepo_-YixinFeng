{"cells":[{"cell_type":"markdown","source":["### Reference\n","https://github.com/GzyAftermath/DATM/tree/main"],"metadata":{"id":"3w7R1D62LdQC"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17546,"status":"ok","timestamp":1701099332330,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"},"user_tz":300},"id":"xZxi8JKMNs9B","outputId":"7baafb69-4581-4986-fbe4-5647cfc2ffcc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/ECE1513/Project_B_Supp\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/ECE1513/Project_B_Supp/"]},{"cell_type":"markdown","source":["# Helper function"],"metadata":{"id":"SQchFMq7LYF5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VwAP7bJusH-_"},"outputs":[],"source":["import networks\n","import utils\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms,datasets\n","import tensorflow as tf\n","import pandas as pd\n","import torch.optim.lr_scheduler\n","import copy\n","from torchvision.utils import save_image\n","#from utils_gsam import get_dataset, get_network, get_daparam,TensorDataset, epoch, ParamDiffAug\n","\n","import sys\n","from tqdm import tqdm\n","import copy\n","\n","import contextlib\n","from torch.distributed import ReduceOp"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6428,"status":"ok","timestamp":1701099349938,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"},"user_tz":300},"id":"QWRrdHk3j5X6","outputId":"29a301b9-4f03-4503-f1d1-587393fb4cf6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting kornia\n","  Downloading kornia-0.7.0-py2.py3-none-any.whl (705 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/705.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/705.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.7/705.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (23.2)\n","Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.1.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.1->kornia) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n","Installing collected packages: kornia\n","Successfully installed kornia-0.7.0\n"]}],"source":["!pip install kornia"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Co90yPinlbov"},"outputs":[],"source":["args = type('', (), {})()\n","args.dataset = 'MHIST'\n","args.model = 'ConvNet'\n","args.num_experts = 100 #training iterations\n","args.lr_teacher = 0.01\n","args.batch_train = 256\n","args.batch_real = 256\n","args.dsa = True\n","args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","args.dsa_param = utils.ParamDiffAug()\n","args.dsa_strategy = 'color_crop_cutout_flip_scale_rotate'\n","args.data_path = './data'\n","args.buffer_path = './buffers'\n","args.train_epochs = 50\n","args.mom = 0\n","args.l2 = 0\n","args.save_interval = 10\n","args.rho_max = 2.0\n","args.rho_min = 2.0\n","args.alpha = 0.4\n","args.adaptive = True\n","args.zca = 'store_true'\n","args.decay = 'store_true'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":410,"status":"ok","timestamp":1701099350477,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"},"user_tz":300},"id":"Am9a_NnxjYbb","outputId":"76ae6d1a-0834-4ba3-8322-bfb8086821b0"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-4998b6eaed7a>:14: DeprecationWarning: Please use `rotate` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n","  from scipy.ndimage.interpolation import rotate as scipyrotate\n"]}],"source":["# adapted from\n","# https://github.com/VICO-UoE/DatasetCondensation\n","\n","import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import os\n","import kornia as K\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets, transforms\n","from scipy.ndimage.interpolation import rotate as scipyrotate\n","from networks import MLP, ConvNet, LeNet, AlexNet, VGG11BN, VGG11, ResNet18, ResNet18BN_AP, ResNet18_AP, ResNet18BN\n","\n","\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","\n","\n","# Custom dataset class to load the resized images and labels\n","class ResizedImageNetDataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.classes = sorted(os.listdir(root_dir))\n","        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n","        self.image_paths = [os.path.join(root, filename) for root, _, files in os.walk(root_dir) for filename in files if filename.endswith('.pt')]\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","        image = torch.load(image_path)\n","\n","        class_name = os.path.basename(os.path.dirname(image_path))\n","        label = self.class_to_idx[class_name]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label\n","\n","\n","class Config:\n","    imagenette = [0, 217, 482, 491, 497, 566, 569, 571, 574, 701]\n","\n","    # [\"australian_terrier\", \"border_terrier\", \"samoyed\", \"beagle\", \"shih-tzu\", \"english_foxhound\", \"rhodesian_ridgeback\", \"dingo\", \"golden_retriever\", \"english_sheepdog\"]\n","    imagewoof = [193, 182, 258, 162, 155, 167, 159, 273, 207, 229]\n","\n","    # [\"tabby_cat\", \"bengal_cat\", \"persian_cat\", \"siamese_cat\", \"egyptian_cat\", \"lion\", \"tiger\", \"jaguar\", \"snow_leopard\", \"lynx\"]\n","    imagemeow = [281, 282, 283, 284, 285, 291, 292, 290, 289, 287]\n","\n","    # [\"peacock\", \"flamingo\", \"macaw\", \"pelican\", \"king_penguin\", \"bald_eagle\", \"toucan\", \"ostrich\", \"black_swan\", \"cockatoo\"]\n","    imagesquawk = [84, 130, 88, 144, 145, 22, 96, 9, 100, 89]\n","\n","    # [\"pineapple\", \"banana\", \"strawberry\", \"orange\", \"lemon\", \"pomegranate\", \"fig\", \"bell_pepper\", \"cucumber\", \"green_apple\"]\n","    imagefruit = [953, 954, 949, 950, 951, 957, 952, 945, 943, 948]\n","\n","    # [\"bee\", \"ladys slipper\", \"banana\", \"lemon\", \"corn\", \"school_bus\", \"honeycomb\", \"lion\", \"garden_spider\", \"goldfinch\"]\n","    imageyellow = [309, 986, 954, 951, 987, 779, 599, 291, 72, 11]\n","\n","    dict = {\n","        \"imagenette\" : imagenette,\n","        \"imagewoof\" : imagewoof,\n","        \"imagefruit\": imagefruit,\n","        \"imageyellow\": imageyellow,\n","        \"imagemeow\": imagemeow,\n","        \"imagesquawk\": imagesquawk,\n","    }\n","\n","config = Config()\n","\n","def get_dataset(dataset, data_path, batch_size=1, subset=\"imagenette\"):\n","\n","    class_map = None\n","    loader_train_dict = None\n","    class_map_inv = None\n","\n","    if dataset == 'CIFAR10':\n","        channel = 3\n","        im_size = (32, 32)\n","        num_classes = 10\n","        mean = [0.4914, 0.4822, 0.4465]\n","        std = [0.2023, 0.1994, 0.2010]\n","        if args.zca:\n","            transform = transforms.Compose([transforms.ToTensor()])\n","        else:\n","            transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.CIFAR10(data_path, train=True, download=True, transform=transform) # no augmentation\n","        dst_test = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n","        class_names = dst_train.classes\n","        class_map = {x:x for x in range(num_classes)}\n","\n","    elif dataset == 'MNIST':\n","        channel = 1\n","        im_size = (28, 28)\n","        num_classes = 10\n","        mean = [0.1307]\n","        std = [0.3081]\n","        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.MNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n","        dst_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n","        class_names = [str(c) for c in range(num_classes)]\n","        class_map = {x:x for x in range(num_classes)}\n","\n","    elif dataset == 'MHIST':\n","        im_size = (64, 64)\n","        num_classes=2\n","        channel=3\n","        mean = [0,0,0]\n","        std = [0,0,0]\n","        train_dir = './mhist_dataset/augmentation'\n","        #train_dir = './mhist_dataset/train'\n","        test_dir = './mhist_dataset/test'\n","\n","        #mhist_data = datasets.ImageFolder(root=train_dir, transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()]))\n","        #mean, std = compute_mean_std(mhist_data)\n","\n","        transform_mhist = transforms.Compose([\n","            transforms.Resize(im_size),\n","            transforms.ToTensor(),\n","            #transforms.Normalize(mean=mean, std=std)\n","        ])\n","        dst_train = datasets.ImageFolder(root=train_dir, transform=transform_mhist)\n","        dst_test = datasets.ImageFolder(root=test_dir, transform=transform_mhist)\n","        class_names = [str(c) for c in range(num_classes)]\n","        class_map = {x:x for x in range(num_classes)}\n","\n","\n","    elif dataset == 'Tiny':\n","        channel = 3\n","        im_size = (64, 64)\n","        num_classes = 200\n","        mean = [0.485, 0.456, 0.406]\n","        std = [0.229, 0.224, 0.225]\n","        if args.zca:\n","            transform = transforms.Compose([transforms.ToTensor()])\n","        else:\n","            transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.ImageFolder(os.path.join(data_path, \"train\"), transform=transform) # no augmentation\n","        dst_test = datasets.ImageFolder(os.path.join(data_path, \"val\", \"images\"), transform=transform)\n","        class_names = dst_train.classes\n","        class_map = {x:x for x in range(num_classes)}\n","\n","\n","    elif dataset == 'ImageNet':\n","        channel = 3\n","        im_size = (128, 128)\n","        num_classes = 10\n","\n","        config.img_net_classes = config.dict[subset]\n","\n","        mean = [0.485, 0.456, 0.406]\n","        std = [0.229, 0.224, 0.225]\n","        if args.zca:\n","            transform = transforms.Compose([transforms.ToTensor(),\n","                                        transforms.Resize(im_size),\n","                                        transforms.CenterCrop(im_size)])\n","        else:\n","            transform = transforms.Compose([transforms.ToTensor(),\n","                                            transforms.Normalize(mean=mean, std=std),\n","                                            transforms.Resize(im_size),\n","                                            transforms.CenterCrop(im_size)])\n","\n","        dst_train = datasets.ImageNet(data_path, split=\"train\", transform=transform) # no augmentation\n","        dst_train_dict = {c : torch.utils.data.Subset(dst_train, np.squeeze(np.argwhere(np.equal(dst_train.targets, config.img_net_classes[c])))) for c in range(len(config.img_net_classes))}\n","        dst_train = torch.utils.data.Subset(dst_train, np.squeeze(np.argwhere(np.isin(dst_train.targets, config.img_net_classes))))\n","        loader_train_dict = {c : torch.utils.data.DataLoader(dst_train_dict[c], batch_size=batch_size, shuffle=True, num_workers=16) for c in range(len(config.img_net_classes))}\n","        dst_test = datasets.ImageNet(data_path, split=\"val\", transform=transform)\n","        dst_test = torch.utils.data.Subset(dst_test, np.squeeze(np.argwhere(np.isin(dst_test.targets, config.img_net_classes))))\n","        for c in range(len(config.img_net_classes)):\n","            dst_test.dataset.targets[dst_test.dataset.targets == config.img_net_classes[c]] = c\n","            dst_train.dataset.targets[dst_train.dataset.targets == config.img_net_classes[c]] = c\n","        print(dst_test.dataset)\n","        class_map = {x: i for i, x in enumerate(config.img_net_classes)}\n","        class_map_inv = {i: x for i, x in enumerate(config.img_net_classes)}\n","        class_names = None\n","\n","\n","    elif dataset.startswith('CIFAR100'):\n","        channel = 3\n","        im_size = (32, 32)\n","        num_classes = 100\n","        mean = [0.4914, 0.4822, 0.4465]\n","        std = [0.2023, 0.1994, 0.2010]\n","\n","        if args.zca:\n","            transform = transforms.Compose([transforms.ToTensor()])\n","        else:\n","            transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n","        dst_train = datasets.CIFAR100(data_path, train=True, download=True, transform=transform)  # no augmentation\n","        dst_test = datasets.CIFAR100(data_path, train=False, download=True, transform=transform)\n","        class_names = dst_train.classes\n","        class_map = {x: x for x in range(num_classes)}\n","\n","    elif dataset == 'ImageNet1K':\n","        channel = 3\n","        im_size = (64, 64)\n","        num_classes = 1000\n","        mean = [0.485, 0.456, 0.406]\n","        std = [0.229, 0.224, 0.225]\n","\n","        data_transforms = {\n","            'train': transforms.Compose([\n","                # transforms.Resize(im_size),\n","                # transforms.ToTensor(),\n","                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","            ]),\n","            'val': transforms.Compose([\n","                # transforms.Resize(im_size),\n","                # transforms.ToTensor(),\n","                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","            ]),\n","        }\n","\n","        # Create datasets and data loaders for training and testing\n","        dst_train = ResizedImageNetDataset(root_dir=os.path.join(data_path, \"train\"), transform=data_transforms['train'])\n","        dst_test = ResizedImageNetDataset(root_dir=os.path.join(data_path, \"val\"), transform=data_transforms['val'])\n","\n","        # dst_train = datasets.ImageFolder(os.path.join(data_path, \"train\"), transform=data_transforms['train']) # no augmentation\n","        # dst_test = datasets.ImageFolder(os.path.join(data_path, \"val\"), transform=data_transforms['val'])\n","        class_names = dst_train.classes\n","        class_map = {x:x for x in range(num_classes)}\n","\n","    else:\n","        exit('unknown dataset: %s'%dataset)\n","\n","    if args.zca:\n","        images = []\n","        labels = []\n","        print(\"Train ZCA\")\n","        for i in tqdm(range(len(dst_train))):\n","            im, lab = dst_train[i]\n","            images.append(im)\n","            labels.append(lab)\n","        images = torch.stack(images, dim=0).to(\"cpu\")\n","        labels = torch.tensor(labels, dtype=torch.long, device=\"cpu\")\n","        zca = K.enhance.ZCAWhitening(eps=0.1, compute_inv=True)\n","        zca.fit(images)\n","        zca_images = zca(images).to(\"cpu\")\n","        dst_train = TensorDataset(zca_images, labels)\n","\n","        images = []\n","        labels = []\n","        print(\"Test ZCA\")\n","        for i in tqdm(range(len(dst_test))):\n","            im, lab = dst_test[i]\n","            images.append(im)\n","            labels.append(lab)\n","        images = torch.stack(images, dim=0).to(\"cpu\")\n","        labels = torch.tensor(labels, dtype=torch.long, device=\"cpu\")\n","\n","        zca_images = zca(images).to(\"cpu\")\n","        dst_test = TensorDataset(zca_images, labels)\n","\n","        args.zca_trans = zca\n","\n","\n","    testloader = torch.utils.data.DataLoader(dst_test, batch_size=128, shuffle=False, num_workers=2)\n","\n","\n","    return channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader, loader_train_dict, class_map, class_map_inv\n","\n","\n","\n","class TensorDataset(Dataset):\n","    def __init__(self, images, labels): # images: n x c x h x w tensor\n","        self.images = images.detach().float()\n","        self.labels = labels.detach()\n","\n","    def __getitem__(self, index):\n","        return self.images[index], self.labels[index]\n","\n","    def __len__(self):\n","        return self.images.shape[0]\n","\n","\n","\n","def get_default_convnet_setting():\n","    net_width, net_depth, net_act, net_norm, net_pooling = 128, 3, 'relu', 'instancenorm', 'avgpooling'\n","    return net_width, net_depth, net_act, net_norm, net_pooling\n","\n","\n","\n","def get_network(model, channel, num_classes, im_size=(32, 32), dist=True):\n","    torch.random.manual_seed(int(time.time() * 1000) % 100000)\n","    net_width, net_depth, net_act, net_norm, net_pooling = get_default_convnet_setting()\n","\n","    if model == 'MLP':\n","        net = MLP(channel=channel, num_classes=num_classes)\n","    elif model == 'ConvNet':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'LeNet':\n","        net = LeNet(channel=channel, num_classes=num_classes)\n","    elif model == 'AlexNet':\n","        net = AlexNet(channel=channel, num_classes=num_classes)\n","    elif model == 'VGG11':\n","        net = VGG11( channel=channel, num_classes=num_classes)\n","    elif model == 'VGG11BN':\n","        net = VGG11BN(channel=channel, num_classes=num_classes)\n","    elif model == 'ResNet18':\n","        net = ResNet18(channel=channel, num_classes=num_classes)\n","    elif model == 'ResNet18BN':\n","        net = ResNet18BN(channel=channel, num_classes=num_classes)\n","    elif model == 'ResNet18BN_AP':\n","        net = ResNet18BN_AP(channel=channel, num_classes=num_classes)\n","    elif model == 'ResNet18_AP':\n","        net = ResNet18_AP(channel=channel, num_classes=num_classes)\n","\n","    elif model == 'ConvNetD1':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=1, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD2':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=2, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD3':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=3, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD4':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=4, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD5':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=5, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD6':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=6, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD7':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=7, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","    elif model == 'ConvNetD8':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=8, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n","\n","\n","    elif model == 'ConvNetW32':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=32, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling)\n","    elif model == 'ConvNetW64':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=64, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling)\n","    elif model == 'ConvNetW128':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=128, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling)\n","    elif model == 'ConvNetW256':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=256, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling)\n","    elif model == 'ConvNetW512':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=512, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling)\n","    elif model == 'ConvNetW1024':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=1024, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling)\n","\n","    elif model == \"ConvNetKIP\":\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=1024, net_depth=net_depth, net_act=net_act,\n","                      net_norm=\"none\", net_pooling=net_pooling)\n","\n","    elif model == 'ConvNetAS':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='sigmoid', net_norm=net_norm, net_pooling=net_pooling)\n","    elif model == 'ConvNetAR':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='relu', net_norm=net_norm, net_pooling=net_pooling)\n","    elif model == 'ConvNetAL':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='leakyrelu', net_norm=net_norm, net_pooling=net_pooling)\n","\n","    elif model == 'ConvNetNN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='none', net_pooling=net_pooling)\n","    elif model == 'ConvNetBN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='batchnorm', net_pooling=net_pooling)\n","    elif model == 'ConvNetLN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='layernorm', net_pooling=net_pooling)\n","    elif model == 'ConvNetIN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='instancenorm', net_pooling=net_pooling)\n","    elif model == 'ConvNetGN':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='groupnorm', net_pooling=net_pooling)\n","\n","    elif model == 'ConvNetNP':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='none')\n","    elif model == 'ConvNetMP':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='maxpooling')\n","    elif model == 'ConvNetAP':\n","        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='avgpooling')\n","\n","\n","    else:\n","        net = None\n","        exit('DC error: unknown model')\n","\n","    if dist:\n","        gpu_num = torch.cuda.device_count()\n","        if gpu_num>0:\n","            device = 'cuda'\n","            if gpu_num>1:\n","                net = nn.DataParallel(net)\n","        else:\n","            device = 'cpu'\n","        net = net.to(device)\n","\n","    return net\n","\n","\n","\n","\n","def smooth_crossentropy(pred, gold, smoothing=0.1):\n","    n_class = pred.size(1)\n","\n","    one_hot = torch.full_like(pred, fill_value=smoothing / (n_class - 1))\n","    one_hot.scatter_(dim=1, index=gold.unsqueeze(1), value=1.0 - smoothing)\n","    log_prob = F.log_softmax(pred, dim=1)\n","\n","    return F.kl_div(input=log_prob, target=one_hot, reduction='none').sum(-1)\n","\n","\n","\n","\n","def get_time():\n","    return str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n","\n","\n","def epoch(mode, dataloader, net, optimizer, criterion, args, aug,scheduler, texture=False):\n","    loss_avg, acc_avg, num_exp = 0, 0, 0\n","    net = net.to(args.device)\n","\n","    if args.dataset == \"ImageNet\":\n","        class_map = {x: i for i, x in enumerate(config.img_net_classes)}\n","\n","    if mode == 'train':\n","        net.train()\n","    else:\n","        net.eval()\n","\n","    for i_batch, datum in enumerate(dataloader):\n","        img = datum[0].float().to(args.device)\n","        lab = datum[1].long().to(args.device)\n","\n","        if mode == \"train\" and texture:\n","            img = torch.cat([torch.stack([torch.roll(im, (torch.randint(args.im_size[0]*args.canvas_size, (1,)), torch.randint(args.im_size[0]*args.canvas_size, (1,))), (1,2))[:,:args.im_size[0],:args.im_size[1]] for im in img]) for _ in range(args.canvas_samples)])\n","            lab = torch.cat([lab for _ in range(args.canvas_samples)])\n","\n","        if aug:\n","            if args.dsa:\n","                img = DiffAugment(img, args.dsa_strategy, param=args.dsa_param)\n","            else:\n","                img = augment(img, args.dc_aug_param, device=args.device)\n","\n","        if args.dataset == \"ImageNet\" and mode != \"train\":\n","            lab = torch.tensor([class_map[x.item()] for x in lab]).to(args.device)\n","\n","        n_b = lab.shape[0]\n","\n","        ##GSAM\n","        if mode == 'train':\n","            def loss_fn(predictions, targets):\n","                #return smooth_crossentropy(predictions, targets,smoothing=args.label_smoothing).mean()\n","                return criterion(predictions, targets)\n","\n","            optimizer.set_closure(loss_fn, img, lab)\n","            output, loss = optimizer.step()\n","            #print(loss)\n","\n","            with torch.no_grad():\n","                acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n","                loss_avg += loss.item()*n_b\n","                acc_avg += acc\n","                num_exp += n_b\n","\n","                scheduler.step()\n","                optimizer.update_rho_t()\n","        else:\n","            with torch.no_grad():\n","                output = net(img)\n","                #loss = smooth_crossentropy(output, lab)\n","                loss = criterion(output, lab)\n","                acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n","                loss_avg += loss.item()*n_b\n","                acc_avg += acc\n","                num_exp += n_b\n","\n","\n","    loss_avg /= num_exp\n","    acc_avg /= num_exp\n","\n","    return loss_avg, acc_avg\n","\n","\n","\n","def evaluate_synset(it_eval, net, images_train, labels_train, testloader, args, return_loss=False, texture=False):\n","    net = net.to(args.device)\n","    images_train = images_train.to(args.device)\n","    labels_train = labels_train.to(args.device)\n","    lr = float(args.lr_net)\n","    Epoch = int(args.epoch_eval_train)\n","    lr_schedule = [Epoch//2+1]\n","    optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n","\n","    criterion = nn.CrossEntropyLoss().to(args.device)\n","\n","    dst_train = TensorDataset(images_train, labels_train)\n","    trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n","\n","    start = time.time()\n","    acc_train_list = []\n","    loss_train_list = []\n","\n","    for ep in tqdm.tqdm(range(Epoch+1)):\n","        loss_train, acc_train = epoch('train', trainloader, net, optimizer, criterion, args, aug=True, texture=texture)\n","        acc_train_list.append(acc_train)\n","        loss_train_list.append(loss_train)\n","        if ep == Epoch:\n","            with torch.no_grad():\n","                loss_test, acc_test = epoch('test', testloader, net, optimizer, criterion, args, aug=False)\n","        if ep in lr_schedule:\n","            lr *= 0.1\n","            optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n","\n","\n","    time_train = time.time() - start\n","\n","    print('%s Evaluate_%02d: epoch = %04d train time = %d s train loss = %.6f train acc = %.4f, test acc = %.4f' % (get_time(), it_eval, Epoch, int(time_train), loss_train, acc_train, acc_test))\n","\n","    if return_loss:\n","        return net, acc_train_list, acc_test, loss_train_list, loss_test\n","    else:\n","        return net, acc_train_list, acc_test\n","\n","\n","def augment(images, dc_aug_param, device):\n","    # This can be sped up in the future.\n","\n","    if dc_aug_param != None and dc_aug_param['strategy'] != 'none':\n","        scale = dc_aug_param['scale']\n","        crop = dc_aug_param['crop']\n","        rotate = dc_aug_param['rotate']\n","        noise = dc_aug_param['noise']\n","        strategy = dc_aug_param['strategy']\n","\n","        shape = images.shape\n","        mean = []\n","        for c in range(shape[1]):\n","            mean.append(float(torch.mean(images[:,c])))\n","\n","        def cropfun(i):\n","            im_ = torch.zeros(shape[1],shape[2]+crop*2,shape[3]+crop*2, dtype=torch.float, device=device)\n","            for c in range(shape[1]):\n","                im_[c] = mean[c]\n","            im_[:, crop:crop+shape[2], crop:crop+shape[3]] = images[i]\n","            r, c = np.random.permutation(crop*2)[0], np.random.permutation(crop*2)[0]\n","            images[i] = im_[:, r:r+shape[2], c:c+shape[3]]\n","\n","        def scalefun(i):\n","            h = int((np.random.uniform(1 - scale, 1 + scale)) * shape[2])\n","            w = int((np.random.uniform(1 - scale, 1 + scale)) * shape[2])\n","            tmp = F.interpolate(images[i:i + 1], [h, w], )[0]\n","            mhw = max(h, w, shape[2], shape[3])\n","            im_ = torch.zeros(shape[1], mhw, mhw, dtype=torch.float, device=device)\n","            r = int((mhw - h) / 2)\n","            c = int((mhw - w) / 2)\n","            im_[:, r:r + h, c:c + w] = tmp\n","            r = int((mhw - shape[2]) / 2)\n","            c = int((mhw - shape[3]) / 2)\n","            images[i] = im_[:, r:r + shape[2], c:c + shape[3]]\n","\n","        def rotatefun(i):\n","            im_ = scipyrotate(images[i].cpu().data.numpy(), angle=np.random.randint(-rotate, rotate), axes=(-2, -1), cval=np.mean(mean))\n","            r = int((im_.shape[-2] - shape[-2]) / 2)\n","            c = int((im_.shape[-1] - shape[-1]) / 2)\n","            images[i] = torch.tensor(im_[:, r:r + shape[-2], c:c + shape[-1]], dtype=torch.float, device=device)\n","\n","        def noisefun(i):\n","            images[i] = images[i] + noise * torch.randn(shape[1:], dtype=torch.float, device=device)\n","\n","\n","        augs = strategy.split('_')\n","\n","        for i in range(shape[0]):\n","            choice = np.random.permutation(augs)[0] # randomly implement one augmentation\n","            if choice == 'crop':\n","                cropfun(i)\n","            elif choice == 'scale':\n","                scalefun(i)\n","            elif choice == 'rotate':\n","                rotatefun(i)\n","            elif choice == 'noise':\n","                noisefun(i)\n","\n","    return images\n","\n","\n","\n","def get_daparam(dataset, model, model_eval, ipc):\n","    # We find that augmentation doesn't always benefit the performance.\n","    # So we do augmentation for some of the settings.\n","\n","    dc_aug_param = dict()\n","    dc_aug_param['crop'] = 4\n","    dc_aug_param['scale'] = 0.2\n","    dc_aug_param['rotate'] = 45\n","    dc_aug_param['noise'] = 0.001\n","    dc_aug_param['strategy'] = 'none'\n","\n","    if dataset == 'MNIST':\n","        dc_aug_param['strategy'] = 'crop_scale_rotate'\n","\n","    if model_eval in ['ConvNetBN']:  # Data augmentation makes model training with Batch Norm layer easier.\n","        dc_aug_param['strategy'] = 'crop_noise'\n","\n","    return dc_aug_param\n","\n","\n","def get_eval_pool(eval_mode, model, model_eval):\n","    if eval_mode == 'M': # multiple architectures\n","        # model_eval_pool = ['MLP', 'ConvNet', 'AlexNet', 'VGG11', 'ResNet18', 'LeNet']\n","        model_eval_pool = ['ConvNet', 'AlexNet', 'VGG11', 'ResNet18_AP', 'ResNet18']\n","        # model_eval_pool = ['MLP', 'ConvNet', 'AlexNet', 'VGG11', 'ResNet18']\n","    elif eval_mode == 'W': # ablation study on network width\n","        model_eval_pool = ['ConvNetW32', 'ConvNetW64', 'ConvNetW128', 'ConvNetW256']\n","    elif eval_mode == 'D': # ablation study on network depth\n","        model_eval_pool = ['ConvNetD1', 'ConvNetD2', 'ConvNetD3', 'ConvNetD4']\n","    elif eval_mode == 'A': # ablation study on network activation function\n","        model_eval_pool = ['ConvNetAS', 'ConvNetAR', 'ConvNetAL']\n","    elif eval_mode == 'P': # ablation study on network pooling layer\n","        model_eval_pool = ['ConvNetNP', 'ConvNetMP', 'ConvNetAP']\n","    elif eval_mode == 'N': # ablation study on network normalization layer\n","        model_eval_pool = ['ConvNetNN', 'ConvNetBN', 'ConvNetLN', 'ConvNetIN', 'ConvNetGN']\n","    elif eval_mode == 'S': # itself\n","        model_eval_pool = [model[:model.index('BN')]] if 'BN' in model else [model]\n","    elif eval_mode == 'C':\n","        model_eval_pool = [model, 'ConvNet']\n","    elif eval_mode == 'BN':\n","        model_eval_pool = ['ConvNet','ConvNetBN','ResNet18','ResNet18BN','AlexNet', 'VGG11', 'ResNet18_AP']\n","    else:\n","        model_eval_pool = [model_eval]\n","    return model_eval_pool\n","\n","\n","class ParamDiffAug():\n","    def __init__(self):\n","        self.aug_mode = 'S' #'multiple or single'\n","        self.prob_flip = 0.5\n","        self.ratio_scale = 1.2\n","        self.ratio_rotate = 15.0\n","        self.ratio_crop_pad = 0.125\n","        self.ratio_cutout = 0.5 # the size would be 0.5x0.5\n","        self.ratio_noise = 0.05\n","        self.brightness = 1.0\n","        self.saturation = 2.0\n","        self.contrast = 0.5\n","\n","\n","def set_seed_DiffAug(param):\n","    if param.latestseed == -1:\n","        return\n","    else:\n","        torch.random.manual_seed(param.latestseed)\n","        param.latestseed += 1\n","\n","\n","def DiffAugment(x, strategy='', seed = -1, param = None):\n","    if seed == -1:\n","        param.batchmode = False\n","    else:\n","        param.batchmode = True\n","\n","    param.latestseed = seed\n","\n","    if strategy == 'None' or strategy == 'none':\n","        return x\n","\n","    if strategy:\n","        if param.aug_mode == 'M': # original\n","            for p in strategy.split('_'):\n","                for f in AUGMENT_FNS[p]:\n","                    x = f(x, param)\n","        elif param.aug_mode == 'S':\n","            pbties = strategy.split('_')\n","            set_seed_DiffAug(param)\n","            p = pbties[torch.randint(0, len(pbties), size=(1,)).item()]\n","            for f in AUGMENT_FNS[p]:\n","                x = f(x, param)\n","        else:\n","            exit('Error ZH: unknown augmentation mode.')\n","        x = x.contiguous()\n","    return x\n","\n","\n","# We implement the following differentiable augmentation strategies based on the code provided in https://github.com/mit-han-lab/data-efficient-gans.\n","def rand_scale(x, param):\n","    # x>1, max scale\n","    # sx, sy: (0, +oo), 1: orignial size, 0.5: enlarge 2 times\n","    ratio = param.ratio_scale\n","    set_seed_DiffAug(param)\n","    sx = torch.rand(x.shape[0]) * (ratio - 1.0/ratio) + 1.0/ratio\n","    set_seed_DiffAug(param)\n","    sy = torch.rand(x.shape[0]) * (ratio - 1.0/ratio) + 1.0/ratio\n","    theta = [[[sx[i], 0,  0],\n","            [0,  sy[i], 0],] for i in range(x.shape[0])]\n","    theta = torch.tensor(theta, dtype=torch.float)\n","    if param.batchmode: # batch-wise:\n","        theta[:] = theta[0]\n","    grid = F.affine_grid(theta, x.shape, align_corners=True).to(x.device)\n","    x = F.grid_sample(x, grid, align_corners=True)\n","    return x\n","\n","\n","def rand_rotate(x, param): # [-180, 180], 90: anticlockwise 90 degree\n","    ratio = param.ratio_rotate\n","    set_seed_DiffAug(param)\n","    theta = (torch.rand(x.shape[0]) - 0.5) * 2 * ratio / 180 * float(np.pi)\n","    theta = [[[torch.cos(theta[i]), torch.sin(-theta[i]), 0],\n","        [torch.sin(theta[i]), torch.cos(theta[i]),  0],]  for i in range(x.shape[0])]\n","    theta = torch.tensor(theta, dtype=torch.float)\n","    if param.batchmode: # batch-wise:\n","        theta[:] = theta[0]\n","    grid = F.affine_grid(theta, x.shape, align_corners=True).to(x.device)\n","    x = F.grid_sample(x, grid, align_corners=True)\n","    return x\n","\n","\n","def rand_flip(x, param):\n","    prob = param.prob_flip\n","    set_seed_DiffAug(param)\n","    randf = torch.rand(x.size(0), 1, 1, 1, device=x.device)\n","    if param.batchmode: # batch-wise:\n","        randf[:] = randf[0]\n","    return torch.where(randf < prob, x.flip(3), x)\n","\n","\n","def rand_brightness(x, param):\n","    ratio = param.brightness\n","    set_seed_DiffAug(param)\n","    randb = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n","    if param.batchmode:  # batch-wise:\n","        randb[:] = randb[0]\n","    x = x + (randb - 0.5)*ratio\n","    return x\n","\n","\n","def rand_saturation(x, param):\n","    ratio = param.saturation\n","    x_mean = x.mean(dim=1, keepdim=True)\n","    set_seed_DiffAug(param)\n","    rands = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n","    if param.batchmode:  # batch-wise:\n","        rands[:] = rands[0]\n","    x = (x - x_mean) * (rands * ratio) + x_mean\n","    return x\n","\n","\n","def rand_contrast(x, param):\n","    ratio = param.contrast\n","    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n","    set_seed_DiffAug(param)\n","    randc = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n","    if param.batchmode:  # batch-wise:\n","        randc[:] = randc[0]\n","    x = (x - x_mean) * (randc + ratio) + x_mean\n","    return x\n","\n","\n","def rand_crop(x, param):\n","    # The image is padded on its surrounding and then cropped.\n","    ratio = param.ratio_crop_pad\n","    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n","    set_seed_DiffAug(param)\n","    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n","    set_seed_DiffAug(param)\n","    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n","    if param.batchmode:  # batch-wise:\n","        translation_x[:] = translation_x[0]\n","        translation_y[:] = translation_y[0]\n","    grid_batch, grid_x, grid_y = torch.meshgrid(\n","        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n","        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n","        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n","    )\n","    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n","    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n","    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n","    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n","    return x\n","\n","\n","def rand_cutout(x, param):\n","    ratio = param.ratio_cutout\n","    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n","    set_seed_DiffAug(param)\n","    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n","    set_seed_DiffAug(param)\n","    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n","    if param.batchmode:  # batch-wise:\n","        offset_x[:] = offset_x[0]\n","        offset_y[:] = offset_y[0]\n","    grid_batch, grid_x, grid_y = torch.meshgrid(\n","        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n","        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n","        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n","    )\n","    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n","    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n","    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n","    mask[grid_batch, grid_x, grid_y] = 0\n","    x = x * mask.unsqueeze(1)\n","    return x\n","\n","\n","AUGMENT_FNS = {\n","    'color': [rand_brightness, rand_saturation, rand_contrast],\n","    'crop': [rand_crop],\n","    'cutout': [rand_cutout],\n","    'flip': [rand_flip],\n","    'scale': [rand_scale],\n","    'rotate': [rand_rotate],\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CEQeFMHLkgoB"},"outputs":[],"source":["from torch.nn.modules.batchnorm import _BatchNorm\n","def disable_running_stats(model):\n","    def _disable(module):\n","        if isinstance(module, _BatchNorm):\n","            module.backup_momentum = module.momentum\n","            module.momentum = 0\n","\n","    model.apply(_disable)\n","\n","def enable_running_stats(model):\n","    def _enable(module):\n","        if isinstance(module, _BatchNorm) and hasattr(module, \"backup_momentum\"):\n","            module.momentum = module.backup_momentum\n","\n","    model.apply(_enable)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o3PkqDcSwJAV"},"outputs":[],"source":["class GSAM(torch.optim.Optimizer):\n","    def __init__(self, params, base_optimizer, model, gsam_alpha, rho_scheduler, adaptive=False, perturb_eps=1e-12, grad_reduce='mean', **kwargs):\n","        defaults = dict(adaptive=adaptive, **kwargs)\n","        super(GSAM, self).__init__(params, defaults)\n","        self.model = model\n","        self.base_optimizer = base_optimizer\n","        self.param_groups = self.base_optimizer.param_groups\n","        self.adaptive = adaptive\n","        self.rho_scheduler = rho_scheduler\n","        self.perturb_eps = perturb_eps\n","        self.alpha = gsam_alpha\n","\n","        # initialize self.rho_t\n","        self.update_rho_t()\n","\n","        # set up reduction for gradient across workers\n","        if grad_reduce.lower() == 'mean':\n","            if hasattr(ReduceOp, 'AVG'):\n","                self.grad_reduce = ReduceOp.AVG\n","                self.manual_average = False\n","            else: # PyTorch <= 1.11.0 does not have AVG, need to manually average across processes\n","                self.grad_reduce = ReduceOp.SUM\n","                self.manual_average = True\n","        elif grad_reduce.lower() == 'sum':\n","            self.grad_reduce = ReduceOp.SUM\n","            self.manual_average = False\n","        else:\n","            raise ValueError('\"grad_reduce\" should be one of [\"mean\", \"sum\"].')\n","\n","    @torch.no_grad()\n","    def update_rho_t(self):\n","        self.rho_t = self.rho_scheduler.step()\n","        return self.rho_t\n","\n","    @torch.no_grad()\n","    def perturb_weights(self, rho=0.0):\n","        grad_norm = self._grad_norm( weight_adaptive = self.adaptive )\n","        for group in self.param_groups:\n","            scale = rho / (grad_norm + self.perturb_eps)\n","\n","            for p in group[\"params\"]:\n","                if p.grad is None: continue\n","                self.state[p][\"old_g\"] = p.grad.data.clone()\n","                e_w = p.grad * scale.to(p)\n","                if self.adaptive:\n","                    e_w *= torch.pow(p, 2)\n","                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n","                self.state[p]['e_w'] = e_w\n","\n","    @torch.no_grad()\n","    def unperturb(self):\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if 'e_w' in self.state[p].keys():\n","                    p.data.sub_(self.state[p]['e_w'])\n","\n","    @torch.no_grad()\n","    def gradient_decompose(self, alpha=0.0):\n","        # calculate inner product\n","        inner_prod = 0.0\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None: continue\n","                inner_prod += torch.sum(\n","                    self.state[p]['old_g'] * p.grad.data\n","                )\n","\n","        # get norm\n","        new_grad_norm = self._grad_norm()\n","        old_grad_norm = self._grad_norm(by='old_g')\n","\n","        # get cosine\n","        cosine = inner_prod / (new_grad_norm * old_grad_norm + self.perturb_eps)\n","\n","        # gradient decomposition\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None: continue\n","                vertical = self.state[p]['old_g'] - cosine * old_grad_norm * p.grad.data / (new_grad_norm + self.perturb_eps)\n","                p.grad.data.add_( vertical, alpha=-alpha)\n","\n","    @torch.no_grad()\n","    def _sync_grad(self):\n","        if torch.distributed.is_initialized(): # synchronize final gardients\n","            for group in self.param_groups:\n","                for p in group['params']:\n","                    if p.grad is None: continue\n","                    if self.manual_average:\n","                        torch.distributed.all_reduce(p.grad, op=self.grad_reduce)\n","                        world_size = torch.distributed.get_world_size()\n","                        p.grad.div_(float(world_size))\n","                    else:\n","                        torch.distributed.all_reduce(p.grad, op=self.grad_reduce)\n","        return\n","\n","    @torch.no_grad()\n","    def _grad_norm(self, by=None, weight_adaptive=False):\n","        #shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n","        if not by:\n","            norm = torch.norm(\n","                    torch.stack([\n","                        ( (torch.abs(p.data) if weight_adaptive else 1.0) *  p.grad).norm(p=2)\n","                        for group in self.param_groups for p in group[\"params\"]\n","                        if p.grad is not None\n","                    ]),\n","                    p=2\n","               )\n","        else:\n","            norm = torch.norm(\n","                torch.stack([\n","                    ( (torch.abs(p.data) if weight_adaptive else 1.0) * self.state[p][by]).norm(p=2)\n","                    for group in self.param_groups for p in group[\"params\"]\n","                    if p.grad is not None\n","                ]),\n","                p=2\n","            )\n","        return norm\n","\n","    def load_state_dict(self, state_dict):\n","        super().load_state_dict(state_dict)\n","        self.base_optimizer.param_groups = self.param_groups\n","\n","    def maybe_no_sync(self):\n","        if torch.distributed.is_initialized():\n","            return self.model.no_sync()\n","        else:\n","            return contextlib.ExitStack()\n","\n","    @torch.no_grad()\n","    def set_closure(self, loss_fn, inputs, targets, **kwargs):\n","        # create self.forward_backward_func, which is a function such that\n","        # self.forward_backward_func() automatically performs forward and backward passes.\n","        # This function does not take any arguments, and the inputs and targets data\n","        # should be pre-set in the definition of partial-function\n","\n","        def get_grad():\n","            self.base_optimizer.zero_grad()\n","            with torch.enable_grad():\n","                outputs = self.model(inputs)\n","                loss = loss_fn(outputs, targets, **kwargs)\n","            loss_value = loss.data.clone().detach()\n","            loss.backward()\n","            return outputs, loss_value\n","\n","        self.forward_backward_func = get_grad\n","\n","    @torch.no_grad()\n","    def step(self, closure=None):\n","\n","        if closure:\n","            get_grad = closure\n","        else:\n","            get_grad = self.forward_backward_func\n","\n","        with self.maybe_no_sync():\n","            # get gradient\n","            outputs, loss_value = get_grad()\n","\n","            # perturb weights\n","            self.perturb_weights(rho=self.rho_t)\n","\n","            # disable running stats for second pass\n","            disable_running_stats(self.model)\n","\n","            # get gradient at perturbed weights\n","            get_grad()\n","\n","            # decompose and get new update direction\n","            self.gradient_decompose(self.alpha)\n","\n","            # unperturb\n","            self.unperturb()\n","\n","        # synchronize gradients across workers\n","        self._sync_grad()\n","\n","        # update with new directions\n","        self.base_optimizer.step()\n","\n","        # enable running stats\n","        enable_running_stats(self.model)\n","\n","        return outputs, loss_value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjqXQcLVwfDz"},"outputs":[],"source":["class ProportionScheduler:\n","    def __init__(self, pytorch_lr_scheduler, max_lr, min_lr, max_value, min_value):\n","        \"\"\"\n","        This scheduler outputs a value that evolves proportional to pytorch_lr_scheduler, e.g.\n","        (value - min_value) / (max_value - min_value) = (lr - min_lr) / (max_lr - min_lr)\n","        \"\"\"\n","        self.t = 0\n","        self.pytorch_lr_scheduler = pytorch_lr_scheduler\n","        self.max_lr = max_lr\n","        self.min_lr = min_lr\n","        self.max_value = max_value\n","        self.min_value = min_value\n","\n","        assert (max_lr > min_lr) or ((max_lr==min_lr) and (max_value==min_value)), \"Current scheduler for `value` is scheduled to evolve proportionally to `lr`,\" \\\n","        \"e.g. `(lr - min_lr) / (max_lr - min_lr) = (value - min_value) / (max_value - min_value)`. Please check `max_lr >= min_lr` and `max_value >= min_value`;\" \\\n","        \"if `max_lr==min_lr` hence `lr` is constant with step, please set 'max_value == min_value' so 'value' is constant with step.\"\n","\n","        assert max_value >= min_value\n","\n","        self.step() # take 1 step during initialization to get self._last_lr\n","\n","    def lr(self):\n","        return self._last_lr[0]\n","\n","    def step(self):\n","        self.t += 1\n","        if hasattr(self.pytorch_lr_scheduler, \"_last_lr\"):\n","            lr = self.pytorch_lr_scheduler._last_lr[0]\n","        else:\n","            lr = self.pytorch_lr_scheduler.optimizer.param_groups[0]['lr']\n","\n","        if self.max_lr > self.min_lr:\n","            value = self.min_value + (self.max_value - self.min_value) * (lr - self.min_lr) / (self.max_lr - self.min_lr)\n","        else:\n","            value = self.max_value\n","\n","        self._last_lr = [value]\n","        return value\n","\n","class SchedulerBase:\n","    def __init__(self, T_max, max_value, min_value=0.0, init_value=0.0, warmup_steps=0, optimizer=None):\n","        super(SchedulerBase, self).__init__()\n","        self.t = 0\n","        self.min_value = min_value\n","        self.max_value = max_value\n","        self.init_value = init_value\n","        self.warmup_steps = warmup_steps\n","        self.total_steps = T_max\n","\n","        # record current value in self._last_lr to match API from torch.optim.lr_scheduler\n","        self._last_lr = [init_value]\n","\n","        # If optimizer is not None, will set learning rate to all trainable parameters in optimizer.\n","        # If optimizer is None, only output the value of lr.\n","        self.optimizer = optimizer\n","\n","    def step(self):\n","        if self.t < self.warmup_steps:\n","            value = self.init_value + (self.max_value - self.init_value) * self.t / self.warmup_steps\n","        elif self.t == self.warmup_steps:\n","            value = self.max_value\n","        else:\n","            value = self.step_func()\n","        self.t += 1\n","\n","        # apply the lr to optimizer if it's provided\n","        if self.optimizer is not None:\n","            for param_group in self.optimizer.param_groups:\n","                param_group['lr'] = value\n","\n","        self._last_lr = [value]\n","        return value\n","\n","    def step_func(self):\n","        pass\n","\n","    def lr(self):\n","        return self._last_lr[0]\n","\n","class LinearScheduler(SchedulerBase):\n","    def step_func(self):\n","        value = self.max_value + (self.min_value - self.max_value) * (self.t - self.warmup_steps) / (\n","                    self.total_steps - self.warmup_steps)\n","        return value\n","\n","class CosineScheduler(SchedulerBase):\n","    def step_func(self):\n","        phase = (self.t-self.warmup_steps) / (self.total_steps-self.warmup_steps) * math.pi\n","        value = self.min_value + (self.max_value-self.min_value) * (np.cos(phase) + 1.) / 2.0\n","        return value\n","\n","class PolyScheduler(SchedulerBase):\n","    def __init__(self, poly_order=-0.5, *args, **kwargs):\n","        super(PolyScheduler, self).__init__(*args, **kwargs)\n","        self.poly_order = poly_order\n","        assert poly_order<=0, \"Please check poly_order<=0 so that the scheduler decreases with steps\"\n","\n","    def step_func(self):\n","        value = self.min_value + (self.max_value-self.min_value) * (self.t - self.warmup_steps)**self.poly_order\n","        return value"]},{"cell_type":"markdown","source":["# Trajectory Generation"],"metadata":{"id":"PMLKc4KQLLvt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEhWVLJ7tVNI"},"outputs":[],"source":["from tqdm import tqdm\n","\n","def trajectory_generation():\n","  channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader,loader_train_dict, class_map, class_map_inv = get_dataset(args.dataset, args.data_path)\n","\n","  #Initialize saved directory\n","  save_dir = os.path.join(args.buffer_path, args.dataset)\n","  save_dir = os.path.join(save_dir, args.model)\n","  if not os.path.exists(save_dir):\n","      os.makedirs(save_dir)\n","\n","  ''' organize the real dataset '''\n","  images_all = []\n","  labels_all = []\n","  indices_class = [[] for c in range(num_classes)]\n","  for i in tqdm(range(len(dst_train))):\n","      sample = dst_train[i]\n","      images_all.append(torch.unsqueeze(sample[0], dim=0))\n","      labels_all.append(class_map[torch.tensor(sample[1]).item()])\n","  #print('num of training images',len(images_all))\n","  len_dst_train = len(images_all)\n","\n","  for i, lab in tqdm(enumerate(labels_all)):\n","      indices_class[lab].append(i)\n","  images_all = torch.cat(images_all, dim=0).to(\"cpu\")\n","  labels_all = torch.tensor(labels_all, dtype=torch.long, device=\"cpu\")\n","\n","  for c in range(num_classes):\n","      print('class c = %d: %d real images'%(c, len(indices_class[c])))\n","\n","  for ch in range(channel):\n","      print('real images channel %d, mean = %.4f, std = %.4f'%(ch, torch.mean(images_all[:, ch]), torch.std(images_all[:, ch])))\n","\n","  criterion = nn.CrossEntropyLoss().to(args.device)\n","\n","  trajectories = []\n","\n","  dst_train = TensorDataset(copy.deepcopy(images_all.detach()), copy.deepcopy(labels_all.detach()))\n","  trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n","\n","  ''' set augmentation for whole-dataset training '''\n","  args.dc_aug_param = get_daparam(args.dataset, args.model, args.model, None)\n","  args.dc_aug_param['strategy'] = 'crop_scale_rotate'  # for whole-dataset training\n","  print('DC augmentation parameters: \\n', args.dc_aug_param)\n","\n","\n","  for it in range(0, args.num_experts):\n","\n","      ''' Train synthetic data '''\n","      teacher_net = get_network(args.model, channel, num_classes, im_size).to(args.device) # get a random model\n","      teacher_net.train()\n","      lr = args.lr_teacher\n","\n","      base_optimizer = torch.optim.SGD(teacher_net.parameters(), lr=lr, momentum=args.mom, weight_decay=args.l2)\n","      # scheduler = CosineScheduler(T_max=args.train_epochs*len_dst_train, max_value=lr, min_value=0.0,\n","          # optimizer=base_optimizer)\n","      scheduler = torch.optim.lr_scheduler.StepLR(base_optimizer,step_size=args.train_epochs*len(trainloader),gamma=1)\n","      rho_scheduler = ProportionScheduler(pytorch_lr_scheduler=scheduler, max_lr=lr, min_lr=lr,\n","          max_value=args.rho_max, min_value=args.rho_min)\n","      teacher_optim = GSAM(params=teacher_net.parameters(), base_optimizer=base_optimizer,\n","              model=teacher_net, gsam_alpha=args.alpha, rho_scheduler=rho_scheduler, adaptive=args.adaptive)\n","\n","\n","      teacher_optim.zero_grad()\n","\n","      timestamps = []\n","\n","      timestamps.append([p.detach().cpu() for p in teacher_net.parameters()])\n","\n","      lr_schedule = [args.train_epochs // 2 + 1]\n","      for e in range(args.train_epochs):\n","\n","          train_loss, train_acc = epoch(\"train\", dataloader=trainloader, net=teacher_net, optimizer=teacher_optim,\n","                                      criterion=criterion, args=args, aug=True,scheduler=scheduler)\n","\n","          test_loss, test_acc = epoch(\"test\", dataloader=testloader, net=teacher_net, optimizer=None,\n","                                      criterion=criterion, args=args, aug=False, scheduler=scheduler)\n","\n","          print(\"Itr: {}\\tEpoch: {}\\tTrain Acc: {}\\tTest Acc: {}\".format(it, e, train_acc, test_acc))\n","\n","          timestamps.append([p.detach().cpu() for p in teacher_net.parameters()])\n","\n","\n","      trajectories.append(timestamps)\n","\n","      if len(trajectories) == args.save_interval:\n","          n = 0\n","          while os.path.exists(os.path.join(save_dir, \"replay_buffer_{}.pt\".format(n))):\n","              n += 1\n","          print(\"Saving {}\".format(os.path.join(save_dir, \"replay_buffer_{}.pt\".format(n))))\n","          torch.save(trajectories, os.path.join(save_dir, \"replay_buffer_{}.pt\".format(n)))\n","          trajectories = []\n"]},{"cell_type":"markdown","source":["## MNIST"],"metadata":{"id":"iVp_Z9D6LOda"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VF8IaukhySxA"},"outputs":[],"source":["args = type('', (), {})()\n","args.dataset = 'MNIST'\n","args.model = 'ConvNet'\n","args.num_experts = 4 #training iterations\n","args.lr_teacher = 0.01\n","args.batch_train = 128\n","args.batch_real = 128\n","args.dsa = True\n","args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","args.dsa_param = utils.ParamDiffAug()\n","args.dsa_strategy = 'color_crop_cutout_flip_scale_rotate'\n","args.data_path = './data'\n","args.buffer_path = './buffers'\n","args.train_epochs = 300\n","args.mom = 0\n","args.l2 = 0\n","args.save_interval = 1\n","args.rho_max = 2\n","args.rho_min = 2\n","args.alpha = 0.4\n","args.adaptive = True\n","args.zca = 'store_true'\n","args.decay = 'store_true'"]},{"cell_type":"code","source":["trajectory_generation()"],"metadata":{"id":"yQGjpqgrLJrK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = type('', (), {})()\n","args.dataset = 'MHIST'\n","args.model = 'ConvNetD5'\n","args.num_experts = 10 #training iterations\n","args.lr_teacher = 1\n","args.batch_train = 256\n","args.batch_real = 256\n","args.dsa = True\n","args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","args.dsa_param = utils.ParamDiffAug()\n","args.dsa_strategy = 'color_crop_cutout_flip_scale_rotate'\n","args.data_path = './data'\n","args.buffer_path = './buffers'\n","args.train_epochs = 50\n","args.mom = 0\n","args.l2 = 0\n","args.save_interval = 1\n","args.rho_max = 0.01\n","args.rho_min = 0.01\n","args.alpha = 1\n","args.adaptive = True\n","args.zca = 'store_true'\n","args.decay = 'store_true'"],"metadata":{"id":"wSySOPe1ajQw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##MHIST"],"metadata":{"id":"tF8kDpoCLRHl"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"27I9CkrJ2KY8","executionInfo":{"status":"error","timestamp":1701101450041,"user_tz":300,"elapsed":2099111,"user":{"displayName":"Eric Feng","userId":"12633795715277578355"}},"outputId":"2af23885-d431-486e-b696-7eef74894c42"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train ZCA\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6304/6304 [09:28<00:00, 11.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test ZCA\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 977/977 [03:42<00:00,  4.39it/s]\n","  0%|          | 0/6304 [00:00<?, ?it/s]<ipython-input-9-73f896140616>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels_all.append(class_map[torch.tensor(sample[1]).item()])\n","100%|██████████| 6304/6304 [00:00<00:00, 68140.48it/s]\n","6304it [00:00, 1298732.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["class c = 0: 4324 real images\n","class c = 1: 1980 real images\n","real images channel 0, mean = 0.0000, std = 0.2059\n","real images channel 1, mean = -0.0000, std = 0.2466\n","real images channel 2, mean = 0.0000, std = 0.1690\n","DC augmentation parameters: \n"," {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]},{"output_type":"stream","name":"stdout","text":["Itr: 0\tEpoch: 0\tTrain Acc: 0.6567258883248731\tTest Acc: 0.631525076765609\n","Itr: 0\tEpoch: 1\tTrain Acc: 0.6859137055837563\tTest Acc: 0.631525076765609\n","Itr: 0\tEpoch: 2\tTrain Acc: 0.6859137055837563\tTest Acc: 0.631525076765609\n","Itr: 0\tEpoch: 3\tTrain Acc: 0.6841687817258884\tTest Acc: 0.631525076765609\n","Itr: 0\tEpoch: 4\tTrain Acc: 0.6851205583756346\tTest Acc: 0.7031729785056294\n","Itr: 0\tEpoch: 5\tTrain Acc: 0.6916243654822335\tTest Acc: 0.6632548618219037\n","Itr: 0\tEpoch: 6\tTrain Acc: 0.6828997461928934\tTest Acc: 0.7195496417604913\n","Itr: 0\tEpoch: 7\tTrain Acc: 0.6882931472081218\tTest Acc: 0.6704196519959058\n","Itr: 0\tEpoch: 8\tTrain Acc: 0.7054251269035533\tTest Acc: 0.6693961105424769\n","Itr: 0\tEpoch: 9\tTrain Acc: 0.7035215736040609\tTest Acc: 0.6908904810644831\n","Itr: 0\tEpoch: 10\tTrain Acc: 0.7147842639593909\tTest Acc: 0.744114636642784\n","Itr: 0\tEpoch: 11\tTrain Acc: 0.7236675126903553\tTest Acc: 0.6417604912998977\n","Itr: 0\tEpoch: 12\tTrain Acc: 0.7246192893401016\tTest Acc: 0.7338792221084954\n","Itr: 0\tEpoch: 13\tTrain Acc: 0.7276332487309645\tTest Acc: 0.7349027635619243\n","Itr: 0\tEpoch: 14\tTrain Acc: 0.7209708121827412\tTest Acc: 0.7717502558853634\n","Itr: 0\tEpoch: 15\tTrain Acc: 0.7469860406091371\tTest Acc: 0.7574206755373593\n","Itr: 0\tEpoch: 16\tTrain Acc: 0.748730964467005\tTest Acc: 0.7758444216990789\n","Itr: 0\tEpoch: 17\tTrain Acc: 0.7381027918781726\tTest Acc: 0.7737973387922211\n","Itr: 0\tEpoch: 18\tTrain Acc: 0.7431789340101523\tTest Acc: 0.7707267144319345\n","Itr: 0\tEpoch: 19\tTrain Acc: 0.7584073604060914\tTest Acc: 0.8147389969293757\n","Itr: 0\tEpoch: 20\tTrain Acc: 0.7614213197969543\tTest Acc: 0.7932446264073695\n","Itr: 0\tEpoch: 21\tTrain Acc: 0.7596763959390863\tTest Acc: 0.8065506653019447\n","Itr: 0\tEpoch: 22\tTrain Acc: 0.7691941624365483\tTest Acc: 0.8321392016376663\n","Itr: 0\tEpoch: 23\tTrain Acc: 0.7741116751269036\tTest Acc: 0.7226202661207779\n","Itr: 0\tEpoch: 24\tTrain Acc: 0.7893401015228426\tTest Acc: 0.7584442169907881\n","Itr: 0\tEpoch: 25\tTrain Acc: 0.7845812182741116\tTest Acc: 0.7113613101330604\n","Itr: 0\tEpoch: 26\tTrain Acc: 0.7818845177664975\tTest Acc: 0.8188331627430911\n","Itr: 0\tEpoch: 27\tTrain Acc: 0.7841053299492385\tTest Acc: 0.8106448311156602\n","Itr: 0\tEpoch: 28\tTrain Acc: 0.789498730964467\tTest Acc: 0.8515864892528148\n","Itr: 0\tEpoch: 29\tTrain Acc: 0.7858502538071066\tTest Acc: 0.8433981576253838\n","Itr: 0\tEpoch: 30\tTrain Acc: 0.8021890862944162\tTest Acc: 0.8239508700102354\n","Itr: 0\tEpoch: 31\tTrain Acc: 0.7839467005076142\tTest Acc: 0.8065506653019447\n","Itr: 0\tEpoch: 32\tTrain Acc: 0.7969543147208121\tTest Acc: 0.8362333674513818\n","Itr: 0\tEpoch: 33\tTrain Acc: 0.7871192893401016\tTest Acc: 0.8454452405322416\n","Itr: 0\tEpoch: 34\tTrain Acc: 0.8120241116751269\tTest Acc: 0.7840327533265097\n","Itr: 0\tEpoch: 35\tTrain Acc: 0.8012373096446701\tTest Acc: 0.7768679631525077\n","Itr: 0\tEpoch: 36\tTrain Acc: 0.8171002538071066\tTest Acc: 0.8853633572159673\n","Itr: 0\tEpoch: 37\tTrain Acc: 0.8063134517766497\tTest Acc: 0.8505629477993859\n","Itr: 0\tEpoch: 38\tTrain Acc: 0.8148794416243654\tTest Acc: 0.7993858751279427\n","Itr: 0\tEpoch: 39\tTrain Acc: 0.8163071065989848\tTest Acc: 0.8628454452405322\n","Itr: 0\tEpoch: 40\tTrain Acc: 0.8282043147208121\tTest Acc: 0.8792221084953941\n","Itr: 0\tEpoch: 41\tTrain Acc: 0.8240799492385786\tTest Acc: 0.8628454452405322\n","Itr: 0\tEpoch: 42\tTrain Acc: 0.8266180203045685\tTest Acc: 0.8955987717502559\n","Itr: 0\tEpoch: 43\tTrain Acc: 0.8259835025380711\tTest Acc: 0.7922210849539406\n","Itr: 0\tEpoch: 44\tTrain Acc: 0.8250317258883249\tTest Acc: 0.8833162743091095\n","Itr: 0\tEpoch: 45\tTrain Acc: 0.8353426395939086\tTest Acc: 0.7482088024564995\n","Itr: 0\tEpoch: 46\tTrain Acc: 0.8231281725888325\tTest Acc: 0.8679631525076765\n","Itr: 0\tEpoch: 47\tTrain Acc: 0.8323286802030457\tTest Acc: 0.8925281473899693\n","Itr: 0\tEpoch: 48\tTrain Acc: 0.8427982233502538\tTest Acc: 0.8915046059365405\n","Itr: 0\tEpoch: 49\tTrain Acc: 0.845019035532995\tTest Acc: 0.8925281473899693\n","Saving ./buffers/MHIST/ConvNetD4/replay_buffer_0.pt\n","Itr: 1\tEpoch: 0\tTrain Acc: 0.6595812182741116\tTest Acc: 0.631525076765609\n","Itr: 1\tEpoch: 1\tTrain Acc: 0.6859137055837563\tTest Acc: 0.631525076765609\n","Itr: 1\tEpoch: 2\tTrain Acc: 0.6860723350253807\tTest Acc: 0.6325486182190379\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-9a899a078629>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrajectory_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-73f896140616>\u001b[0m in \u001b[0;36mtrajectory_generation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m           train_loss, train_acc = epoch(\"train\", dataloader=trainloader, net=teacher_net, optimizer=teacher_optim,\n\u001b[0m\u001b[1;32m     74\u001b[0m                                       criterion=criterion, args=args, aug=True,scheduler=scheduler)\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-4998b6eaed7a>\u001b[0m in \u001b[0;36mepoch\u001b[0;34m(mode, dataloader, net, optimizer, criterion, args, aug, scheduler, texture)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0macc_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["trajectory_generation()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YbgH4cNd7Ld9"},"outputs":[],"source":["trajectory = torch.load('/content/drive/MyDrive/ECE1513/Project_B_Supp/buffers/MNIST/ConvNet/replay_buffer_0.pt')\n","#label_syn_noise_mhist = torch.load('label_syn.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xtJrtXCx7-Qk"},"outputs":[],"source":["len(trajectory[0][0])"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyPJcmWPzblZdbf4MteNvMH6"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}